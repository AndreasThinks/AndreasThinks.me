[
  {
    "objectID": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html",
    "href": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html",
    "title": "Twitter Sentiment Analysis with FastAI",
    "section": "",
    "text": "The twitter API infuriatingly does not let you pull tweets more than a week old. As such, I’ve had to use a combination of the GetOldTweets library and then Tweepy to extract the biography data. As such, this does need access to the Twitter API and a developer account (though I found that quite easy to get). \n\npip install GetOldTweets3\n\nCollecting GetOldTweets3\n  Downloading https://files.pythonhosted.org/packages/ed/f4/a00c2a7c90801abc875325bb5416ce9090ac86d06a00cc887131bd73ba45/GetOldTweets3-0.0.11-py3-none-any.whl\nRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (4.2.6)\nCollecting pyquery>=1.2.10\n  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\nCollecting cssselect>0.7.9\n  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\nInstalling collected packages: cssselect, pyquery, GetOldTweets3\nSuccessfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.1\n\n\n\nimport tweepy\nimport logging\nimport GetOldTweets3 as got\nimport pandas as pd\n\n\nAPI_KEY = \"xx\"\nAPI_SECRET = \"xx\"\nACCESS_TOKEN = \"xx\"\nACCESS_TOKEN_SECRET =\"xx\"\n\nauth = tweepy.OAuthHandler(API_KEY, API_SECRET)\nauth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\napi = tweepy.API(auth)\n\nfrom tweepy import API\n\n\nuser_name = \"@metpoliceuk\"\nq='to:{}'.format(user_name)\n\n\nreplies = api.search(q=q)\n\nIn this case, I’ve put together a query that pulls all tweets directed to the MPS username between set dates. I’d warn that even using GetOldTweets, pull too many tweets at once and Twitter will kick you out - I found that especially during especially active social media days, I had to add a delay to my code.\n\ntweetCriteria = got.manager.TweetCriteria().setQuerySearch(q)\\\n                                           .setSince(\"2020-08-24\")\\\n                                           .setUntil(\"2020-08-25\")\ntweets = got.manager.TweetManager.getTweets(tweetCriteria)\n\ndf = pd.DataFrame(tweets)\n\ndef get_text(tweet):\n  return tweet.text\n\ndef get_username(tweet):\n  return tweet.username\n\ndf[\"username\"] = df[0].apply(get_username)\ndf[\"text\"] = df[0].apply(get_text)\n\nbios_df = df.copy()\nbios = bios_df.dropna(axis=0, subset=[\"username\"])\n\nlist_of_bios = bios[\"username\"].tolist()\n\ndef get_bio(user):\n  list_of_bios = pd.DataFrame(columns=[\"Users\",\"Bios\"])\n  i = 100\n  while i < len(user):\n    users = user[i-100:i]\n    test = api.lookup_users(screen_names =users)\n    for person in test:\n      list_of_bios = list_of_bios.append({'Users' : person.screen_name , 'Bios' : person.description}, ignore_index=True)\n    i = i + 100\n  users = user[i-100:len(user)]\n  test = api.lookup_users(screen_names =users)\n  for person in test:\n      list_of_bios = list_of_bios.append({'Users' : person.screen_name , 'Bios' : person.description}, ignore_index=True)\n  return list_of_bios\n \nlist_of_new_bios = get_bio(df[\"username\"].tolist())\n\nlist_of_new_bios[\"username\"] = list_of_new_bios[\"Users\"]\ndf_with_bio = df.merge(list_of_new_bios, on=\"username\")\ndf_with_bio = df_with_bio.rename(columns={\"Bios\":\"bio\"})\ndf_with_bio = df_with_bio.drop(\"Users\",axis=1)\n\nWith that all done, you can pull out a list of all your tweets that day, as well a the userbiography.\n\ndf_with_bio\n\n\n\n\n\n  \n    \n      \n      0\n      username\n      text\n      bio\n    \n  \n  \n    \n      0\n      <GetOldTweets3.models.Tweet.Tweet object at 0x...\n      ClintClease\n      He’s got smug look on his face. A creature lik...\n      Author, Designer, Composer, Photographer, Anti...\n    \n    \n      1\n      <GetOldTweets3.models.Tweet.Tweet object at 0x...\n      Listhebest2020\n      London is safe huh?! Judging by the majority o...\n      Nobody you know. #NLF #TheKLF and #KBF. RTs ar...\n    \n    \n      2\n      <GetOldTweets3.models.Tweet.Tweet object at 0x...\n      repentandtrust\n      @metpoliceuk @UKParliament why are you still a...\n      ✝️ Christian\\n✝️ Defending Christianity agains...\n    \n    \n      3\n      <GetOldTweets3.models.Tweet.Tweet object at 0x...\n      hoxtonist\n      If you killed someone driving a car, you reall...\n      \n    \n    \n      4\n      <GetOldTweets3.models.Tweet.Tweet object at 0x...\n      joepublic99\n      @metpoliceuk @UKSupremeCourt @TheFCA @LibDems ...\n      True Social Justice ¦ Thriving Fair Economy ¦ ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      175\n      <GetOldTweets3.models.Tweet.Tweet object at 0x...\n      MornnigGlory\n      @metpoliceuk #metpoliceuk\n      Self-Made man - confident reliable honest cari...\n    \n    \n      176\n      <GetOldTweets3.models.Tweet.Tweet object at 0x...\n      MatthewFahey15\n      Um, not to sure what to say other than that's ...\n      Disability rights advocate, Supporter of the a...\n    \n    \n      177\n      <GetOldTweets3.models.Tweet.Tweet object at 0x...\n      AlfredWintle\n      London isn’t safe so stop virtue signalling, s...\n      Western virtues such as democracy, free speech...\n    \n    \n      178\n      <GetOldTweets3.models.Tweet.Tweet object at 0x...\n      PatriciaRickey2\n      But BLM riots allowed to happen\n      \n    \n    \n      179\n      <GetOldTweets3.models.Tweet.Tweet object at 0x...\n      Me1Chri\n      \n      Journalist forever (I care, write, edit, desig...\n    \n  \n\n180 rows × 4 columns"
  },
  {
    "objectID": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html#coding-and-learning",
    "href": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html#coding-and-learning",
    "title": "Twitter Sentiment Analysis with FastAI",
    "section": "Coding and Learning",
    "text": "Coding and Learning\nThis is the painful bit: I had to manually go through a few hundred tweets and manually label them as “Right Wing” or not based on my own hunches.\nI went with the below\n\nAnti-immigration sentiment\nAnti-socialism/BLM\nPro-Brexit/Anti-EU\nPro-Trump/MAGA\n\nThe coded set can be found below.\n\nimport pandas as pd\nfrom fastai.text.all import *\n\n\ntraining = pd.read_csv(\"/content/drive/My Drive/data/sentiment training set.csv\")\ntraining\n\n\n\n\n\n  \n    \n      \n      username\n      bio\n      RW\n      is_valid\n    \n  \n  \n    \n      0\n      RichardSTOCKDA4\n      RUGBY LOVING EX PLAYER NOW ONE OF THE PROUD RVS/NHS VOLUNTEERING TEAM ACTIVELY SEEKING OUT/EXPOSING GROOMING GANG MEMBERS\n      1\n      1\n    \n    \n      1\n      cold957\n      #Veteran. 1st Bn Coldstream Guards/Guards Para.\\nWe can forgive a child who is afraid of the dark. The real tragedy of life is when men fear the light.\\nNO DMs.\n      1\n      1\n    \n    \n      2\n      leftwant2brite\n      Be aware of leftwing politics, Jihadists & militant Communists like those in ANTIFA. fascism is totalitarianism born from socialism. It's also an economic syst🧐\n      1\n      1\n    \n    \n      3\n      HermioneMidwife\n      Digital Midwife at the RCM and King’s College Hospital. Women centred, clinically led, digitally driven. FNF Scholar\n      0\n      1\n    \n    \n      4\n      scoutingfamily\n      Professional Football Scout & Player Recruitment Specialist \\n@england @stokecity \\nEngland U17 World Cup Winners 🏆🥇\n      0\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      502\n      AlfredWintle\n      Western virtues such as democracy, free speech, equal & human rights need to be preserved & protected before it’s too late. Love Europe & therefore hate the EU.\n      1\n      1\n    \n    \n      503\n      Me1Chri\n      Journalist forever (I care, write, edit, design, research and study, cook, volunteer). Project Editor. Kingston University, London.\n      0\n      1\n    \n    \n      504\n      UK Cop Humour\n      Simply trying to show the human side of our fantastic Police Officers & raise a bit of money for good causes whilst we're at it! RT's are not an endorsement.\n      0\n      1\n    \n    \n      505\n      CopThatCooks\n      Detective with \\n@WMerciaPolice\\n working in #Worcester CID . Also tweeting as \\n@HistoryCop\\n. Here to engage, explain and encourage.\n      0\n      1\n    \n    \n      506\n      Northern_Bobby\n      Traffic Sgt on a Roads Policing Team somewhere in the UK. Passionate about Traffic and Road SafteyMen holding handsRainbow flag #Taser #TPAC\n      0\n      1\n    \n  \n\n507 rows × 4 columns\n\n\n\nFastAI includes the very convenient concept of a “DataLoader”: a python object that combines all the data you’re working with, split into a training and test set, in a format it can work with.\nFor text classication, that object does a lot of the heavy lifting for you. NLP normally requires an awful lot of data-cleaning: you break your text into words, then add special characters to capitalisation, sentence starts, etc. As you can see below, FastAI has done all of that in a few lines of code. The down-side is, I’m not hugely clear exactly what it’s done, or how I can now read my text.\n\ntraining = training.dropna(axis=0)\ntraining[\"RW\"] = training[\"RW\"].astype(\"boolean\")\ntraining[\"is_valid\"] = training[\"is_valid\"].astype(\"boolean\")\ndls = TextDataLoaders.from_df(training, text_col = \"bio\", label_col=\"RW\")\ndls.show_batch()\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxmaj xxunk xxmaj united , xxmaj welsh xxmaj rugby , xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk , xxmaj the xxmaj xxunk , xxmaj small xxmaj xxunk , xxmaj the xxmaj who , xxmaj the xxmaj xxunk , xxmaj the xxmaj xxunk , xxmaj xxunk , xxmaj xxunk + xxmaj the xxmaj xxunk\n      False\n    \n    \n      1\n      xxbos xxmaj ex xxunk . xxmaj xxunk . xxmaj xxunk living in xxmaj london . xxmaj england great . xxmaj dislike xxup xxunk . \\n\\n ( xxunk : xxmaj xxunk xxmaj xxunk xxmaj xxunk xxmaj xxunk , xxup xxunk , & xxunk ) \\n ( small : xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxup xxunk ) xxpad xxpad xxpad xxpad xxpad\n      False\n    \n    \n      2\n      xxbos xxmaj peace , i am xxmaj xxunk , \\n xxmaj xxunk xxmaj xxunk xxmaj xxunk xxmaj xxunk \\n xxmaj xxunk xxmaj xxunk xxmaj xxunk xxmaj xxunk xxmaj xxunk \\n xxmaj follow xxmaj me xxmaj for xxmaj follow xxmaj back \\n xxmaj retweet xxmaj me xxmaj for xxmaj follow xxmaj back \\n xxmaj xxunk xxmaj love xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n    \n    \n      3\n      xxbos xxmaj loves xxmaj xxunk , xxmaj cats , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk , xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj the xxmaj xxunk , \\n xxmaj xxunk & xxmaj xxunk . \\n xxmaj art xxunk xxunk - https : / / t.co / xxunk 🙏 xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n    \n    \n      4\n      xxbos xxmaj xxunk me know when the xxunk xxunk , \\n xxmaj no xxmaj xxunk xxmaj brexit \\n xxmaj if its on the xxup bbc its not true , \\n▁ # defundthebbc \\n▁ # xxup maga , # xxup mbga , # xxunk , # xxunk \\n i xxunk not xxunk of xxmaj xxunk , xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n    \n    \n      5\n      xxbos xxmaj love my xxunk xxmaj xxunk fc , xxmaj football , xxmaj xxunk , xxmaj xxunk , xxmaj xxunk and xxmaj xxunk . xxmaj but i love nothing more than my family and xxmaj xxunk xxmaj xxunk xxunk ️ # xxup lfc # xxup xxunk # xxup ynwa \\n\\n xxmaj xxunk - xxmaj xxunk xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n    \n    \n      6\n      xxbos xxmaj xxunk . xxup xxunk xxmaj member . xxup xxunk xxmaj member , xxmaj xxunk xxrep 3 i xxmaj member , xxmaj xxunk xxmaj member , xxmaj xxunk not xxmaj xxunk , # xxup xxunk , # xxup xxunk , # xxup xxunk , # xxup xxunk , 🇬 🇧 🇨 xxunk xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n    \n    \n      7\n      xxbos xxmaj xxunk xxmaj west xxunk live for xxunk xxunk . xxmaj xxunk xxmaj xxunk xxmaj xxunk , hate xxmaj london xxmaj xxunk . xxmaj still a xxup xxunk xxunk block xxunk . xxunk ️ # xxup xxunk # xxmaj xxunk xxrep 3 xxunk # xxunk # xxmaj brexit # xxmaj england xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n    \n    \n      8\n      xxbos # xxmaj xxunk . 1st xxmaj xxunk xxmaj xxunk xxmaj xxunk / xxmaj xxunk xxmaj xxunk . \\n xxmaj we can xxunk a xxunk who is afraid of the xxunk . xxmaj the real xxunk of life is when men fear the light . \\n xxup no dms . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      True\n    \n  \n\n\n\nThis is the fun bit: learning. Notice hear we aren’t starting from scracth: we call the “fine_tune” function, because fastAI already contains a model trained on text.\n\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(5)\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.881830\n      0.655739\n      0.861386\n      00:12\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.825345\n      0.595676\n      0.851485\n      00:34\n    \n    \n      1\n      0.796265\n      0.541380\n      0.891089\n      00:29\n    \n    \n      2\n      0.784324\n      0.509539\n      0.881188\n      00:28\n    \n    \n      3\n      0.764621\n      0.497642\n      0.871287\n      00:30\n    \n    \n      4\n      0.742817\n      0.503554\n      0.801980\n      00:29\n    \n  \n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n      category_\n    \n  \n  \n    \n      0\n      xxbos xxmaj xxunk xxmaj of # xxunk xxmaj with xxmaj old # xxup xxunk xxmaj xxunk / s , & # xxup xxunk / # xxup xxunk , u xxup must xxup be xxunk + xxunk xxup me . xxup i 'm xxup open & xxup xxunk xxup about xxup my # xxup xxunk , # xxup xxunk , & xxup life xxunk # xxup xxunk\n      False\n      False\n    \n    \n      1\n      xxbos xxmaj bye xxmaj bye xxup eu xxunk xxmaj xxunk so xxunk xxmaj i xxunk only xxunk % xxmaj english with xxunk % xxmaj welsh , xxmaj irish , xxmaj xxunk + little bit of xxmaj xxunk . that ’ll be the xxmaj xxunk side . xxmaj xxunk ; i xxunk back ! xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      True\n      True\n    \n    \n      2\n      xxbos xxmaj xxunk 1 xxmaj xxunk since ' xxunk ; xxup xxunk xxmaj football xxmaj referee ; xxup xxunk xxmaj referee xxmaj xxunk ; xxup xxunk ; xxmaj xxunk - time xxmaj xxunk & xxmaj xxunk xxmaj xxunk . i may xxunk with you but will never xxunk you ! xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n      False\n    \n    \n      3\n      xxbos xxmaj xxunk , the xxunk , xxunk xxunk . xxmaj xxunk xxmaj xxunk . ' i do n't agree xxunk / what u have 2 say , but xxmaj i 'll xxunk 2 xxunk xxunk right 2 say it . ' xxmaj xxunk xxmaj xxunk xxmaj xxunk xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n      False\n    \n    \n      4\n      xxbos xxmaj xxunk up with xxmaj all the xxmaj politicians xxunk to us . \\n xxunk \\n xxmaj do nt xxunk me off about xxmaj snowflakes of the world . \\n\\n xxmaj xxunk when xxmaj i 'm out xxunk my bike xxunk or walking my dog 🐶 xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n      False\n    \n    \n      5\n      xxbos xxmaj xxunk with xxup me / xxmaj xxunk 20 xxunk . xxmaj loves - xxmaj xxunk ( my xxunk ) xxunk xxmaj my xxunk , mum , xxunk family . xxmaj dogs ( more than people ! ) 🐶 \\n xxmaj xxunk / xxmaj xxunk xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n      False\n    \n    \n      6\n      xxbos xxmaj xxunk for xxmaj xxunk , xxmaj xxunk of xxmaj xxunk xxmaj xxunk xxmaj trust . xxmaj xxunk xxmaj xxunk xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj face xxmaj uk , xxmaj supports our xxmaj armed xxmaj forces and our xxmaj emergency services . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      True\n      False\n    \n    \n      7\n      xxbos xxmaj xxunk in xxmaj xxunk xxmaj xxunk . xxmaj first xxmaj xxunk xxunk xxunk from xxmaj xxunk . xxmaj xxunk xxunk xxmaj may 1st xxunk . xxunk xxmaj xxunk to xxmaj prem , back to xxunk , back to xxmaj prem in xxunk years ! xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n      False\n    \n    \n      8\n      xxbos xxmaj xxunk . xxmaj author . xxmaj xxunk with the xxunk from xxmaj xxunk . xxmaj xxunk . xxmaj retweet / follow does n't xxunk xxunk - i use xxunk a lot . xxmaj xxunk but not xxmaj xxunk . xxup xxunk . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\n      False\n      False\n    \n  \n\n\n\nNow, we can start predicting! FastAI will take any bundle of text and tell you whether it thinks it’s right wing or not (based on my coding). If I’m entirely honest, this doesn’t work fantastically - it seems to have decided that the phrase “Justice for” is essentially a right wing calling card, which I’m not entirely comfortable with. That said, the process works! Ish.\n\nlearn.predict(\"Justice for our Brexit\")\n\n\n\n\n('True', tensor(1), tensor([0.3523, 0.6477]))\n\n\n\nlearn.predict(\"Hate snowflakes, socialists, and the EU\")\n\n\n\n\n('True', tensor(1), tensor([0.2483, 0.7517]))\n\n\n\nlearn.predict(\"I love puppies and stuff\")\n\n\n\n\n('False', tensor(0), tensor([0.5286, 0.4714]))\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()"
  },
  {
    "objectID": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html#the-prototype",
    "href": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html#the-prototype",
    "title": "Twitter Sentiment Analysis with FastAI",
    "section": "The Prototype",
    "text": "The Prototype\nSo, can we convert all this into a working application? Sure! Ish. My combining my tweet extractor with sentiment analysis of the tweet text itself (I found this medium blog post very helpful) we can analyse for any specific day, the volume of tweets flagged as “right wing”, and contrast them to the overall messaging, and compare their “Subjectivity (how subjective or opinionated the text is — a score of 0 is fact, and a score of +1 is very much an opinion) and the other to get the tweets called Polarity (how positive or negative the text is, — score of -1 is the highest negative score, and a score of +1 is the highest positive score)”.\nWhile I would have loved to deploy this to Heroku or Binder using Voila, the Pytorch text classification model annoying takes far over 500mb of space, so neither free option will support it - it does look like you can chose to use the CPU pytorch option instead, but frankly it’s painful to get the implementation working!\n\nHelper Functions\nWe start by bringing together all my previous code (as well as some rough sentiment analysis of the text itself) into one chunk, then added widgets.\n\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\nimport plotly.express as px\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\nfrom ipywidgets import *\n\n\nimport tweepy\nimport logging\nfrom tweepy import API\n\nauth = tweepy.OAuthHandler(API_KEY, API_SECRET)\nauth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\napi = tweepy.API(auth)\n\nlearn_inf = learn\n\n#Tweet cleaning helper function\ndef cleanTxt(text):\n text = re.sub('@[A-Za-z0–9]+', '', text) #Removing @mentions\n text = re.sub('#', '', text) # Removing '#' hash tag\n text = re.sub('RT[\\s]+', '', text) # Removing RT\n text = re.sub('https?:\\/\\/\\S+', '', text) # Removing hyperlink\n \n return text\n\n#helper function that pulls the biography of a username using the Twitter api\ndef get_bio(user):\n  list_of_bios = pd.DataFrame(columns=[\"Users\",\"Bios\"])\n  i = 100\n  while i < len(user):\n    users = user[i-100:i]\n    test = api.lookup_users(screen_names =users)\n    for person in test:\n      list_of_bios = list_of_bios.append({'Users' : person.screen_name , 'Bios' : person.description}, ignore_index=True)\n    i = i + 100\n  users = user[i-100:len(user)]\n  test = api.lookup_users(screen_names =users)\n  for person in test:\n      list_of_bios = list_of_bios.append({'Users' : person.screen_name , 'Bios' : person.description}, ignore_index=True)\n  return list_of_bios\n\n\ndef get_label(row):\n  return row[0]\n\ndef getSubjectivity(text):\n   return TextBlob(text).sentiment.subjectivity\n\n# Create a function to get the polarity\ndef getPolarity(text):\n   return  TextBlob(text).sentiment.polarity\n\ndef cleanTxt(text):\n text = re.sub('@[A-Za-z0–9]+', '', text) #Removing @mentions\n text = re.sub('#', '', text) # Removing '#' hash tag\n text = re.sub('RT[\\s]+', '', text) # Removing RT\n text = re.sub('https?:\\/\\/\\S+', '', text) # Removing hyperlink\n \n return text\n\n\nlist_of_new_bios = get_bio(df[\"username\"].tolist())\n\n\ndef run_process(user_name, date_from, date_to):\n\n  q='to:{}'.format(user_name)\n\n  tweetCriteria = got.manager.TweetCriteria().setQuerySearch(q)\\\n                                            .setSince(date_from)\\\n                                            .setUntil(date_to)\n  tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n\n  df = pd.DataFrame(tweets)\n\n  def get_text(tweet):\n    return tweet.text\n\n  def get_username(tweet):\n    return tweet.username\n\n  df[\"username\"] = df[0].apply(get_username)\n  df[\"text\"] = df[0].apply(get_text)\n\n  bios_df = df.copy()\n  bios = bios_df.dropna(axis=0, subset=[\"username\"])\n  bios = bios.drop_duplicates(subset=[\"username\"])\n\n  auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n  auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\n  api = tweepy.API(auth)\n\n  list_of_new_bios = get_bio(df[\"username\"].tolist())\n\n  list_of_new_bios[\"RW_prediction\"] = list_of_new_bios[\"Bios\"].apply(learn_inf.predict)\n\n  list_of_new_bios[\"RW_prediction\"] = list_of_new_bios[\"RW_prediction\"].apply(get_label)\n\n  list_of_new_bios = list_of_new_bios.rename({\"Users\":\"username\"}, axis=1)\n\n  new_df = df.merge(list_of_new_bios, on=[\"username\"])\n\n\n\n  # Clean the tweets\n  new_df['clean_text'] = new_df['text'].apply(cleanTxt)\n\n    # Create two new columns 'Subjectivity' & 'Polarity'\n  new_df['Subjectivity'] = new_df['clean_text'].apply(getSubjectivity)\n  new_df['Polarity'] = new_df['clean_text'].apply(getPolarity)\n\n  right_wing_mask = new_df[\"RW_prediction\"] == \"True\"\n  right_wing = new_df[right_wing_mask]\n\n  not_right_wing_mask = new_df[\"RW_prediction\"] == \"False\"\n  not_right_wing = new_df[not_right_wing_mask]\n\n  fig2 = px.scatter(new_df, x=\"Subjectivity\", y=\"Polarity\", color=\"RW_prediction\", trendline=\"ols\", hover_data=[\"Bios\"])\n  fig2.update_xaxes(range=[-0.1,1.1])\n  fig2.update_yaxes(range=[-1.1, 1.1])\n\n  grouped_by_RW = new_df.groupby(\"RW_prediction\").agg(\n    count= (\"text\", len),\n    Subjectivity = (\"Subjectivity\", np.mean),\n    Polarity = (\"Polarity\", np.mean))\n  \n  fig1 = px.bar(grouped_by_RW, y=\"count\", color=grouped_by_RW.index)\n\n  final = round(grouped_by_RW[\"count\"].iloc[1] / grouped_by_RW[\"count\"].iloc[0],3) * 100\n\n  return grouped_by_RW, fig1, fig2, final\n\n\nusername_select =widgets.Text(\n    value='@metpoliceuk',\n    placeholder='Type something',\n    description='Username:',\n    disabled=False\n)\n\ndate_from_widget =widgets.Text(\n    value='2020-08-27',\n    placeholder='Type something',\n    description='Date From:',\n    disabled=False\n)\n\ndate_to_widget =widgets.Text(\n    value='2020-08-28',\n    placeholder='Type something',\n    description='Date To:',\n    disabled=False\n)\n\ndef show_graph2(change):\n  with out_df:\n    print(\"analysing..\")\n  results = run_process(username_select.value, date_from_widget.value, date_to_widget.value)\n  out_df.clear_output()\n  out_analysis.clear_output()\n  out_graph1.clear_output()\n  out_graph2.clear_output()\n  with out_df:\n    print(str(results[3]) + \"% RW interactions\")\n  with out_analysis:\n    print(results[0])\n  with out_graph1:\n    results[1].show()\n  with out_graph2:\n    results[2].show()\n  \nanalyse_button = widgets.Button(description='Analyse')\n\nanalyse_button.on_click(show_graph2)\n\nout_df = widgets.Output(layout={'border': '1px solid black'})\nout_analysis = widgets.Output(layout={'border': '1px solid black'})\nout_graph1 = widgets.Output(layout={'border': '1px solid black'})\nout_graph2 = widgets.Output(layout={'border': '1px solid black'})\n\n\n\nThe Product\nFinally, my working product - given a user-name and two dates, it will pull all tweets on those dates, run the sentiment analysis, show the ratio of right-wing tweets, and the sentiment on those. Here I’ve specifically picked the day Dawn Butler was stopped by police, and notice how right wing\n\n'''VBox([widgets.Label('Tweet Monitor'),\n      username_select, date_from_widget, date_to_widget, analyse_button, out_df, out_analysis, out_graph1, out_graph2])'''\n\n\"VBox([widgets.Label('Tweet Monitor'),\\n      username_select, date_from_widget, date_to_widget, analyse_button, out_df, out_analysis, out_graph1, out_graph2])\"\n\n\n\ndef run_process_2(user_name, date_from, date_to):\n\n  q='to:{}'.format(user_name)\n\n  tweetCriteria = got.manager.TweetCriteria().setQuerySearch(q)\\\n                                            .setSince(date_from)\\\n                                            .setUntil(date_to)\n  tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n\n  df = pd.DataFrame(tweets)\n\n  def get_text(tweet):\n    return tweet.text\n\n  def get_username(tweet):\n    return tweet.username\n\n  df[\"username\"] = df[0].apply(get_username)\n  df[\"text\"] = df[0].apply(get_text)\n\n  bios_df = df.copy()\n  bios = bios_df.dropna(axis=0, subset=[\"username\"])\n  bios = bios.drop_duplicates(subset=[\"username\"])\n\n  auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n  auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\n  api = tweepy.API(auth)\n\n  list_of_new_bios = get_bio(df[\"username\"].tolist())\n\n  list_of_new_bios[\"RW_prediction\"] = list_of_new_bios[\"Bios\"].apply(learn_inf.predict)\n\n  list_of_new_bios[\"RW_prediction\"] = list_of_new_bios[\"RW_prediction\"].apply(get_label)\n\n  list_of_new_bios = list_of_new_bios.rename({\"Users\":\"username\"}, axis=1)\n\n  new_df = df.merge(list_of_new_bios, on=[\"username\"])\n\n\n\n  # Clean the tweets\n  new_df['clean_text'] = new_df['text'].apply(cleanTxt)\n\n    # Create two new columns 'Subjectivity' & 'Polarity'\n  new_df['Subjectivity'] = new_df['clean_text'].apply(getSubjectivity)\n  new_df['Polarity'] = new_df['clean_text'].apply(getPolarity)\n\n  right_wing_mask = new_df[\"RW_prediction\"] == \"True\"\n  right_wing = new_df[right_wing_mask]\n\n  not_right_wing_mask = new_df[\"RW_prediction\"] == \"False\"\n  not_right_wing = new_df[not_right_wing_mask]\n\n  fig2 = px.scatter(new_df, x=\"Subjectivity\", y=\"Polarity\", color=\"RW_prediction\", trendline=\"ols\", hover_data=[\"Bios\"])\n  fig2.update_xaxes(range=[-0.1,1.1])\n  fig2.update_yaxes(range=[-1.1, 1.1])\n\n  import seaborn as sns; sns.set()\n  fig2 = sns.jointplot(new_df[\"Polarity\"], new_df[\"Subjectivity\"], kind=\"hex\", height=7, space=0)\n  fig2.fig.subplots_adjust(top=0.9)\n  fig2.fig.suptitle(\"All Tweets\")\n\n\n  grouped_by_RW = new_df.groupby(\"RW_prediction\").agg(\n    count= (\"text\", len),\n    Subjectivity = (\"Subjectivity\", np.mean),\n    Polarity = (\"Polarity\", np.mean))\n  \n  fig1 = sns.jointplot(right_wing[\"Polarity\"], right_wing[\"Subjectivity\"], kind=\"hex\", height=7, space=0)\n  fig1.fig.subplots_adjust(top=0.9)\n  fig1.fig.suptitle(\"Right Wing Tweets\")\n\n  final = round(grouped_by_RW[\"count\"].iloc[1] / grouped_by_RW[\"count\"].iloc[0],3) * 100\n\n  return final, fig1, fig2\n\n\nfinal, fig1, fig2 = run_process_2(\"@MetPoliceUK\", \"2020-08-09\",\"2020-08-10\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfinal, fig1, fig2 = run_process_2(\"@GuidoFawkes\", \"2020-08-09\",\"2020-08-10\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfinal, fig1, fig2 = run_process_2(\"@OwenJones84\", \"2020-08-09\",\"2020-08-10\")"
  },
  {
    "objectID": "posts/lapd_and_prophet/lapd_and_prophet.html",
    "href": "posts/lapd_and_prophet/lapd_and_prophet.html",
    "title": "LAPD Call Prediction for Fun (and Prophet)",
    "section": "",
    "text": "Time series prediction is more complicated than I originally anticipated when I tackled the subject during my thesis - while you can treat the events independently, like geographic data, everything is related: what happened yesterday will affect what happened today, and a Friday in July is not the same as a Monday in October.\nThere are various weird and wonderful algorithms to cope with these complexities, but Facebook’s open source Prophet does a fantastic job of providing a “fire and forget” solution that just works.\nThis is the code extract from my Medium blog here."
  },
  {
    "objectID": "posts/lapd_and_prophet/lapd_and_prophet.html#data-cleaning-and-aggregating",
    "href": "posts/lapd_and_prophet/lapd_and_prophet.html#data-cleaning-and-aggregating",
    "title": "LAPD Call Prediction for Fun (and Prophet)",
    "section": "Data Cleaning and Aggregating",
    "text": "Data Cleaning and Aggregating\nWe’ll be using four years of LAPD call data, aggregated to hourly intervals. Prophet actually copes with various intervals quite well, so don’t worry too much about how you do yours: just try and keep regular intervals, without too many missings bits.\n\ndf_by_hour = pd.read_csv(\"/content/total_calls_per_hour.csv\")\n\n\ndf_by_hour[\"ds\"] = pd.date_range(min(df_by_hour[\"ds\"]), max(df_by_hour[\"ds\"]), freq='H')\n\nYour final cleaned data-set must contain the below two columns, ds and y. Everything else, Prophet will deal with.\n\ndf_by_hour.head()\n\n\n\n\n\n  \n    \n      \n      Dispatch Date_Dispatch Time\n      call volume\n      ds\n      y\n    \n  \n  \n    \n      0\n      2015-01-01 00:00:00\n      286\n      2015-01-01 00:00:00\n      286\n    \n    \n      1\n      2015-01-01 01:00:00\n      265\n      2015-01-01 01:00:00\n      265\n    \n    \n      2\n      2015-01-01 02:00:00\n      179\n      2015-01-01 02:00:00\n      179\n    \n    \n      3\n      2015-01-01 03:00:00\n      152\n      2015-01-01 03:00:00\n      152\n    \n    \n      4\n      2015-01-01 04:00:00\n      127\n      2015-01-01 04:00:00\n      127\n    \n  \n\n\n\n\n\ndf_by_hour.tail()\n\n\n\n\n\n  \n    \n      \n      Dispatch Date_Dispatch Time\n      call volume\n      ds\n      y\n    \n  \n  \n    \n      43819\n      2019-12-31 19:00:00\n      310\n      2019-12-31 19:00:00\n      310\n    \n    \n      43820\n      2019-12-31 20:00:00\n      320\n      2019-12-31 20:00:00\n      320\n    \n    \n      43821\n      2019-12-31 21:00:00\n      373\n      2019-12-31 21:00:00\n      373\n    \n    \n      43822\n      2019-12-31 22:00:00\n      354\n      2019-12-31 22:00:00\n      354\n    \n    \n      43823\n      2019-12-31 23:00:00\n      281\n      2019-12-31 23:00:00\n      281\n    \n  \n\n\n\n\n##Fitting and Deploying Prophet works similar to most Python sklearn type implementations - just fit the data and you’re off.\nHelpfully, it will also make you a data-frame containing future dates for you to predict on. It will also provide a breakdown of seasonality trends.\n\nm = Prophet()\nm.fit(df_by_hour)\n\nINFO:numexpr.utils:NumExpr defaulting to 2 threads.\n\n\n<fbprophet.forecaster.Prophet at 0x7fdbb146bdd8>\n\n\n\nfuture = m.make_future_dataframe(periods=365)\nforecast = m.predict(future)\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)\n\n\n\n\n\n\nfig = plot_plotly(m, forecast)  # This returns a plotly Figure\nfig.show()"
  },
  {
    "objectID": "posts/lapd_and_prophet/lapd_and_prophet.html#diagnostics-and-cross-validation",
    "href": "posts/lapd_and_prophet/lapd_and_prophet.html#diagnostics-and-cross-validation",
    "title": "LAPD Call Prediction for Fun (and Prophet)",
    "section": "Diagnostics and Cross Validation",
    "text": "Diagnostics and Cross Validation\nHelpfully, Prophet also contains cross-validation functionality - and performs quite well, very quickly!\n\ndf_cv = cross_validation(m, initial='730 days', period='180 days', horizon = '365 days')\ndf_cv.head()\n\nINFO:fbprophet:Making 5 forecasts with cutoffs between 2017-01-10 23:00:00 and 2018-12-31 23:00:00\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      ds\n      yhat\n      yhat_lower\n      yhat_upper\n      y\n      cutoff\n    \n  \n  \n    \n      0\n      2017-01-11 00:00:00\n      116.692077\n      89.859738\n      142.382745\n      79\n      2017-01-10 23:00:00\n    \n    \n      1\n      2017-01-11 01:00:00\n      95.572727\n      69.135035\n      122.850000\n      71\n      2017-01-10 23:00:00\n    \n    \n      2\n      2017-01-11 02:00:00\n      71.996010\n      45.771390\n      98.558667\n      55\n      2017-01-10 23:00:00\n    \n    \n      3\n      2017-01-11 03:00:00\n      49.332357\n      23.467584\n      76.244978\n      51\n      2017-01-10 23:00:00\n    \n    \n      4\n      2017-01-11 04:00:00\n      33.137479\n      4.387625\n      60.912735\n      41\n      2017-01-10 23:00:00\n    \n  \n\n\n\n\n\ndf_p = performance_metrics(df_cv)\ndf_p.head()\n\nINFO:fbprophet:Skipping MAPE because y close to 0\n\n\n\n\n\n\n  \n    \n      \n      horizon\n      mse\n      rmse\n      mae\n      mdape\n      coverage\n    \n  \n  \n    \n      0\n      36 days 12:00:00\n      2938.141411\n      54.204625\n      33.287400\n      0.167800\n      0.660046\n    \n    \n      1\n      36 days 13:00:00\n      2929.579619\n      54.125591\n      33.235735\n      0.167594\n      0.660731\n    \n    \n      2\n      36 days 14:00:00\n      2919.565265\n      54.033002\n      33.187913\n      0.167276\n      0.660959\n    \n    \n      3\n      36 days 15:00:00\n      2908.693562\n      53.932305\n      33.155190\n      0.166688\n      0.661187\n    \n    \n      4\n      36 days 16:00:00\n      2910.565179\n      53.949654\n      33.167673\n      0.166688\n      0.660959\n    \n  \n\n\n\n\n\nfig = plot_cross_validation_metric(df_cv, metric='rmse')"
  },
  {
    "objectID": "posts/lockdown_effect/index.html",
    "href": "posts/lockdown_effect/index.html",
    "title": "Learning R - Exploring the COVID Crime Effect in London",
    "section": "",
    "text": "The lockdown and social distancing measures that were brought in throughout the world to tackle COVID in 2020 have had a significant, widespread effect on crime. In this notebook, I use public London crime data on robbery and burglary to examine where this “COVID crime shift” was strongest, and whether any specific drivers or correlates can be identified. I use three years of Metropolitan Police Service data from data.police.uk.\nThe findings suggest that the relative change in burglary and robbery in April and May 2020 was heavily affected by local characteristics: areas with a high residential population saw the sharpest decreases in burglary (likely due to a reduction in available targets) while the reduction in robberies instead seem to be driven by geographic features and indicators of deprivation (potentially suggesting more available targets for robbery in communities least able to work for from home).\nThe primary purpose of this exercise was to learn R - I’ve previously worked entirely in Python, which is more than sufficient 99% of the time, but has at times proved a blocker when I want to tackle some more experimental geospatial and statistical methods. With that in mind, this is likely to be a little messy, and I’ll aim to condense my main lessons into a blog post in the future. The models are not heavily tuned (aiming to explore correlates rather than provide accurate predictions) and there are likely to be correlation between our various predictors - as such these should not be taken to suggest direct causation.\nThe full code and data for this exercise are available on my Github repo. I’m hoping to summarise my key lessons in the Python to R journey in Medium post in the next few weeks."
  },
  {
    "objectID": "posts/lockdown_effect/index.html#ingest-data",
    "href": "posts/lockdown_effect/index.html#ingest-data",
    "title": "Learning R - Exploring the COVID Crime Effect in London",
    "section": "Ingest Data",
    "text": "Ingest Data\nFor this exercise, I’ll be importing crime and robbery data by MSOA.MSOAs are geographical units specifically designed for analysis, and to be comparable: they all have an average population of just over 8,000. There is a compromise here between smaller geographical units (that create more variance that may help us identify predictors), but the necessity for enough crime per unit to identify meaningful trends - MSOAs should be suitable.\n\n\n\nTo build our process, we’ll start by taking one month of crime data, exploring it, and writing all our steps for automation.\n\ntest_df <- read.csv(\"crimes/2018-01/2018-01-metropolitan-street.csv\")\n\nOur crime data is categorised according to the Home Office major crime types, and like Python, we can list them all through the “unique” function. Here I’ll be focusing on robbery and burglary: two crime types that are heavily reliant on encountering victim’s in public spaces, and as such should be affected by the “COVID effect”.\nTo avoid this getting particularly computationally intensive, let’s write a function to pull out robberies and burglaries, and assign them a specific MSOA. Then we can iterate over all our months and get monthly counts for each offence type.\n\nsubset_df <- filter(test_df, Crime.type==\"Burglary\" | Crime.type==\"Robbery\")\nhead(subset_df)\n\n\n\n\n\n\n\nCrime.ID\n\n\nMonth\n\n\nReported.by\n\n\nFalls.within\n\n\nLongitude\n\n\nLatitude\n\n\nLocation\n\n\nLSOA.code\n\n\nLSOA.name\n\n\nCrime.type\n\n\nLast.outcome.category\n\n\nContext\n\n\n\n\n\n\n628e0d673aa1b6a70479342a64b02884499df85b18dcd63cc9bff3cff9f704bc\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.140035\n\n\n51.58911\n\n\nOn or near Beansland Grove\n\n\nE01000027\n\n\nBarking and Dagenham 001A\n\n\nBurglary\n\n\nOffender sent to prison\n\n\nNA\n\n\n\n\nf8e9db16dca534a83493198a838567aa5adc9dd56496edc2fff5bb4c62b8303e\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.140035\n\n\n51.58911\n\n\nOn or near Beansland Grove\n\n\nE01000027\n\n\nBarking and Dagenham 001A\n\n\nBurglary\n\n\nInvestigation complete; no suspect identified\n\n\nNA\n\n\n\n\ncc34822074b130f141f16d02fdb2d500c86e22ae18324b43a3231b381af3f45c\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.135554\n\n\n51.58499\n\n\nOn or near Rose Lane\n\n\nE01000027\n\n\nBarking and Dagenham 001A\n\n\nBurglary\n\n\nStatus update unavailable\n\n\nNA\n\n\n\n\n10de581c3cd0a8c9b970824cd7589d13148d63a70b3115d95ef6c24dc0bd2c3b\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.140035\n\n\n51.58911\n\n\nOn or near Beansland Grove\n\n\nE01000027\n\n\nBarking and Dagenham 001A\n\n\nBurglary\n\n\nStatus update unavailable\n\n\nNA\n\n\n\n\n50ad5d2dfea24afec9e17218db62b3d29786775db1060634ae7d4a6e7cafc3ff\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.127794\n\n\n51.58419\n\n\nOn or near Hope Close\n\n\nE01000028\n\n\nBarking and Dagenham 001B\n\n\nBurglary\n\n\nStatus update unavailable\n\n\nNA\n\n\n\n\n95abc6eb0b755c9250d19bbe0062fcd4a509b701964d89667401c9dc96ca257d\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.138439\n\n\n51.57850\n\n\nOn or near Geneva Gardens\n\n\nE01000029\n\n\nBarking and Dagenham 001C\n\n\nBurglary\n\n\nInvestigation complete; no suspect identified\n\n\nNA\n\n\n\n\n\n\n\n\nOur single month of data contains 10,501 crimes.\nWe now need to link this to our spatial data. We use the MSOA borders provided by MOPAC, and use the UK National Grid coordinate system. Police.uk does not use that system, so we’ll need to reproject our crime data.\n\nlsoa_borders <- st_read(\"msoa_borders/MSOA_2011_London_gen_MHW.tab\", crs=27700)\n\nReading layer `MSOA_2011_London_gen_MHW' from data source \n  `D:\\Dropbox\\Data Projects\\Covid_crime_shift\\msoa_borders\\MSOA_2011_London_gen_MHW.tab' \n  using driver `MapInfo File'\nSimple feature collection with 983 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB 1936 / British National Grid\n\nplot(lsoa_borders)\n\n\n\n\nBefore we can link our crimes to MSOA, we’ll need to ensure identical coordinate systems, and remove any non-geolocated values we’ll need to erase any missing values (while checking we retain enough data for analysis.)\n\n#count missing values in the longitude column\nprint(\"Missing values identified:\")\n\n[1] \"Missing values identified:\"\n\nsum(is.na(subset_df[\"Longitude\"]))\n\n[1] 82\n\n\nThankfully, we only identify 82 crimes which we need to remove, leaving plenty for analysis.\n\nclean_df <- subset_df[!rowSums(is.na(subset_df[\"Longitude\"])), ]\n\nWe can now convert our crime data to spacial data, using our longitude and latitude coordinates - this allows us to quickly plot our data, and confirm it looks right.\n\n\n\n\n\nWarning: plotting the first 9 out of 12 attributes; use max.plot = 12 to plot\nall\n\n\n\n\n\nWith our data now mapped, we ensure everything is aligned to the appropriate coordinate system, and assign each crime to an MSOA from our data - the data is then aggregated into a monthly MSOA crime count, to which we assign our monthly date.\n\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\n\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\nMSOA11CD\n\n\nCrime.type\n\n\ncount_by_msoa\n\n\nMonth\n\n\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-01\n\n\n\n\nE02000002\n\n\nBurglary\n\n\n9\n\n\n2018-01\n\n\n\n\nE02000002\n\n\nRobbery\n\n\n1\n\n\n2018-01\n\n\n\n\nE02000003\n\n\nBurglary\n\n\n11\n\n\n2018-01\n\n\n\n\nE02000003\n\n\nRobbery\n\n\n2\n\n\n2018-01\n\n\n\n\nE02000004\n\n\nBurglary\n\n\n10\n\n\n2018-01\n\n\n\n\n\n\n\n\nBringing together all the code so far into a function, we can create an pipeline to generate our crime count per MSOA time series for the entirety of our dataset.\n\n\n\nFor this project, I haven’t used the Police.uk API (which would have enabled me to automate the downloads and query the data directly) - as such, we have to iterate over our subfolders, ingesting our CSV data and running through our process.\n\n\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\nMSOA11CD\n\n\nCrime.type\n\n\ncount_by_msoa\n\n\nMonth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-01\n\n\n\n\nE02000002\n\n\nBurglary\n\n\n9\n\n\n2018-01\n\n\n\n\nE02000002\n\n\nRobbery\n\n\n1\n\n\n2018-01\n\n\n\n\nE02000003\n\n\nBurglary\n\n\n11\n\n\n2018-01\n\n\n\n\nE02000003\n\n\nRobbery\n\n\n2\n\n\n2018-01\n\n\n\n\n\n\n\n\nWe now have a combined dataframe of 71,848 rows, from January 2018 through December 2020.\n\n#saving file to CSV\n#write.csv(empty_df,\"msoa_crime_matrix.csv\")"
  },
  {
    "objectID": "posts/lockdown_effect/index.html#predict-trend-by-msoa",
    "href": "posts/lockdown_effect/index.html#predict-trend-by-msoa",
    "title": "Learning R - Exploring the COVID Crime Effect in London",
    "section": "2. Predict trend by MSOA",
    "text": "2. Predict trend by MSOA\n\nVisualisation and Exploration\nWith our data now cleaned and aggregated, we can focus on the more interesting part - forecasting our “expected” pandemic crime, and examining how much it diverges from our “actual” crime.\n\nempty_df <- read.csv(\"msoa_crime_matrix.csv\")\nempty_df <- empty_df[2:70848,2:5]\nhead(empty_df)\n\n\n\n\n\n\n\n\n\nMSOA11CD\n\n\nCrime.type\n\n\ncount_by_msoa\n\n\nMonth\n\n\n\n\n\n\n2\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-01\n\n\n\n\n3\n\n\nE02000002\n\n\nBurglary\n\n\n9\n\n\n2018-01\n\n\n\n\n4\n\n\nE02000002\n\n\nRobbery\n\n\n1\n\n\n2018-01\n\n\n\n\n5\n\n\nE02000003\n\n\nBurglary\n\n\n11\n\n\n2018-01\n\n\n\n\n6\n\n\nE02000003\n\n\nRobbery\n\n\n2\n\n\n2018-01\n\n\n\n\n7\n\n\nE02000004\n\n\nBurglary\n\n\n10\n\n\n2018-01\n\n\n\n\n\n\n\n\nBefore going any further, let’s use this to explore and visualise the distribution of robbery and burglary across time and space during our “pre-pandemic” period, in March 2020 - based on London mobility indicators, this is when movement accross London began to be heavily affected, and the disruption was most notable in April\n\n\n\nLondon mobility data\n\n\n\nburglary_df<-empty_df\n\n#add a \"1\" so our month can be converted to a full date\nburglary_df$DateString <- paste(burglary_df$Month, \"-01\", sep=\"\")\n\n#convert to date format\nburglary_df$DateClean <- ymd(burglary_df$DateString)\n\n#filter out only burglary prior to the pandemic\nburglaryExplore <- filter(burglary_df,  DateClean < \"2020-03-01\" & Crime.type==\"Burglary\")\n\nhead(burglaryExplore)\n\n\n\n\n\n\n\nMSOA11CD\n\n\nCrime.type\n\n\ncount_by_msoa\n\n\nMonth\n\n\nDateString\n\n\nDateClean\n\n\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\nE02000002\n\n\nBurglary\n\n\n9\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\nE02000003\n\n\nBurglary\n\n\n11\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\nE02000004\n\n\nBurglary\n\n\n10\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\nE02000005\n\n\nBurglary\n\n\n6\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\nE02000007\n\n\nBurglary\n\n\n7\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\n\n\n\n\nLooking at the aggregate counts of burglary across London, a visual observation suggests yearly trends (which we’ll have to consider in our forecast), which sharp peaks during the Winter months and the lowest numbers in summer (when the days are longest).\n\n#group burglary count by months and plot\nburglary_by_month <- burglaryExplore %>%\n  group_by(DateClean) %>%\n  summarize(total_burglaries = sum(count_by_msoa))\n\nggplot(burglary_by_month, aes(x=DateClean, y=total_burglaries)) +\n  geom_line()\n\n\n\n\nTo observe how crime counts are distributed in space, let’s map both counts by MSOA. As previously mentioned, MSOAs are designed to be comparable units, at least from a population perspective - we don’t need to produce per population rates.\n\nburglary_by_msoa <- burglaryExplore %>%\n  group_by(MSOA11CD) %>%\n  summarize(total_burglaries = sum(count_by_msoa))\n\n#we join our burglary counts to their geographic msoa\nburglary_map <- left_join(lsoa_borders, burglary_by_msoa, by = \"MSOA11CD\")\n\n#user brewer colour palette https://colorbrewer2.org\npal <- brewer.pal(5,\"BuGn\")\n\n#create our map, and add the layout options\nburglary_map <-tm_shape(burglary_map) +\n  tm_fill(col = \"total_burglaries\", title = \"Total Burglary Count by MSOA\", style=\"quantile\", palette=\"BuGn\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\nrobbery_df<-empty_df\n\nrobbery_df$DateString <- paste(robbery_df$Month, \"-01\", sep=\"\")\nrobbery_df$DateClean <- ymd(robbery_df$DateString)\nrobberyExplore <- filter(robbery_df,  DateClean < \"2020-03-01\" & Crime.type==\"Robbery\")\n\nrobbery_by_msoa <- robberyExplore %>%\n  group_by(MSOA11CD) %>%\n  summarize(total_robberies = sum(count_by_msoa))\n\nrobbery_map <- left_join(lsoa_borders, robbery_by_msoa, by = \"MSOA11CD\")\n\npal <- brewer.pal(5,\"BuGn\")\n\n\nrobbery_map <-tm_shape(robbery_map) +\n  tm_fill(col = \"total_robberies\", title = \"Total Robbery Count by MSOA\", style=\"quantile\", palette=\"BuGn\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\n#arrange the maps together\ntmap_arrange(burglary_map, robbery_map, nrow = 2)\n\n\n\n\nWe notice that robbery is noticeably more concentrated in central London, with burglary remaining quite common across the city. That said, there are also obvious spatial patterns here - these crimes are clustered in certain geographies.\n\n\nModelling\nWe can now begin the forecasting process. To design our process, we’ll start by focusing on a single MSOA - the first in our dataset, E02000001, or the City of London.\n\nsingle_msoa_df <- filter(empty_df, MSOA11CD == \"E02000001\" & Crime.type==\"Burglary\")\n\n#we add a 01 to our date to ensure R recognises the date format\nsingle_msoa_df$DateString <- paste(single_msoa_df$Month, \"-01\")\n\n\nsingle_msoa_df$DateClean <- ymd(single_msoa_df$DateString)\nsingle_msoa_df\n\n\n\n\n\n\n\nMSOA11CD\n\n\nCrime.type\n\n\ncount_by_msoa\n\n\nMonth\n\n\nDateString\n\n\nDateClean\n\n\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-01\n\n\n2018-01 -01\n\n\n2018-01-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-02\n\n\n2018-02 -01\n\n\n2018-02-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2018-03\n\n\n2018-03 -01\n\n\n2018-03-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-04\n\n\n2018-04 -01\n\n\n2018-04-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n4\n\n\n2018-05\n\n\n2018-05 -01\n\n\n2018-05-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n0\n\n\n2018-06\n\n\n2018-06 -01\n\n\n2018-06-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n3\n\n\n2018-07\n\n\n2018-07 -01\n\n\n2018-07-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2018-08\n\n\n2018-08 -01\n\n\n2018-08-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-09\n\n\n2018-09 -01\n\n\n2018-09-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n0\n\n\n2018-10\n\n\n2018-10 -01\n\n\n2018-10-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-11\n\n\n2018-11 -01\n\n\n2018-11-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n0\n\n\n2018-12\n\n\n2018-12 -01\n\n\n2018-12-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2019-01\n\n\n2019-01 -01\n\n\n2019-01-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n7\n\n\n2019-02\n\n\n2019-02 -01\n\n\n2019-02-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2019-03\n\n\n2019-03 -01\n\n\n2019-03-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n3\n\n\n2019-04\n\n\n2019-04 -01\n\n\n2019-04-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n6\n\n\n2019-05\n\n\n2019-05 -01\n\n\n2019-05-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n6\n\n\n2019-06\n\n\n2019-06 -01\n\n\n2019-06-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2019-07\n\n\n2019-07 -01\n\n\n2019-07-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2019-08\n\n\n2019-08 -01\n\n\n2019-08-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2019-09\n\n\n2019-09 -01\n\n\n2019-09-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n5\n\n\n2019-10\n\n\n2019-10 -01\n\n\n2019-10-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2019-11\n\n\n2019-11 -01\n\n\n2019-11-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2019-12\n\n\n2019-12 -01\n\n\n2019-12-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n4\n\n\n2020-01\n\n\n2020-01 -01\n\n\n2020-01-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-02\n\n\n2020-02 -01\n\n\n2020-02-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n0\n\n\n2020-03\n\n\n2020-03 -01\n\n\n2020-03-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n0\n\n\n2020-04\n\n\n2020-04 -01\n\n\n2020-04-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-05\n\n\n2020-05 -01\n\n\n2020-05-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-06\n\n\n2020-06 -01\n\n\n2020-06-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2020-07\n\n\n2020-07 -01\n\n\n2020-07-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-08\n\n\n2020-08 -01\n\n\n2020-08-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-09\n\n\n2020-09 -01\n\n\n2020-09-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-10\n\n\n2020-10 -01\n\n\n2020-10-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-11\n\n\n2020-11 -01\n\n\n2020-11-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2020-12\n\n\n2020-12 -01\n\n\n2020-12-01\n\n\n\n\n\n\n\n\nFrom a forecasting/time-series perspective, this is a very small dataset - 36 monthly observations. We will be shrinking this further to only 26 by focusing on data prior to March 2020, when the COVID crime impact is felt. This significantly limits our forecasting options, and will impact accuracy, if we treat each MSOA in isolation - we could explore some sort of Vector Autoregressive Model to limit this, but given that we’re then going to be exploring the error of all our models in aggregation, this isn’t crucial. Our focus is on models that we can accurately deploy without needing to tune each of them individually, and that can capture the seasonal trend, and generate reliable predictions on our limited dataset.\nGiven these limitations, I’ve opted for the Prophet algorith. While it’s more opaque than a auto-arima or VAR model, it works well with monthly data, and extracting seasonal trends. It also requires very little tuning.\nAs such, we’ll extract our “training set” prior to March, and start forecasting.\n\ntraining_set <- filter(single_msoa_df, DateClean < \"2020-03-01\")\n\ntraining_df <- tibble(\n  ds=training_set$DateClean,\n  y=training_set$count_by_msoa\n)\nhead(training_df)\n\n\n\n\n\n\n\nds\n\n\ny\n\n\n\n\n\n\n2018-01-01\n\n\n1\n\n\n\n\n2018-02-01\n\n\n1\n\n\n\n\n2018-03-01\n\n\n2\n\n\n\n\n2018-04-01\n\n\n1\n\n\n\n\n2018-05-01\n\n\n4\n\n\n\n\n2018-06-01\n\n\n0\n\n\n\n\n\n\n\n\n\nlibrary(prophet)\n\nLoading required package: Rcpp\n\n\nLoading required package: rlang\n\n\n\nAttaching package: 'rlang'\n\n\nThe following object is masked from 'package:Metrics':\n\n    ll\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,\n    flatten_lgl, flatten_raw, invoke, splice\n\nm <- prophet(training_df)\n\nDisabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.\n\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n\nn.changepoints greater than number of observations. Using 19\n\n\nFor now, we’ll forecast on a 6 month horizon - we obviously wouldn’t expect it to be accurate that far into the future.\n\n#prophet generates a future dataframe using our data, for 6 mperiods\nfuture <- make_future_dataframe(m, periods = 6, freq = 'month')\n\n\nforecast <- predict(m, future)\n\nplot(m, forecast)\n\n\n\n\nAs we can see, the model seems consistent on a short horizon, and gets very wide as it goes further into the future. More importantly however, it has extracted a yearly seasonal compontent - the summer decrease we identified previously - as well as a long term trend.\n\nprophet_plot_components(m, forecast)\n\n\n\n\nThese predictions seem far-fetched, but remember we will be observing a London wide error rate. As such, we must now isolate our “pandemic period” - which we define as April and May 2020 - and compare the predicted crime counts to the actual crime counts to obtain a metric of our “COVID crime shift”, or our error rate.\n\nforecast$Month <- month(forecast$ds)\nforecast$Year <- year(forecast$ds)\n\n\nthis_year <- filter(forecast, Year > 2019)\npeak_pandemic <- filter(this_year, Month== 4 | Month== 5 )\n\npredictionPivot <- peak_pandemic %>%\n  group_by(Month) %>%\n  summarize(predicted_burglary = mean(yhat))\n\n\nsingle_msoa_df$MonthNum <- month(single_msoa_df$DateClean)\nsingle_msoa_df$YearNum <- year(single_msoa_df$DateClean)\n\nthis_year_actual <- filter(single_msoa_df, YearNum > 2019)\npeak_pandemic_actual <- filter(this_year_actual, MonthNum== 4 | MonthNum== 5 )\n\nactual_burglary <- sum(peak_pandemic_actual$count_by_msoa)\npred_burglary <- sum(predictionPivot$predicted_burglary)\n\nerror <- actual_burglary - pred_burglary\npercentage_error <- error / pred_burglary \n\nprint(\"Burglary Count\")\n\n[1] \"Burglary Count\"\n\nprint(actual_burglary)\n\n[1] 1\n\nprint(\"Predicted\")\n\n[1] \"Predicted\"\n\nprint(pred_burglary)\n\n[1] 7.625019\n\nprint(\"Actual Error\")\n\n[1] \"Actual Error\"\n\nprint(error)\n\n[1] -6.625019\n\nprint(\"Percentage Error\")\n\n[1] \"Percentage Error\"\n\nprint(percentage_error)\n\n[1] -0.8688528\n\n\nIn this MSOA, our model predicted nearly 8 burglaries would occur in these two months, based on pre-pandemic trends. In reality, 1 took place - a large error rate, suggesting a strong “COVID effect”.\nThis process can now be replicated for every MSOA in London, to obtain this metric for each MSOA.\n\nlength(unique(empty_df$MSOA11CD))\n\n[1] 984\n\n\n\n## NULL\n\n\nhead(msoa_error_tibble)\n\n\n\n\n\n\n\nMSOA11CD\n\n\nburglaryActual\n\n\nburglaryPredicted\n\n\nburglaryError\n\n\nburglaryPercentError\n\n\nrobberyActual\n\n\nrobberyPredicted\n\n\nrobberyError\n\n\nrobberyPercentError\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE02000001\n\n\n1\n\n\n7.62501853994038\n\n\n-6.62501853994038\n\n\n-0.868852777896614\n\n\n1\n\n\n-1.97666206600358\n\n\n2.97666206600358\n\n\n-1.50590336972561\n\n\n\n\nE02000002\n\n\n8\n\n\n-9.23326713435216\n\n\n17.2332671343522\n\n\n-1.86643220472157\n\n\n0\n\n\n1.12957561845153\n\n\n-1.12957561845153\n\n\n-1\n\n\n\n\nE02000003\n\n\n11\n\n\n12.3480006370534\n\n\n-1.34800063705343\n\n\n-0.109167522473914\n\n\n10\n\n\n6.9224283913514\n\n\n3.0775716086486\n\n\n0.444579768061392\n\n\n\n\nE02000004\n\n\n2\n\n\n-4.71960263280976\n\n\n6.71960263280976\n\n\n-1.42376448942892\n\n\n0\n\n\n-1.27129225183737\n\n\n1.27129225183737\n\n\n-1\n\n\n\n\nE02000005\n\n\n4\n\n\n4.58490182624086\n\n\n-0.584901826240862\n\n\n-0.127571286890655\n\n\n1\n\n\n9.21402738381341\n\n\n-8.21402738381341\n\n\n-0.89146982547971\n\n\n\n\n\n\n\n\nOur process has completed: we have a “COVID shift” measure for all of London."
  },
  {
    "objectID": "posts/lockdown_effect/index.html#measuring-local-covid-crime-shifts",
    "href": "posts/lockdown_effect/index.html#measuring-local-covid-crime-shifts",
    "title": "Learning R - Exploring the COVID Crime Effect in London",
    "section": "3. Measuring Local COVID Crime Shifts",
    "text": "3. Measuring Local COVID Crime Shifts\nWe now need to use our forecasts to measure the “error” - this should provide an indication of the “COVID Crime Shift”, or how much the actual crime diverted from the previous forecasts.\nI explored various avenues for this: the ideal solution would be a relative rate of the error, as MSOAs with large crime numbers will likely generate large errors, and so a rate would be ideal, though this is complicated by our erratic prediction and mix of positive and negative numbers.\nOur final solution has explored two options: - the absolute error number - the relative error once the crime and predictions have been transformed (by adding 50)\n\\[\nactual_{k} = actual + 50\n\\]\n\\[\npredicted_{k} = predicted + 50\n\\]\n\\[\nRPD = \\frac{(actual_{k} - predicted_{k})}  {(actual_{k} + predicted_{k})/2}\n\\]\nWe visualise and describe these statistics first to ensure they appear sensible.\n\n\n\n\nmsoa_error_tibble <- read_csv(\"msoa_error_table2.csv\")\n\nRows: 980 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): MSOA11CD\ndbl (8): burglaryActual, burglaryPredicted, burglaryError, burglaryPercentEr...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmsoa_error_tibble[,2:9] <- lapply(msoa_error_tibble[,2:9], as.numeric)\n\nmsoa_error_tibble <- msoa_error_tibble[2:980, ]\n\nmsoa_error_tibble <- left_join(msoa_error_tibble, robbery_by_msoa, by = \"MSOA11CD\")\nmsoa_error_tibble <- left_join(msoa_error_tibble, burglary_by_msoa, by = \"MSOA11CD\")\n\n\nmsoa_error_tibble$RPDBurglary <- (msoa_error_tibble$burglaryActual - msoa_error_tibble$burglaryPredicted)/((msoa_error_tibble$burglaryPredicted + msoa_error_tibble$burglaryActual)/2)\n\nmsoa_error_tibble$RPDRobbery <- (msoa_error_tibble$robberyActual - msoa_error_tibble$robberyPredicted)/((msoa_error_tibble$robberyPredicted + msoa_error_tibble$robberyActual)/2)\n\nmsoa_error_tibble$robberyActualShifted <- msoa_error_tibble$robberyActual + 50\nmsoa_error_tibble$robberyPredictedShifted <- msoa_error_tibble$robberyPredicted + 50\n\n\nmsoa_error_tibble$RPDRobberyShifted <- (msoa_error_tibble$robberyActualShifted - msoa_error_tibble$robberyPredictedShifted)/((msoa_error_tibble$robberyPredictedShifted + msoa_error_tibble$robberyActualShifted)/2)\n\nmsoa_error_tibble$burglaryActualShifted <- msoa_error_tibble$burglaryActual + 50\nmsoa_error_tibble$burglaryPredictedShifted <- msoa_error_tibble$burglaryPredicted + 50\n\n\nmsoa_error_tibble$RPDburglaryShifted <- (msoa_error_tibble$burglaryActualShifted - msoa_error_tibble$burglaryPredictedShifted)/((msoa_error_tibble$burglaryPredictedShifted + msoa_error_tibble$burglaryActualShifted)/2)\n\n\nprint(\"Burglary Error\")\n\n[1] \"Burglary Error\"\n\nsummary(msoa_error_tibble$burglaryError)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-97.191 -12.870  -5.271  -6.090   2.111  48.727 \n\nprint(\"Burglary Relative Error\")\n\n[1] \"Burglary Relative Error\"\n\nsummary(msoa_error_tibble$RPDburglaryShifted)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.81266 -0.20689 -0.08826 -0.07976  0.03612  1.29467 \n\nprint(\"Robbery Error\")\n\n[1] \"Robbery Error\"\n\nsummary(msoa_error_tibble$robberyError)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-309.618   -6.577   -2.150   -3.840    2.205   25.564 \n\nprint(\"Robbery Relative Error\")\n\n[1] \"Robbery Relative Error\"\n\nsummary(msoa_error_tibble$RPDRobberyShifted)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-1.45491 -0.11808 -0.04081 -0.04876  0.04309  0.62020 \n\n\nAs we can see, the average London MSOA experienced a negative COVID crime shift for both burglary and robbery, but this is far from equally distributed - at the extremes, some areas actually see large increases on our predicted values.\n\nburg_hist <- ggplot(msoa_error_tibble, aes(x=burglaryError)) + geom_histogram()\nrob_hist <-ggplot(msoa_error_tibble, aes(x=robberyError)) + geom_histogram()\nburg_r_hist <- ggplot(msoa_error_tibble, aes(x=RPDburglaryShifted)) + geom_histogram()\nrob_r_hist <- ggplot(msoa_error_tibble, aes(x=RPDRobberyShifted)) + geom_histogram()\nscatter <- ggplot(msoa_error_tibble, aes(x = RPDRobberyShifted, y = RPDburglaryShifted)) +\n  geom_point()\n\nr_scatter <- ggplot(msoa_error_tibble, aes(x = robberyError, y = burglaryError)) +\n  geom_point()\n\nggarrange(rob_hist, burg_hist, rob_r_hist, burg_r_hist,scatter, r_scatter, ncol=2, nrow=3 )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nOur shifted relative error rate seems to function as intended: while there are still outliers, they are more concentrated than they are for the pure error term, and the overall distribution is more focused, while still indicating the direction and relative strength of our COVID effect.\nLet’s map this effect visually, and see if any particular areas stand out.\n\n#re-ingest our geographic MSOA borders\nmsoa_borders <- st_read(\"msoa_borders/MSOA_2011_London_gen_MHW.tab\", crs=27700)\n\nReading layer `MSOA_2011_London_gen_MHW' from data source \n  `D:\\Dropbox\\Data Projects\\Covid_crime_shift\\msoa_borders\\MSOA_2011_London_gen_MHW.tab' \n  using driver `MapInfo File'\nSimple feature collection with 983 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB 1936 / British National Grid\n\ngeographic_error_map <- left_join(msoa_borders, msoa_error_tibble, by = \"MSOA11CD\")\n\nburg_map <- tm_shape(geographic_error_map) +\n  tm_fill(col = \"robberyError\", title = \"Robbery Error\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\nrob_map <-tm_shape(geographic_error_map) +\n  tm_fill(col = \"burglaryError\", title = \"Burglary  Error\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\nburg_map_rate <- tm_shape(geographic_error_map) +\n  tm_fill(col = \"RPDRobberyShifted\", title = \"Robbery Error Relative\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\nrob_map_rate <-tm_shape(geographic_error_map) +\n  tm_fill(col = \"RPDburglaryShifted\", title = \"Burglary  Error Relative\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\ntmap_arrange(burg_map, rob_map, burg_map_rate, rob_map_rate , nrow = 2, ncol=2)\n\nVariable(s) \"robberyError\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nVariable(s) \"burglaryError\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nVariable(s) \"RPDRobberyShifted\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nVariable(s) \"RPDburglaryShifted\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nIt’s hard to identify any obvious effect visually, but we do notice that while central London sees some very strong reductions, it also sees some increases. Conversely, the outskirts of London (notably to the south and West) are a near continuous area of large decreases. The effect does vary by offence type, but the pattern seen in South and West London appears broadly consistent."
  },
  {
    "objectID": "posts/lockdown_effect/index.html#identifying-correlates-and-modelling",
    "href": "posts/lockdown_effect/index.html#identifying-correlates-and-modelling",
    "title": "Learning R - Exploring the COVID Crime Effect in London",
    "section": "Identifying Correlates and Modelling",
    "text": "Identifying Correlates and Modelling\nWe’ve identified that the COVID crime effect was felt unequally accross London, and varies by offence type. To finalise our project, we will be linking our data to demographic data provided by MOPAC, and aiming to use it to identify correlates to our “covid shift”, and hopefully build models disentangling the effect.\n\nlibrary(readxl)\n#ingest ATLAS\nmsoa_atlas <- read_excel(\"msoa_atlas/msoa-data.xls\")\n\nNew names:\n• `House Prices Sales 2011` -> `House Prices Sales 2011...129`\n• `House Prices Sales 2011` -> `House Prices Sales 2011...130`\n\n#join by MSOA\ngeographic_msoa_matrix <- left_join(geographic_error_map, msoa_atlas, by = \"MSOA11CD\")\n\n#convert to tibble\nmsoa_matrix_tbl <- as_tibble(geographic_msoa_matrix)\nwrite_csv(msoa_matrix_tbl, \"msoa_matrix.csv\")\n\n\n#select only numeric data\nmsoa_matrix_numeric <-dplyr::select_if(msoa_matrix_tbl, is.numeric)\nhead(msoa_matrix_numeric)\n\n\n\n\n\n\n\nUsualRes\n\n\nHholdRes\n\n\nComEstRes\n\n\nPopDen\n\n\nHholds\n\n\nAvHholdSz\n\n\nburglaryActual\n\n\nburglaryPredicted\n\n\nburglaryError\n\n\nburglaryPercentError\n\n\nrobberyActual\n\n\nrobberyPredicted\n\n\nrobberyError\n\n\nrobberyPercentError\n\n\ntotal_robberies\n\n\ntotal_burglaries\n\n\nRPDBurglary\n\n\nRPDRobbery\n\n\nrobberyActualShifted\n\n\nrobberyPredictedShifted\n\n\nRPDRobberyShifted\n\n\nburglaryActualShifted\n\n\nburglaryPredictedShifted\n\n\nRPDburglaryShifted\n\n\nAge Structure (2011 Census) All Ages\n\n\nAge Structure (2011 Census) 0-15\n\n\nAge Structure (2011 Census) 16-29\n\n\nAge Structure (2011 Census) 30-44\n\n\nAge Structure (2011 Census) 45-64\n\n\nAge Structure (2011 Census) 65+\n\n\nAge Structure (2011 Census) Working-age\n\n\nMid-year Estimate totals All Ages 2002\n\n\nMid-year Estimate totals All Ages 2003\n\n\nMid-year Estimate totals All Ages 2004\n\n\nMid-year Estimate totals All Ages 2005\n\n\nMid-year Estimate totals All Ages 2006\n\n\nMid-year Estimate totals All Ages 2007\n\n\nMid-year Estimate totals All Ages 2008\n\n\nMid-year Estimate totals All Ages 2009\n\n\nMid-year Estimate totals All Ages 2010\n\n\nMid-year Estimate totals All Ages 2011\n\n\nMid-year Estimate totals All Ages 2012\n\n\nMid-year Estimates 2012, by age % 0 to 14\n\n\nMid-year Estimates 2012, by age % 15-64\n\n\nMid-year Estimates 2012, by age % 65+\n\n\nMid-year Estimates 2012, by age 0-4\n\n\nMid-year Estimates 2012, by age 5-9\n\n\nMid-year Estimates 2012, by age 10-14\n\n\nMid-year Estimates 2012, by age 15-19\n\n\nMid-year Estimates 2012, by age 20-24\n\n\nMid-year Estimates 2012, by age 25-29\n\n\nMid-year Estimates 2012, by age 30-34\n\n\nMid-year Estimates 2012, by age 35-39\n\n\nMid-year Estimates 2012, by age 40-44\n\n\nMid-year Estimates 2012, by age 45-49\n\n\nMid-year Estimates 2012, by age 50-54\n\n\nMid-year Estimates 2012, by age 55-59\n\n\nMid-year Estimates 2012, by age 60-64\n\n\nMid-year Estimates 2012, by age 65-69\n\n\nMid-year Estimates 2012, by age 70-74\n\n\nMid-year Estimates 2012, by age 75-79\n\n\nMid-year Estimates 2012, by age 80-84\n\n\nMid-year Estimates 2012, by age 85-89\n\n\nMid-year Estimates 2012, by age 90+\n\n\nHouseholds (2011) All Households\n\n\nHousehold Composition (2011) Numbers Couple household with dependent children\n\n\nHousehold Composition (2011) Numbers Couple household without dependent children\n\n\nHousehold Composition (2011) Numbers Lone parent household\n\n\nHousehold Composition (2011) Numbers One person household\n\n\nHousehold Composition (2011) Numbers Other household Types\n\n\nHousehold Composition (2011) Percentages Couple household with dependent children\n\n\nHousehold Composition (2011) Percentages Couple household without dependent children\n\n\nHousehold Composition (2011) Percentages Lone parent household\n\n\nHousehold Composition (2011) Percentages One person household\n\n\nHousehold Composition (2011) Percentages Other household Types\n\n\nEthnic Group (2011 Census) White\n\n\nEthnic Group (2011 Census) Mixed/multiple ethnic groups\n\n\nEthnic Group (2011 Census) Asian/Asian British\n\n\nEthnic Group (2011 Census) Black/African/Caribbean/Black British\n\n\nEthnic Group (2011 Census) Other ethnic group\n\n\nEthnic Group (2011 Census) BAME\n\n\nEthnic Group (2011 Census) White (%)\n\n\nEthnic Group (2011 Census) Mixed/multiple ethnic groups (%)\n\n\nEthnic Group (2011 Census) Asian/Asian British (%)\n\n\nEthnic Group (2011 Census) Black/African/Caribbean/Black British (%)\n\n\nEthnic Group (2011 Census) Other ethnic group (%)\n\n\nEthnic Group (2011 Census) BAME (%)\n\n\nCountry of Birth (2011) United Kingdom\n\n\nCountry of Birth (2011) Not United Kingdom\n\n\nCountry of Birth (2011) United Kingdom (%)\n\n\nCountry of Birth (2011) Not United Kingdom (%)\n\n\nHousehold Language (2011) At least one person aged 16 and over in household has English as a main language\n\n\nHousehold Language (2011) No people in household have English as a main language\n\n\nHousehold Language (2011) % of people aged 16 and over in household have English as a main language\n\n\nHousehold Language (2011) % of households where no people in household have English as a main language\n\n\nReligion (2011) Christian\n\n\nReligion (2011) Buddhist\n\n\nReligion (2011) Hindu\n\n\nReligion (2011) Jewish\n\n\nReligion (2011) Muslim\n\n\nReligion (2011) Sikh\n\n\nReligion (2011) Other religion\n\n\nReligion (2011) No religion\n\n\nReligion (2011) Religion not stated\n\n\nReligion (2011) Christian (%)\n\n\nReligion (2011) Buddhist (%)\n\n\nReligion (2011) Hindu (%)\n\n\nReligion (2011) Jewish (%)\n\n\nReligion (2011) Muslim (%)\n\n\nReligion (2011) Sikh (%)\n\n\nReligion (2011) Other religion (%)\n\n\nReligion (2011) No religion (%)\n\n\nReligion (2011) Religion not stated (%)\n\n\nTenure (2011) Owned: Owned outright\n\n\nTenure (2011) Owned: Owned with a mortgage or loan\n\n\nTenure (2011) Social rented\n\n\nTenure (2011) Private rented\n\n\nTenure (2011) Owned: Owned outright (%)\n\n\nTenure (2011) Owned: Owned with a mortgage or loan (%)\n\n\nTenure (2011) Social rented (%)\n\n\nTenure (2011) Private rented (%)\n\n\nDwelling type (2011) Household spaces with at least one usual resident\n\n\nDwelling type (2011) Household spaces with no usual residents\n\n\nDwelling type (2011) Whole house or bungalow: Detached\n\n\nDwelling type (2011) Whole house or bungalow: Semi-detached\n\n\nDwelling type (2011) Whole house or bungalow: Terraced (including end-terrace)\n\n\nDwelling type (2011) Flat, maisonette or apartment\n\n\nDwelling type (2011) Household spaces with at least one usual resident (%)\n\n\nDwelling type (2011) Household spaces with no usual residents (%)\n\n\nDwelling type (2011) Whole house or bungalow: Detached (%)\n\n\nDwelling type (2011) Whole house or bungalow: Semi-detached (%)\n\n\nDwelling type (2011) Whole house or bungalow: Terraced (including end-terrace) (%)\n\n\nDwelling type (2011) Flat, maisonette or apartment (%)\n\n\nLand Area Hectares\n\n\nPopulation Density Persons per hectare (2012)\n\n\nHouse Prices Median House Price (£) 2005\n\n\nHouse Prices Median House Price (£) 2006\n\n\nHouse Prices Median House Price (£) 2007\n\n\nHouse Prices Median House Price (£) 2008\n\n\nHouse Prices Median House Price (£) 2009\n\n\nHouse Prices Median House Price (£) 2010\n\n\nHouse Prices Median House Price (£) 2011\n\n\nHouse Prices Median House Price (£) 2012\n\n\nHouse Prices Median House Price (£) 2013 (p)\n\n\nHouse Prices Sales 2005\n\n\nHouse Prices Sales 2006\n\n\nHouse Prices Sales 2007\n\n\nHouse Prices Sales 2008\n\n\nHouse Prices Sales 2009\n\n\nHouse Prices Sales 2010\n\n\nHouse Prices Sales 2011…129\n\n\nHouse Prices Sales 2011…130\n\n\nHouse Prices Sales 2013(p)\n\n\nQualifications (2011 Census) No qualifications\n\n\nQualifications (2011 Census) Highest level of qualification: Level 1 qualifications\n\n\nQualifications (2011 Census) Highest level of qualification: Level 2 qualifications\n\n\nQualifications (2011 Census) Highest level of qualification: Apprenticeship\n\n\nQualifications (2011 Census) Highest level of qualification: Level 3 qualifications\n\n\nQualifications (2011 Census) Highest level of qualification: Level 4 qualifications and above\n\n\nQualifications (2011 Census) Highest level of qualification: Other qualifications\n\n\nQualifications (2011 Census) Schoolchildren and full-time students: Age 18 and over\n\n\nEconomic Activity (2011 Census) Economically active: Total\n\n\nEconomic Activity (2011 Census) Economically active: Unemployed\n\n\nEconomic Activity (2011 Census) Economically inactive: Total\n\n\nEconomic Activity (2011 Census) Economically active %\n\n\nEconomic Activity (2011 Census) Unemployment Rate\n\n\nEconomic Activity (2011 Census) Economically inactive %\n\n\nAdults in Employment (2011 Census) No adults in employment in household: With dependent children\n\n\nAdults in Employment (2011 Census) % of households with no adults in employment: With dependent children\n\n\nHousehold Income Estimates (2011/12) Total Mean Annual Household Income (£)\n\n\nHousehold Income Estimates (2011/12) Total Median Annual Household Income (£)\n\n\nIncome Deprivation (2010) % living in income deprived households reliant on means tested benefit\n\n\nIncome Deprivation (2010) % of people aged over 60 who live in pension credit households\n\n\nLone Parents (2011 Census) All lone parent housholds with dependent children\n\n\nLone Parents (2011 Census) Lone parents not in employment\n\n\nLone Parents (2011 Census) Lone parent not in employment %\n\n\nCentral Heating (2011 Census) Households with central heating (%)\n\n\nHealth (2011 Census) Day-to-day activities limited a lot\n\n\nHealth (2011 Census) Day-to-day activities limited a little\n\n\nHealth (2011 Census) Day-to-day activities not limited\n\n\nHealth (2011 Census) Day-to-day activities limited a lot (%)\n\n\nHealth (2011 Census) Day-to-day activities limited a little (%)\n\n\nHealth (2011 Census) Day-to-day activities not limited (%)\n\n\nHealth (2011 Census) Very good health\n\n\nHealth (2011 Census) Good health\n\n\nHealth (2011 Census) Fair health\n\n\nHealth (2011 Census) Bad health\n\n\nHealth (2011 Census) Very bad health\n\n\nHealth (2011 Census) Very good health (%)\n\n\nHealth (2011 Census) Good health (%)\n\n\nHealth (2011 Census) Fair health (%)\n\n\nHealth (2011 Census) Bad health (%)\n\n\nHealth (2011 Census) Very bad health (%)\n\n\nLow Birth Weight Births (2007-2011) Low Birth Weight Births (%)\n\n\nLow Birth Weight Births (2007-2011) LCL - Lower confidence limit\n\n\nLow Birth Weight Births (2007-2011) UCL - Upper confidence limit\n\n\nObesity % of measured children in Year 6 who were classified as obese, 2009/10-2011/12\n\n\nObesity Percentage of the population aged 16+ with a BMI of 30+, modelled estimate, 2006-2008\n\n\nIncidence of Cancer All\n\n\nIncidence of Cancer Breast Cancer\n\n\nIncidence of Cancer Colorectal Cancer\n\n\nIncidence of Cancer Lung Cancer\n\n\nIncidence of Cancer Prostate Cancer\n\n\nLife Expectancy Males\n\n\nLife Expectancy Females\n\n\nCar or van availability (2011 Census) No cars or vans in household\n\n\nCar or van availability (2011 Census) 1 car or van in household\n\n\nCar or van availability (2011 Census) 2 cars or vans in household\n\n\nCar or van availability (2011 Census) 3 cars or vans in household\n\n\nCar or van availability (2011 Census) 4 or more cars or vans in household\n\n\nCar or van availability (2011 Census) Sum of all cars or vans in the area\n\n\nCar or van availability (2011 Census) No cars or vans in household (%)\n\n\nCar or van availability (2011 Census) 1 car or van in household (%)\n\n\nCar or van availability (2011 Census) 2 cars or vans in household (%)\n\n\nCar or van availability (2011 Census) 3 cars or vans in household (%)\n\n\nCar or van availability (2011 Census) 4 or more cars or vans in household (%)\n\n\nCar or van availability (2011 Census) Cars per household\n\n\nRoad Casualties 2010 Fatal\n\n\nRoad Casualties 2010 Serious\n\n\nRoad Casualties 2010 Slight\n\n\nRoad Casualties 2010 2010 Total\n\n\nRoad Casualties 2011 Fatal\n\n\nRoad Casualties 2011 Serious\n\n\nRoad Casualties 2011 Slight\n\n\nRoad Casualties 2011 2011 Total\n\n\nRoad Casualties 2012 Fatal\n\n\nRoad Casualties 2012 Serious\n\n\nRoad Casualties 2012 Slight\n\n\nRoad Casualties 2012 2012 Total\n\n\n\n\n\n\n7375\n\n\n7187\n\n\n188\n\n\n25.5\n\n\n4385\n\n\n1.6\n\n\n1\n\n\n7.625019\n\n\n-6.6250185\n\n\n-0.8688528\n\n\n1\n\n\n-1.976662\n\n\n2.976662\n\n\n-1.5059034\n\n\n53\n\n\n58\n\n\n-1.5362329\n\n\n-6.0955833\n\n\n51\n\n\n48.02334\n\n\n0.0601204\n\n\n51\n\n\n57.62502\n\n\n-0.1219796\n\n\n7375\n\n\n620\n\n\n1665\n\n\n2045\n\n\n2010\n\n\n1035\n\n\n5720\n\n\n7280\n\n\n7115\n\n\n7118\n\n\n7131\n\n\n7254\n\n\n7607\n\n\n7429\n\n\n7472\n\n\n7338\n\n\n7412\n\n\n7604\n\n\n8.771699\n\n\n76.68332\n\n\n14.54498\n\n\n297\n\n\n205\n\n\n165\n\n\n231\n\n\n495\n\n\n949\n\n\n826\n\n\n622\n\n\n663\n\n\n598\n\n\n504\n\n\n470\n\n\n473\n\n\n363\n\n\n263\n\n\n192\n\n\n155\n\n\n86\n\n\n47\n\n\n4385\n\n\n306\n\n\n927\n\n\n153\n\n\n2472\n\n\n527\n\n\n6.978335\n\n\n21.14025\n\n\n3.489168\n\n\n56.37400\n\n\n12.01824\n\n\n5799\n\n\n289\n\n\n940\n\n\n193\n\n\n154\n\n\n1576\n\n\n78.63051\n\n\n3.918644\n\n\n12.745763\n\n\n2.616949\n\n\n2.0881356\n\n\n21.36949\n\n\n4670\n\n\n2705\n\n\n63.32203\n\n\n36.67797\n\n\n3825\n\n\n560\n\n\n87.22919\n\n\n12.770810\n\n\n3344\n\n\n92\n\n\n145\n\n\n166\n\n\n409\n\n\n18\n\n\n28\n\n\n2522\n\n\n651\n\n\n45.3\n\n\n1.2\n\n\n2.0\n\n\n2.3\n\n\n5.5\n\n\n0.2\n\n\n0.4\n\n\n34.2\n\n\n8.8\n\n\n1093\n\n\n762\n\n\n725\n\n\n1573\n\n\n24.9\n\n\n17.4\n\n\n16.5\n\n\n35.9\n\n\n4385\n\n\n1145\n\n\n22\n\n\n12\n\n\n80\n\n\n5416\n\n\n79.3\n\n\n20.7\n\n\n0.4\n\n\n0.2\n\n\n1.4\n\n\n98.0\n\n\n289.78\n\n\n26.24060\n\n\n310000\n\n\n341000\n\n\n412500\n\n\n365000.0\n\n\n410000\n\n\n450000\n\n\n465000\n\n\n485000\n\n\n595000\n\n\n303\n\n\n295\n\n\n268\n\n\n141\n\n\n157\n\n\n235\n\n\n256\n\n\n195\n\n\n353\n\n\n454\n\n\n291\n\n\n445\n\n\n47\n\n\n484\n\n\n4618\n\n\n416\n\n\n422\n\n\n4972\n\n\n187\n\n\n1335\n\n\n78.83304\n\n\n3.761062\n\n\n21.16696\n\n\n38\n\n\n0.9\n\n\n59728.48\n\n\n46788.30\n\n\n5.2\n\n\n9.9\n\n\n91\n\n\n22\n\n\n24.2\n\n\n95.7\n\n\n328\n\n\n520\n\n\n6527\n\n\n4.447458\n\n\n7.050847\n\n\n88.50169\n\n\n4112\n\n\n2374\n\n\n643\n\n\n190\n\n\n56\n\n\n55.75593\n\n\n32.18983\n\n\n8.718644\n\n\n2.576271\n\n\n0.759322\n\n\n4.2\n\n\n2.5\n\n\n7.1\n\n\nNA\n\n\n13.7\n\n\n76.8\n\n\n90.9\n\n\n83.9\n\n\n57.1\n\n\n80.2\n\n\n83.6\n\n\n88.4\n\n\n3043\n\n\n1100\n\n\n173\n\n\n51\n\n\n18\n\n\n1692\n\n\n69.4\n\n\n25.1\n\n\n3.9\n\n\n1.2\n\n\n0.4\n\n\n0.3858609\n\n\n1\n\n\n39\n\n\n334\n\n\n374\n\n\n0\n\n\n46\n\n\n359\n\n\n405\n\n\n2\n\n\n51\n\n\n361\n\n\n414\n\n\n\n\n6775\n\n\n6724\n\n\n51\n\n\n31.3\n\n\n2713\n\n\n2.5\n\n\n8\n\n\n-9.233267\n\n\n17.2332671\n\n\n-1.8664322\n\n\n0\n\n\n1.129576\n\n\n-1.129576\n\n\n-1.0000000\n\n\n53\n\n\n136\n\n\n-27.9473386\n\n\n-2.0000000\n\n\n50\n\n\n51.12958\n\n\n-0.0223392\n\n\n58\n\n\n40.76673\n\n\n0.3489691\n\n\n6775\n\n\n1751\n\n\n1277\n\n\n1388\n\n\n1258\n\n\n1101\n\n\n3923\n\n\n6333\n\n\n6312\n\n\n6329\n\n\n6341\n\n\n6330\n\n\n6323\n\n\n6369\n\n\n6570\n\n\n6636\n\n\n6783\n\n\n6853\n\n\n25.113089\n\n\n58.73340\n\n\n16.15351\n\n\n652\n\n\n607\n\n\n462\n\n\n458\n\n\n399\n\n\n468\n\n\n466\n\n\n466\n\n\n461\n\n\n450\n\n\n347\n\n\n254\n\n\n256\n\n\n254\n\n\n206\n\n\n215\n\n\n201\n\n\n137\n\n\n94\n\n\n2713\n\n\n491\n\n\n366\n\n\n597\n\n\n814\n\n\n445\n\n\n18.098046\n\n\n13.49060\n\n\n22.005160\n\n\n30.00369\n\n\n16.40251\n\n\n4403\n\n\n330\n\n\n820\n\n\n1133\n\n\n89\n\n\n2372\n\n\n64.98893\n\n\n4.870849\n\n\n12.103321\n\n\n16.723247\n\n\n1.3136531\n\n\n35.01107\n\n\n5159\n\n\n1616\n\n\n76.14760\n\n\n23.85240\n\n\n2459\n\n\n254\n\n\n90.63767\n\n\n9.362329\n\n\n3975\n\n\n19\n\n\n174\n\n\n27\n\n\n591\n\n\n122\n\n\n16\n\n\n1417\n\n\n434\n\n\n58.7\n\n\n0.3\n\n\n2.6\n\n\n0.4\n\n\n8.7\n\n\n1.8\n\n\n0.2\n\n\n20.9\n\n\n6.4\n\n\n596\n\n\n663\n\n\n1133\n\n\n269\n\n\n22.0\n\n\n24.4\n\n\n41.8\n\n\n9.9\n\n\n2713\n\n\n82\n\n\n99\n\n\n744\n\n\n865\n\n\n1087\n\n\n97.1\n\n\n2.9\n\n\n3.5\n\n\n26.6\n\n\n30.9\n\n\n38.9\n\n\n216.15\n\n\n31.70483\n\n\n168500\n\n\n180000\n\n\n187500\n\n\n197500.0\n\n\n190000\n\n\n173000\n\n\n185000\n\n\n182250\n\n\n190000\n\n\n81\n\n\n100\n\n\n100\n\n\n68\n\n\n45\n\n\n61\n\n\n51\n\n\n42\n\n\n61\n\n\n1623\n\n\n789\n\n\n706\n\n\n118\n\n\n479\n\n\n914\n\n\n395\n\n\n272\n\n\n2847\n\n\n335\n\n\n1513\n\n\n65.29817\n\n\n11.766772\n\n\n34.70183\n\n\n319\n\n\n11.8\n\n\n31788.19\n\n\n27058.70\n\n\n31.0\n\n\n27.5\n\n\n445\n\n\n249\n\n\n56.0\n\n\n97.5\n\n\n707\n\n\n678\n\n\n5390\n\n\n10.435424\n\n\n10.007380\n\n\n79.55720\n\n\n2933\n\n\n2288\n\n\n1059\n\n\n389\n\n\n106\n\n\n43.29151\n\n\n33.77122\n\n\n15.630996\n\n\n5.741697\n\n\n1.564576\n\n\n10.6\n\n\n8.4\n\n\n13.3\n\n\n23.6\n\n\n29.8\n\n\n100.7\n\n\n93.3\n\n\n82.5\n\n\n97.2\n\n\n107.6\n\n\n78.0\n\n\n80.1\n\n\n1020\n\n\n1186\n\n\n424\n\n\n66\n\n\n17\n\n\n2305\n\n\n37.6\n\n\n43.7\n\n\n15.6\n\n\n2.4\n\n\n0.6\n\n\n0.8496130\n\n\n0\n\n\n0\n\n\n18\n\n\n18\n\n\n0\n\n\n2\n\n\n16\n\n\n18\n\n\n0\n\n\n1\n\n\n15\n\n\n16\n\n\n\n\n10045\n\n\n10033\n\n\n12\n\n\n46.9\n\n\n3834\n\n\n2.6\n\n\n11\n\n\n12.348001\n\n\n-1.3480006\n\n\n-0.1091675\n\n\n10\n\n\n6.922428\n\n\n3.077572\n\n\n0.4445798\n\n\n124\n\n\n239\n\n\n-0.1154703\n\n\n0.3637269\n\n\n60\n\n\n56.92243\n\n\n0.0526430\n\n\n61\n\n\n62.34800\n\n\n-0.0218569\n\n\n10045\n\n\n2247\n\n\n1959\n\n\n2300\n\n\n2259\n\n\n1280\n\n\n6518\n\n\n9236\n\n\n9252\n\n\n9155\n\n\n9072\n\n\n9144\n\n\n9227\n\n\n9564\n\n\n9914\n\n\n10042\n\n\n10088\n\n\n10218\n\n\n21.442552\n\n\n65.81523\n\n\n12.74222\n\n\n849\n\n\n739\n\n\n603\n\n\n615\n\n\n686\n\n\n804\n\n\n818\n\n\n743\n\n\n770\n\n\n713\n\n\n624\n\n\n526\n\n\n426\n\n\n372\n\n\n277\n\n\n258\n\n\n198\n\n\n124\n\n\n73\n\n\n3834\n\n\n776\n\n\n730\n\n\n589\n\n\n1039\n\n\n700\n\n\n20.239958\n\n\n19.04017\n\n\n15.362546\n\n\n27.09963\n\n\n18.25769\n\n\n5486\n\n\n433\n\n\n2284\n\n\n1618\n\n\n224\n\n\n4559\n\n\n54.61424\n\n\n4.310602\n\n\n22.737680\n\n\n16.107516\n\n\n2.2299652\n\n\n45.38576\n\n\n7193\n\n\n2852\n\n\n71.60777\n\n\n28.39223\n\n\n3431\n\n\n403\n\n\n89.48878\n\n\n10.511215\n\n\n5475\n\n\n46\n\n\n476\n\n\n42\n\n\n1351\n\n\n430\n\n\n34\n\n\n1514\n\n\n677\n\n\n54.5\n\n\n0.5\n\n\n4.7\n\n\n0.4\n\n\n13.4\n\n\n4.3\n\n\n0.3\n\n\n15.1\n\n\n6.7\n\n\n1028\n\n\n1473\n\n\n446\n\n\n830\n\n\n26.8\n\n\n38.4\n\n\n11.6\n\n\n21.6\n\n\n3834\n\n\n110\n\n\n161\n\n\n936\n\n\n1591\n\n\n1256\n\n\n97.2\n\n\n2.8\n\n\n4.1\n\n\n23.7\n\n\n40.3\n\n\n31.8\n\n\n214.15\n\n\n47.71422\n\n\n185000\n\n\n197750\n\n\n220000\n\n\n225000.0\n\n\n188500\n\n\n215000\n\n\n200000\n\n\n205500\n\n\n237000\n\n\n203\n\n\n232\n\n\n295\n\n\n100\n\n\n80\n\n\n97\n\n\n89\n\n\n114\n\n\n98\n\n\n1778\n\n\n1210\n\n\n1236\n\n\n169\n\n\n847\n\n\n1829\n\n\n729\n\n\n442\n\n\n5038\n\n\n459\n\n\n2111\n\n\n70.47139\n\n\n9.110758\n\n\n29.52861\n\n\n268\n\n\n7.0\n\n\n43356.93\n\n\n36834.53\n\n\n18.9\n\n\n21.2\n\n\n408\n\n\n199\n\n\n48.8\n\n\n96.5\n\n\n792\n\n\n810\n\n\n8443\n\n\n7.884520\n\n\n8.063713\n\n\n84.05177\n\n\n4566\n\n\n3633\n\n\n1325\n\n\n406\n\n\n115\n\n\n45.45545\n\n\n36.16725\n\n\n13.190642\n\n\n4.041812\n\n\n1.144848\n\n\n7.8\n\n\n6.1\n\n\n9.9\n\n\n25.5\n\n\n28.3\n\n\n91.4\n\n\n107.3\n\n\n124.6\n\n\n105.5\n\n\n82.9\n\n\n80.2\n\n\n85.6\n\n\n1196\n\n\n1753\n\n\n691\n\n\n155\n\n\n39\n\n\n3766\n\n\n31.2\n\n\n45.7\n\n\n18.0\n\n\n4.0\n\n\n1.0\n\n\n0.9822640\n\n\n0\n\n\n3\n\n\n34\n\n\n37\n\n\n1\n\n\n4\n\n\n40\n\n\n45\n\n\n0\n\n\n3\n\n\n47\n\n\n50\n\n\n\n\n6182\n\n\n5937\n\n\n245\n\n\n24.8\n\n\n2318\n\n\n2.6\n\n\n2\n\n\n-4.719603\n\n\n6.7196026\n\n\n-1.4237645\n\n\n0\n\n\n-1.271292\n\n\n1.271292\n\n\n-1.0000000\n\n\n30\n\n\n81\n\n\n-4.9416062\n\n\n-2.0000000\n\n\n50\n\n\n48.72871\n\n\n0.0257532\n\n\n52\n\n\n45.28040\n\n\n0.1381492\n\n\n6182\n\n\n1196\n\n\n1277\n\n\n1154\n\n\n1543\n\n\n1012\n\n\n3974\n\n\n6208\n\n\n6159\n\n\n6163\n\n\n6152\n\n\n5997\n\n\n6005\n\n\n6084\n\n\n6268\n\n\n6237\n\n\n6185\n\n\n6308\n\n\n18.246671\n\n\n65.82118\n\n\n15.93215\n\n\n409\n\n\n364\n\n\n378\n\n\n516\n\n\n472\n\n\n435\n\n\n363\n\n\n399\n\n\n388\n\n\n463\n\n\n459\n\n\n354\n\n\n303\n\n\n226\n\n\n238\n\n\n194\n\n\n172\n\n\n106\n\n\n69\n\n\n2318\n\n\n508\n\n\n524\n\n\n322\n\n\n609\n\n\n355\n\n\n21.915444\n\n\n22.60569\n\n\n13.891286\n\n\n26.27265\n\n\n15.31493\n\n\n5006\n\n\n186\n\n\n313\n\n\n649\n\n\n28\n\n\n1176\n\n\n80.97703\n\n\n3.008735\n\n\n5.063086\n\n\n10.498221\n\n\n0.4529279\n\n\n19.02297\n\n\n5292\n\n\n890\n\n\n85.60336\n\n\n14.39664\n\n\n2214\n\n\n104\n\n\n95.51337\n\n\n4.486626\n\n\n4070\n\n\n21\n\n\n34\n\n\n23\n\n\n234\n\n\n10\n\n\n18\n\n\n1373\n\n\n399\n\n\n65.8\n\n\n0.3\n\n\n0.5\n\n\n0.4\n\n\n3.8\n\n\n0.2\n\n\n0.3\n\n\n22.2\n\n\n6.5\n\n\n718\n\n\n969\n\n\n371\n\n\n228\n\n\n31.0\n\n\n41.8\n\n\n16.0\n\n\n9.8\n\n\n2318\n\n\n45\n\n\n92\n\n\n858\n\n\n1094\n\n\n314\n\n\n98.1\n\n\n1.9\n\n\n3.9\n\n\n36.3\n\n\n46.3\n\n\n13.2\n\n\n249.28\n\n\n25.30488\n\n\n181000\n\n\n182000\n\n\n212000\n\n\n208997.5\n\n\n182000\n\n\n192000\n\n\n193750\n\n\n205000\n\n\n210000\n\n\n93\n\n\n131\n\n\n125\n\n\n66\n\n\n54\n\n\n61\n\n\n72\n\n\n58\n\n\n67\n\n\n1502\n\n\n800\n\n\n825\n\n\n163\n\n\n539\n\n\n891\n\n\n266\n\n\n215\n\n\n3187\n\n\n296\n\n\n1251\n\n\n71.81163\n\n\n9.287731\n\n\n28.18837\n\n\n122\n\n\n5.3\n\n\n46701.44\n\n\n39668.21\n\n\n15.8\n\n\n21.3\n\n\n206\n\n\n87\n\n\n42.2\n\n\n98.5\n\n\n586\n\n\n547\n\n\n5049\n\n\n9.479133\n\n\n8.848269\n\n\n81.67260\n\n\n2857\n\n\n2086\n\n\n861\n\n\n302\n\n\n76\n\n\n46.21482\n\n\n33.74313\n\n\n13.927532\n\n\n4.885150\n\n\n1.229376\n\n\n5.8\n\n\n3.9\n\n\n8.6\n\n\nNA\n\n\n26.9\n\n\n96.1\n\n\n95.5\n\n\n105.8\n\n\n102.4\n\n\n110.3\n\n\n77.9\n\n\n80.7\n\n\n556\n\n\n1085\n\n\n515\n\n\n128\n\n\n34\n\n\n2650\n\n\n24.0\n\n\n46.8\n\n\n22.2\n\n\n5.5\n\n\n1.5\n\n\n1.1432269\n\n\n0\n\n\n1\n\n\n13\n\n\n14\n\n\n0\n\n\n2\n\n\n7\n\n\n9\n\n\n0\n\n\n2\n\n\n5\n\n\n7\n\n\n\n\n8562\n\n\n8562\n\n\n0\n\n\n72.1\n\n\n3183\n\n\n2.7\n\n\n4\n\n\n4.584902\n\n\n-0.5849018\n\n\n-0.1275713\n\n\n1\n\n\n9.214027\n\n\n-8.214027\n\n\n-0.8914698\n\n\n87\n\n\n161\n\n\n-0.1362629\n\n\n-1.6083817\n\n\n51\n\n\n59.21403\n\n\n-0.1490559\n\n\n54\n\n\n54.58490\n\n\n-0.0107732\n\n\n8562\n\n\n2200\n\n\n1592\n\n\n1995\n\n\n1829\n\n\n946\n\n\n5416\n\n\n7919\n\n\n7922\n\n\n7882\n\n\n7887\n\n\n7917\n\n\n7916\n\n\n8025\n\n\n8317\n\n\n8519\n\n\n8588\n\n\n8660\n\n\n24.237875\n\n\n64.57275\n\n\n11.18938\n\n\n783\n\n\n692\n\n\n624\n\n\n657\n\n\n525\n\n\n608\n\n\n616\n\n\n643\n\n\n673\n\n\n656\n\n\n508\n\n\n386\n\n\n320\n\n\n321\n\n\n202\n\n\n194\n\n\n127\n\n\n90\n\n\n35\n\n\n3183\n\n\n691\n\n\n583\n\n\n593\n\n\n808\n\n\n508\n\n\n21.709080\n\n\n18.31605\n\n\n18.630223\n\n\n25.38486\n\n\n15.95979\n\n\n5674\n\n\n313\n\n\n1050\n\n\n1445\n\n\n80\n\n\n2888\n\n\n66.26956\n\n\n3.655688\n\n\n12.263490\n\n\n16.876898\n\n\n0.9343611\n\n\n33.73044\n\n\n6425\n\n\n2137\n\n\n75.04088\n\n\n24.95912\n\n\n2868\n\n\n315\n\n\n90.10368\n\n\n9.896324\n\n\n4986\n\n\n28\n\n\n138\n\n\n35\n\n\n762\n\n\n166\n\n\n13\n\n\n1816\n\n\n618\n\n\n58.2\n\n\n0.3\n\n\n1.6\n\n\n0.4\n\n\n8.9\n\n\n1.9\n\n\n0.2\n\n\n21.2\n\n\n7.2\n\n\n711\n\n\n1146\n\n\n793\n\n\n482\n\n\n22.3\n\n\n36.0\n\n\n24.9\n\n\n15.1\n\n\n3183\n\n\n89\n\n\n136\n\n\n622\n\n\n2141\n\n\n373\n\n\n97.3\n\n\n2.7\n\n\n4.2\n\n\n19.0\n\n\n65.4\n\n\n11.4\n\n\n118.81\n\n\n72.88949\n\n\n162250\n\n\n170000\n\n\n185000\n\n\n200000.0\n\n\n166250\n\n\n169000\n\n\n165000\n\n\n167000\n\n\n180000\n\n\n152\n\n\n168\n\n\n184\n\n\n73\n\n\n60\n\n\n58\n\n\n81\n\n\n59\n\n\n75\n\n\n1839\n\n\n1026\n\n\n1038\n\n\n160\n\n\n653\n\n\n1119\n\n\n527\n\n\n333\n\n\n4052\n\n\n394\n\n\n1881\n\n\n68.29597\n\n\n9.723593\n\n\n31.70403\n\n\n307\n\n\n9.6\n\n\n34293.82\n\n\n29155.68\n\n\n22.9\n\n\n25.3\n\n\n442\n\n\n231\n\n\n52.3\n\n\n96.3\n\n\n713\n\n\n722\n\n\n7127\n\n\n8.327494\n\n\n8.432609\n\n\n83.23990\n\n\n4089\n\n\n2811\n\n\n1134\n\n\n412\n\n\n116\n\n\n47.75753\n\n\n32.83111\n\n\n13.244569\n\n\n4.811960\n\n\n1.354824\n\n\n7.7\n\n\n6.0\n\n\n9.9\n\n\n24.4\n\n\n29.7\n\n\n110.0\n\n\n106.1\n\n\n113.7\n\n\n168.6\n\n\n63.1\n\n\n76.8\n\n\n79.9\n\n\n1080\n\n\n1423\n\n\n551\n\n\n109\n\n\n20\n\n\n2937\n\n\n33.9\n\n\n44.7\n\n\n17.3\n\n\n3.4\n\n\n0.6\n\n\n0.9227144\n\n\n1\n\n\n5\n\n\n24\n\n\n30\n\n\n0\n\n\n2\n\n\n27\n\n\n29\n\n\n0\n\n\n1\n\n\n24\n\n\n25\n\n\n\n\n8791\n\n\n8672\n\n\n119\n\n\n50.6\n\n\n3441\n\n\n2.5\n\n\n5\n\n\n13.290915\n\n\n-8.2909151\n\n\n-0.6238032\n\n\n5\n\n\n1.267089\n\n\n3.732911\n\n\n2.9460534\n\n\n96\n\n\n136\n\n\n-0.9065610\n\n\n1.1912744\n\n\n55\n\n\n51.26709\n\n\n0.0702553\n\n\n55\n\n\n63.29092\n\n\n-0.1401784\n\n\n8791\n\n\n2388\n\n\n1765\n\n\n1867\n\n\n1736\n\n\n1035\n\n\n5368\n\n\n7806\n\n\n7726\n\n\n7771\n\n\n7820\n\n\n7877\n\n\n7974\n\n\n8145\n\n\n8394\n\n\n8571\n\n\n8823\n\n\n9076\n\n\n26.586602\n\n\n62.02071\n\n\n11.39268\n\n\n957\n\n\n792\n\n\n664\n\n\n633\n\n\n642\n\n\n686\n\n\n634\n\n\n651\n\n\n596\n\n\n610\n\n\n483\n\n\n398\n\n\n296\n\n\n242\n\n\n207\n\n\n180\n\n\n181\n\n\n139\n\n\n85\n\n\n3441\n\n\n643\n\n\n489\n\n\n776\n\n\n1064\n\n\n469\n\n\n18.686428\n\n\n14.21099\n\n\n22.551584\n\n\n30.92124\n\n\n13.62976\n\n\n5906\n\n\n307\n\n\n526\n\n\n1997\n\n\n55\n\n\n2885\n\n\n67.18235\n\n\n3.492208\n\n\n5.983392\n\n\n22.716414\n\n\n0.6256399\n\n\n32.81765\n\n\n6658\n\n\n2133\n\n\n75.73655\n\n\n24.26345\n\n\n3117\n\n\n324\n\n\n90.58413\n\n\n9.415867\n\n\n5409\n\n\n22\n\n\n67\n\n\n13\n\n\n577\n\n\n19\n\n\n24\n\n\n2148\n\n\n512\n\n\n61.5\n\n\n0.3\n\n\n0.8\n\n\n0.1\n\n\n6.6\n\n\n0.2\n\n\n0.3\n\n\n24.4\n\n\n5.8\n\n\n558\n\n\n821\n\n\n1663\n\n\n333\n\n\n16.2\n\n\n23.9\n\n\n48.3\n\n\n9.7\n\n\n3441\n\n\n93\n\n\n82\n\n\n761\n\n\n1219\n\n\n1471\n\n\n97.4\n\n\n2.6\n\n\n2.3\n\n\n21.5\n\n\n34.5\n\n\n41.6\n\n\n173.58\n\n\n52.28713\n\n\n165000\n\n\n165000\n\n\n187000\n\n\n199000.0\n\n\n155000\n\n\n165000\n\n\n153750\n\n\n172500\n\n\n168500\n\n\n112\n\n\n133\n\n\n139\n\n\n39\n\n\n40\n\n\n75\n\n\n76\n\n\n48\n\n\n55\n\n\n2057\n\n\n1065\n\n\n1001\n\n\n166\n\n\n556\n\n\n1129\n\n\n429\n\n\n402\n\n\n3905\n\n\n511\n\n\n1893\n\n\n67.35081\n\n\n13.085788\n\n\n32.64919\n\n\n437\n\n\n12.7\n\n\n29975.83\n\n\n25568.60\n\n\n33.3\n\n\n28.9\n\n\n602\n\n\n345\n\n\n57.3\n\n\n98.1\n\n\n828\n\n\n752\n\n\n7211\n\n\n9.418724\n\n\n8.554203\n\n\n82.02707\n\n\n3996\n\n\n3015\n\n\n1252\n\n\n407\n\n\n121\n\n\n45.45558\n\n\n34.29644\n\n\n14.241838\n\n\n4.629735\n\n\n1.376408\n\n\n6.8\n\n\n5.3\n\n\n8.8\n\n\n26.0\n\n\n29.0\n\n\n117.0\n\n\n80.1\n\n\n123.6\n\n\n151.7\n\n\n112.9\n\n\n75.7\n\n\n79.5\n\n\n1496\n\n\n1444\n\n\n419\n\n\n63\n\n\n19\n\n\n2549\n\n\n43.5\n\n\n42.0\n\n\n12.2\n\n\n1.8\n\n\n0.6\n\n\n0.7407730\n\n\n0\n\n\n2\n\n\n18\n\n\n20\n\n\n1\n\n\n1\n\n\n18\n\n\n20\n\n\n0\n\n\n2\n\n\n25\n\n\n27\n\n\n\n\n\n\n\n\nHaving now ingested and linked our data, we begin by exploring the factors most highly correlated with our relative error rates.\n\ncorr_df <- correlate(dplyr::select_if(msoa_matrix_tbl, is.numeric), quiet = TRUE)\n\noptions(scipen=999)\n\nStarting by our relative robbery error, a few interesting correlates stand out: - road traffic casualties - burglary numbers - the number of dwellings with no usual residents, and the number of commercial residents - households with no cars - the age composition of the area - general deprivation indicators (such as the proportion of households with central heating)\n\n#show only correlates with an absolute value higher than 0.2\n#filter(dplyr::select(corr_df[order(corr_df$RPDRobberyShifted),] , term, RPDRobberyShifted), RPDRobberyShifted < -0.2 | RPDRobberyShifted > 0.2)\n#filter(dplyr::select(corr_df[order(corr_df$RPDRobberyShifted),] , term, RPDRobberyShifted), RPDRobberyShifted < -0.2 | RPDRobberyShifted > 0.2)\nhigh_corr_rob <- filter(dplyr::select(corr_df, term, RPDRobberyShifted), RPDRobberyShifted < -0.15 | RPDRobberyShifted > 0.15)\n\nhigh_corr_rob[order(high_corr_rob$RPDRobberyShifted),]\n\n\n\n\n\n\n\nterm\n\n\nRPDRobberyShifted\n\n\n\n\n\n\nrobberyPredicted\n\n\n-0.8387666\n\n\n\n\nrobberyPredictedShifted\n\n\n-0.8387666\n\n\n\n\ntotal_robberies\n\n\n-0.5369450\n\n\n\n\ntotal_burglaries\n\n\n-0.4717060\n\n\n\n\nRoad Casualties 2011 2011 Total\n\n\n-0.3701356\n\n\n\n\nRoad Casualties 2011 Slight\n\n\n-0.3635665\n\n\n\n\nRoad Casualties 2010 2010 Total\n\n\n-0.3597137\n\n\n\n\nRoad Casualties 2010 Slight\n\n\n-0.3567229\n\n\n\n\nRoad Casualties 2012 2012 Total\n\n\n-0.3563600\n\n\n\n\nRoad Casualties 2012 Slight\n\n\n-0.3521528\n\n\n\n\nRoad Casualties 2011 Serious\n\n\n-0.3474359\n\n\n\n\nRoad Casualties 2012 Serious\n\n\n-0.3200851\n\n\n\n\nRoad Casualties 2010 Serious\n\n\n-0.3057949\n\n\n\n\nburglaryPredicted\n\n\n-0.2952051\n\n\n\n\nburglaryPredictedShifted\n\n\n-0.2952051\n\n\n\n\nburglaryActual\n\n\n-0.2646662\n\n\n\n\nburglaryActualShifted\n\n\n-0.2646662\n\n\n\n\nDwelling type (2011) Household spaces with no usual residents\n\n\n-0.2587599\n\n\n\n\nDwelling type (2011) Household spaces with no usual residents (%)\n\n\n-0.2308005\n\n\n\n\nCar or van availability (2011 Census) No cars or vans in household\n\n\n-0.2279835\n\n\n\n\nQualifications (2011 Census) Schoolchildren and full-time students: Age 18 and over\n\n\n-0.2219251\n\n\n\n\nMid-year Estimates 2012, by age 20-24\n\n\n-0.2182673\n\n\n\n\nHousehold Composition (2011) Numbers One person household\n\n\n-0.2145786\n\n\n\n\nTenure (2011) Private rented\n\n\n-0.2029037\n\n\n\n\nDwelling type (2011) Flat, maisonette or apartment\n\n\n-0.2015178\n\n\n\n\nHousehold Composition (2011) Percentages One person household\n\n\n-0.1900561\n\n\n\n\nAge Structure (2011 Census) 16-29\n\n\n-0.1896718\n\n\n\n\nCar or van availability (2011 Census) No cars or vans in household (%)\n\n\n-0.1895818\n\n\n\n\nComEstRes\n\n\n-0.1858042\n\n\n\n\nMid-year Estimates 2012, by age % 15-64\n\n\n-0.1821470\n\n\n\n\nReligion (2011) Buddhist\n\n\n-0.1772919\n\n\n\n\nReligion (2011) Buddhist (%)\n\n\n-0.1759725\n\n\n\n\nHousehold Language (2011) No people in household have English as a main language\n\n\n-0.1746012\n\n\n\n\nQualifications (2011 Census) Highest level of qualification: Level 3 qualifications\n\n\n-0.1642773\n\n\n\n\nHholds\n\n\n-0.1615083\n\n\n\n\nHouseholds (2011) All Households\n\n\n-0.1615083\n\n\n\n\nDwelling type (2011) Household spaces with at least one usual resident\n\n\n-0.1615083\n\n\n\n\nEconomic Activity (2011 Census) Economically inactive: Total\n\n\n-0.1570016\n\n\n\n\nAge Structure (2011 Census) Working-age\n\n\n-0.1531622\n\n\n\n\nEthnic Group (2011 Census) Other ethnic group\n\n\n-0.1523189\n\n\n\n\nCar or van availability (2011 Census) Cars per household\n\n\n0.1604540\n\n\n\n\nTenure (2011) Owned: Owned with a mortgage or loan (%)\n\n\n0.1701928\n\n\n\n\nCentral Heating (2011 Census) Households with central heating (%)\n\n\n0.2055461\n\n\n\n\nburglaryError\n\n\n0.2115687\n\n\n\n\nHousehold Composition (2011) Percentages Couple household with dependent children\n\n\n0.2130557\n\n\n\n\nCar or van availability (2011 Census) 1 car or van in household (%)\n\n\n0.2305679\n\n\n\n\nDwelling type (2011) Household spaces with at least one usual resident (%)\n\n\n0.2308005\n\n\n\n\nrobberyError\n\n\n0.8565425\n\n\n\n\n\n\n\n\nThe correlations for burglary are weaker - only a few have an absolute value higher than 0.2 - but a few stand out: - households with no residents - high robbery numbers - house prices\n\nhigh_corr_burg <- filter(dplyr::select(corr_df, term, RPDburglaryShifted), RPDburglaryShifted < -0.15 | RPDburglaryShifted > 0.15)\n\nhigh_corr_burg[order(high_corr_burg$RPDburglaryShifted),]\n\n\n\n\n\n\n\nterm\n\n\nRPDburglaryShifted\n\n\n\n\n\n\nburglaryPredicted\n\n\n-0.8600903\n\n\n\n\nburglaryPredictedShifted\n\n\n-0.8600903\n\n\n\n\ntotal_burglaries\n\n\n-0.2169094\n\n\n\n\nDwelling type (2011) Household spaces with no usual residents\n\n\n-0.1761768\n\n\n\n\ntotal_robberies\n\n\n-0.1714721\n\n\n\n\nDwelling type (2011) Household spaces with no usual residents (%)\n\n\n-0.1706826\n\n\n\n\nrobberyPredicted\n\n\n-0.1631558\n\n\n\n\nrobberyPredictedShifted\n\n\n-0.1631558\n\n\n\n\nHouse Prices Median House Price (£) 2010\n\n\n-0.1609004\n\n\n\n\nHouse Prices Median House Price (£) 2008\n\n\n-0.1527950\n\n\n\n\nHouse Prices Median House Price (£) 2011\n\n\n-0.1507911\n\n\n\n\nHouse Prices Median House Price (£) 2007\n\n\n-0.1505301\n\n\n\n\nMid-year Estimates 2012, by age % 0 to 14\n\n\n0.1628106\n\n\n\n\nrobberyError\n\n\n0.1628523\n\n\n\n\nDwelling type (2011) Household spaces with at least one usual resident (%)\n\n\n0.1706826\n\n\n\n\nburglaryError\n\n\n0.9656142\n\n\n\n\n\n\n\n\nThese correlates suggest we can model this shift - this is likely to prove more reliable for robbery (where the correlations are stronger), and seem linked to usual resident population(as measured by household composition), deprivation (through various proxy indicators such as central heating presence or housing type), and general crime patterns (through total burglary and robbery numbers)\nWe will take two approaches for modelling: a simple regression (to identify strong links) and random forest regressors (to identify non-linear associations)\n\nSimple Regression\nWe begin through the use of simple OLS regression. This is a linear model that has large limitations for modelling complex relationships, but can be an effective first step, effectively with a few transformations.\nR does not cope well with blank spaces in terms, so we’ll extract and rename our key correlates.\n\n#make copy of df and rename \n\nmsoa_copy <- msoa_matrix_numeric\n\nnames(msoa_copy)[names(msoa_copy) == \"Dwelling type (2011) Household spaces with no usual residents\"] <- \"DwellingNoResidents\"\nnames(msoa_copy)[names(msoa_copy) == \"House Prices Median House Price (£) 2010\"] <- \"MedianHousePrice\"\nnames(msoa_copy)[names(msoa_copy) == \"Dwelling type (2011) Flat, maisonette or apartment\"] <- \"FlatAprt\"\nnames(msoa_copy)[names(msoa_copy) == \"Qualifications (2011 Census) Schoolchildren and full-time students: Age 18 and over\"] <- \"fullTimeStudents\"\nnames(msoa_copy)[names(msoa_copy) == \"Car or van availability (2011 Census) No cars or vans in household\"] <- \"NoCars\"\nnames(msoa_copy)[names(msoa_copy) == \"Ethnic Group (2011 Census) Other ethnic group\"] <- \"OtherEthnicGroup\"\nnames(msoa_copy)[names(msoa_copy) == \"Central Heating (2011 Census) Households with central heating (%)\"] <- \"CentralHealingPercent\"\n\nLet’s now extract these to a separate dataframe, remove any missing values, and provide some quick summary statistics to identify any obvious concerns.\nWe also generate a matrix of scatter-plots, so as to identify any obvious relationships between our key values.\n\nfeature_df <- dplyr::select(msoa_copy, RPDburglaryShifted, RPDRobberyShifted, total_burglaries, total_robberies, DwellingNoResidents, MedianHousePrice, FlatAprt, fullTimeStudents, NoCars, OtherEthnicGroup, CentralHealingPercent, AvHholdSz, ComEstRes)\n\nfeature_df <- drop_na(feature_df, RPDburglaryShifted, RPDRobberyShifted)\nhead(feature_df)\n\n\n\n\n\n\n\nRPDburglaryShifted\n\n\nRPDRobberyShifted\n\n\ntotal_burglaries\n\n\ntotal_robberies\n\n\nDwellingNoResidents\n\n\nMedianHousePrice\n\n\nFlatAprt\n\n\nfullTimeStudents\n\n\nNoCars\n\n\nOtherEthnicGroup\n\n\nCentralHealingPercent\n\n\nAvHholdSz\n\n\nComEstRes\n\n\n\n\n\n\n-0.1219796\n\n\n0.0601204\n\n\n58\n\n\n53\n\n\n1145\n\n\n450000\n\n\n5416\n\n\n422\n\n\n3043\n\n\n154\n\n\n95.7\n\n\n1.6\n\n\n188\n\n\n\n\n0.3489691\n\n\n-0.0223392\n\n\n136\n\n\n53\n\n\n82\n\n\n173000\n\n\n1087\n\n\n272\n\n\n1020\n\n\n89\n\n\n97.5\n\n\n2.5\n\n\n51\n\n\n\n\n-0.0218569\n\n\n0.0526430\n\n\n239\n\n\n124\n\n\n110\n\n\n215000\n\n\n1256\n\n\n442\n\n\n1196\n\n\n224\n\n\n96.5\n\n\n2.6\n\n\n12\n\n\n\n\n0.1381492\n\n\n0.0257532\n\n\n81\n\n\n30\n\n\n45\n\n\n192000\n\n\n314\n\n\n215\n\n\n556\n\n\n28\n\n\n98.5\n\n\n2.6\n\n\n245\n\n\n\n\n-0.0107732\n\n\n-0.1490559\n\n\n161\n\n\n87\n\n\n89\n\n\n169000\n\n\n373\n\n\n333\n\n\n1080\n\n\n80\n\n\n96.3\n\n\n2.7\n\n\n0\n\n\n\n\n-0.1401784\n\n\n0.0702553\n\n\n136\n\n\n96\n\n\n93\n\n\n165000\n\n\n1471\n\n\n402\n\n\n1496\n\n\n55\n\n\n98.1\n\n\n2.5\n\n\n119\n\n\n\n\n\n\n\n\n\nsummary(feature_df)\n\n RPDburglaryShifted RPDRobberyShifted  total_burglaries total_robberies  \n Min.   :-0.81266   Min.   :-1.45491   Min.   :  46.0   Min.   :   2.00  \n 1st Qu.:-0.20689   1st Qu.:-0.11808   1st Qu.: 121.0   1st Qu.:  29.00  \n Median :-0.08826   Median :-0.04081   Median : 159.0   Median :  53.00  \n Mean   :-0.07976   Mean   :-0.04876   Mean   : 175.5   Mean   :  78.53  \n 3rd Qu.: 0.03612   3rd Qu.: 0.04309   3rd Qu.: 205.0   3rd Qu.:  92.00  \n Max.   : 1.29467   Max.   : 0.62020   Max.   :1973.0   Max.   :2456.00  \n DwellingNoResidents MedianHousePrice     FlatAprt      fullTimeStudents\n Min.   :  14.0      Min.   : 137625   Min.   :  54.0   Min.   : 122.0  \n 1st Qu.:  59.0      1st Qu.: 222500   1st Qu.: 830.5   1st Qu.: 305.5  \n Median :  86.0      Median : 265975   Median :1607.0   Median : 465.0  \n Mean   : 123.4      Mean   : 310710   Mean   :1798.2   Mean   : 539.5  \n 3rd Qu.: 133.0      3rd Qu.: 351525   3rd Qu.:2566.5   3rd Qu.: 654.5  \n Max.   :1556.0      Max.   :1425000   Max.   :6429.0   Max.   :3370.0  \n     NoCars     OtherEthnicGroup CentralHealingPercent   AvHholdSz    \n Min.   : 185   Min.   :  16.0   Min.   :92.10         Min.   :1.600  \n 1st Qu.: 796   1st Qu.: 134.0   1st Qu.:96.60         1st Qu.:2.300  \n Median :1256   Median : 232.0   Median :97.40         Median :2.500  \n Mean   :1382   Mean   : 285.8   Mean   :97.24         Mean   :2.506  \n 3rd Qu.:1858   3rd Qu.: 376.5   3rd Qu.:98.00         3rd Qu.:2.700  \n Max.   :4319   Max.   :3001.0   Max.   :99.60         Max.   :3.900  \n   ComEstRes   \n Min.   :   0  \n 1st Qu.:   9  \n Median :  41  \n Mean   : 102  \n 3rd Qu.: 105  \n Max.   :2172  \n\n\n\ncolSums(is.na(feature_df))\n\n   RPDburglaryShifted     RPDRobberyShifted      total_burglaries \n                    0                     0                     0 \n      total_robberies   DwellingNoResidents      MedianHousePrice \n                    0                     0                     0 \n             FlatAprt      fullTimeStudents                NoCars \n                    0                     0                     0 \n     OtherEthnicGroup CentralHealingPercent             AvHholdSz \n                    0                     0                     0 \n            ComEstRes \n                    0 \n\npairs(feature_df)\n\n\n\n\nAs we hoped, some obvious relationships stand out: for instance, the presence of apartments, and households with no cars, or central heating and average household size.\nOur robbery and burglary data and change rates are densely clustered - they’re unlikely to cleanly associate with anything. With that in mind, we’ll perform a log transformation. This cannot be undertaken with negative values, so once again we’ll perform a shift (of 2) for both of our relative error numbers, as well as a commercial resident column, before log transforming our features.\n\nfeature_df$BurglaryRPDTranform <- feature_df$RPDburglaryShifted    + 2\nfeature_df$RobberyRPDTranform <- feature_df$RPDRobberyShifted   + 2\n\nfeature_df$ComEstResTranform <- feature_df$ComEstRes   + 2\n\n\nfor (col in colnames(feature_df)){\n  new_name <- paste(\"log_\", col, sep = \"\")\n  feature_df[new_name] <- log(feature_df[col])\n}\n\ndrop<- c( \"log_ComEstRes\", \"log_RPDburglaryShifted\", \"log_RPDRobberyShifted\")\nfeature_df<- feature_df[,!(names(feature_df) %in% drop)]\n\n\nfeat_transform_df <- feature_df[,17:29]\norig_feat_df <- feature_df[,0:17]\n\nWe’ve now separated a separate dataframe where each value has been log transformed - while this isn’t hugely rigorous (and would benefit from inspecting the relationships in more detail) it serves our immediate purpose.\n\npairs(feat_transform_df)\n\n\n\n\nWhile we’ve introduced a bit of noise, we’ve also “forced” some of our variables into relationships that look semi linear.\nTo dig into this deeper, let’s create a correlation matrix for our entire transformed dataframe.\n\ncorrelate(feat_transform_df)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\n\nterm\n\n\nlog_total_burglaries\n\n\nlog_total_robberies\n\n\nlog_DwellingNoResidents\n\n\nlog_MedianHousePrice\n\n\nlog_FlatAprt\n\n\nlog_fullTimeStudents\n\n\nlog_NoCars\n\n\nlog_OtherEthnicGroup\n\n\nlog_CentralHealingPercent\n\n\nlog_AvHholdSz\n\n\nlog_BurglaryRPDTranform\n\n\nlog_RobberyRPDTranform\n\n\nlog_ComEstResTranform\n\n\n\n\n\n\nlog_total_burglaries\n\n\nNA\n\n\n0.6793730\n\n\n0.5009180\n\n\n0.2641776\n\n\n0.5386775\n\n\n0.3830059\n\n\n0.5287612\n\n\n0.3864406\n\n\n-0.3234183\n\n\n-0.3632653\n\n\n-0.2224450\n\n\n-0.3858625\n\n\n0.2461299\n\n\n\n\nlog_total_robberies\n\n\n0.6793730\n\n\nNA\n\n\n0.3528253\n\n\n0.0289555\n\n\n0.6210392\n\n\n0.6262973\n\n\n0.7312742\n\n\n0.5328260\n\n\n-0.4392580\n\n\n-0.1885684\n\n\n-0.1143993\n\n\n-0.3860350\n\n\n0.2003534\n\n\n\n\nlog_DwellingNoResidents\n\n\n0.5009180\n\n\n0.3528253\n\n\nNA\n\n\n0.5259771\n\n\n0.5455129\n\n\n0.2061306\n\n\n0.4187217\n\n\n0.2388495\n\n\n-0.3673864\n\n\n-0.6022479\n\n\n-0.1801381\n\n\n-0.2106489\n\n\n0.2991287\n\n\n\n\nlog_MedianHousePrice\n\n\n0.2641776\n\n\n0.0289555\n\n\n0.5259771\n\n\nNA\n\n\n0.2266006\n\n\n-0.0590916\n\n\n0.0310431\n\n\n0.1415866\n\n\n-0.0743475\n\n\n-0.4236094\n\n\n-0.1667024\n\n\n-0.0843802\n\n\n0.1860743\n\n\n\n\nlog_FlatAprt\n\n\n0.5386775\n\n\n0.6210392\n\n\n0.5455129\n\n\n0.2266006\n\n\nNA\n\n\n0.4899513\n\n\n0.8808568\n\n\n0.5041155\n\n\n-0.4913225\n\n\n-0.5893529\n\n\n-0.0645864\n\n\n-0.1646088\n\n\n0.2781184\n\n\n\n\nlog_fullTimeStudents\n\n\n0.3830059\n\n\n0.6262973\n\n\n0.2061306\n\n\n-0.0590916\n\n\n0.4899513\n\n\nNA\n\n\n0.6188377\n\n\n0.6459824\n\n\n-0.3243579\n\n\n0.1050335\n\n\n-0.0491348\n\n\n-0.2206372\n\n\n0.2923494\n\n\n\n\nlog_NoCars\n\n\n0.5287612\n\n\n0.7312742\n\n\n0.4187217\n\n\n0.0310431\n\n\n0.8808568\n\n\n0.6188377\n\n\nNA\n\n\n0.5238758\n\n\n-0.5402424\n\n\n-0.4448476\n\n\n-0.0361088\n\n\n-0.2002598\n\n\n0.1658930\n\n\n\n\nlog_OtherEthnicGroup\n\n\n0.3864406\n\n\n0.5328260\n\n\n0.2388495\n\n\n0.1415866\n\n\n0.5041155\n\n\n0.6459824\n\n\n0.5238758\n\n\nNA\n\n\n-0.2366892\n\n\n0.0716831\n\n\n-0.0182390\n\n\n-0.1700667\n\n\n0.1260174\n\n\n\n\nlog_CentralHealingPercent\n\n\n-0.3234183\n\n\n-0.4392580\n\n\n-0.3673864\n\n\n-0.0743475\n\n\n-0.4913225\n\n\n-0.3243579\n\n\n-0.5402424\n\n\n-0.2366892\n\n\nNA\n\n\n0.4351582\n\n\n0.0943127\n\n\n0.2435584\n\n\n-0.1453510\n\n\n\n\nlog_AvHholdSz\n\n\n-0.3632653\n\n\n-0.1885684\n\n\n-0.6022479\n\n\n-0.4236094\n\n\n-0.5893529\n\n\n0.1050335\n\n\n-0.4448476\n\n\n0.0716831\n\n\n0.4351582\n\n\nNA\n\n\n0.1430359\n\n\n0.1747301\n\n\n-0.2479341\n\n\n\n\nlog_BurglaryRPDTranform\n\n\n-0.2224450\n\n\n-0.1143993\n\n\n-0.1801381\n\n\n-0.1667024\n\n\n-0.0645864\n\n\n-0.0491348\n\n\n-0.0361088\n\n\n-0.0182390\n\n\n0.0943127\n\n\n0.1430359\n\n\nNA\n\n\n0.1726291\n\n\n-0.1678558\n\n\n\n\nlog_RobberyRPDTranform\n\n\n-0.3858625\n\n\n-0.3860350\n\n\n-0.2106489\n\n\n-0.0843802\n\n\n-0.1646088\n\n\n-0.2206372\n\n\n-0.2002598\n\n\n-0.1700667\n\n\n0.2435584\n\n\n0.1747301\n\n\n0.1726291\n\n\nNA\n\n\n-0.1638404\n\n\n\n\nlog_ComEstResTranform\n\n\n0.2461299\n\n\n0.2003534\n\n\n0.2991287\n\n\n0.1860743\n\n\n0.2781184\n\n\n0.2923494\n\n\n0.1658930\n\n\n0.1260174\n\n\n-0.1453510\n\n\n-0.2479341\n\n\n-0.1678558\n\n\n-0.1638404\n\n\nNA\n\n\n\n\n\n\n\n\nTo provide a visual aid, I’ve extracted the column for our robbery relative change rate, and sorted the table accordingly.\n\ndplyr::select(correlate(feat_transform_df)[order(correlate(feat_transform_df)$log_BurglaryRPDTranform),], term, log_BurglaryRPDTranform)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\n\nterm\n\n\nlog_BurglaryRPDTranform\n\n\n\n\n\n\nlog_total_burglaries\n\n\n-0.2224450\n\n\n\n\nlog_DwellingNoResidents\n\n\n-0.1801381\n\n\n\n\nlog_ComEstResTranform\n\n\n-0.1678558\n\n\n\n\nlog_MedianHousePrice\n\n\n-0.1667024\n\n\n\n\nlog_total_robberies\n\n\n-0.1143993\n\n\n\n\nlog_FlatAprt\n\n\n-0.0645864\n\n\n\n\nlog_fullTimeStudents\n\n\n-0.0491348\n\n\n\n\nlog_NoCars\n\n\n-0.0361088\n\n\n\n\nlog_OtherEthnicGroup\n\n\n-0.0182390\n\n\n\n\nlog_CentralHealingPercent\n\n\n0.0943127\n\n\n\n\nlog_AvHholdSz\n\n\n0.1430359\n\n\n\n\nlog_RobberyRPDTranform\n\n\n0.1726291\n\n\n\n\nlog_BurglaryRPDTranform\n\n\nNA\n\n\n\n\n\n\n\n\n\n\nRegression\nNow that we’ve transformed our data, cleaned it up, and identified potential correlates, let’s build our linear model.\nThere are various automated tools for this process that seek to provide the highest fit and significance, but given the high degree of correlation between my chosen features, I’ve taken a more manual approach and tested a variety of models until I identified one with a suitable fit. The final model is below.\n\nmod_burglary <- lm(log_BurglaryRPDTranform ~ log_total_burglaries + log_FlatAprt + log_MedianHousePrice  + log_ComEstResTranform, data = feat_transform_df)\nsummary(mod_burglary)\n\n\nCall:\nlm(formula = log_BurglaryRPDTranform ~ log_total_burglaries + \n    log_FlatAprt + log_MedianHousePrice + log_ComEstResTranform, \n    data = feat_transform_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.39022 -0.06547 -0.00309  0.05830  0.56322 \n\nCoefficients:\n                       Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)            1.255068   0.112681  11.138 < 0.0000000000000002 ***\nlog_total_burglaries  -0.058851   0.009708  -6.062        0.00000000192 ***\nlog_FlatAprt           0.016153   0.005157   3.132             0.001786 ** \nlog_MedianHousePrice  -0.031691   0.009245  -3.428             0.000634 ***\nlog_ComEstResTranform -0.008125   0.002119  -3.833             0.000135 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1027 on 974 degrees of freedom\nMultiple R-squared:  0.08188,   Adjusted R-squared:  0.07811 \nF-statistic: 21.72 on 4 and 974 DF,  p-value: < 0.00000000000000022\n\n\nOur model suggests the largest burglary decrease linked to lockdown was in MSOAs which a high level of historic burglary. The composition of housing/accomodation and area type also seems to play a role, with those areas with higher median house prices, and a larger number of commercial residents, seeing stronger decreases, while conversely areas with large number of apartments temper the effect.\nWhile all our variables are significant, the model is not a particularly good fit - the adjusted R2 is around 0.08, suggesting that less than 10% of the variance is accounted for by our model. I suspect more geographic features - such as distance from central London, more accurate footfall, or spatial lags - would probably be useful, but that’s outside the scope of this project.\nLet’s perform a similar exercise for robbery.\n\ndplyr::select(correlate(feat_transform_df)[order(correlate(feat_transform_df)$log_RobberyRPDTranform),], term, log_RobberyRPDTranform)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\n\nterm\n\n\nlog_RobberyRPDTranform\n\n\n\n\n\n\nlog_total_robberies\n\n\n-0.3860350\n\n\n\n\nlog_total_burglaries\n\n\n-0.3858625\n\n\n\n\nlog_fullTimeStudents\n\n\n-0.2206372\n\n\n\n\nlog_DwellingNoResidents\n\n\n-0.2106489\n\n\n\n\nlog_NoCars\n\n\n-0.2002598\n\n\n\n\nlog_OtherEthnicGroup\n\n\n-0.1700667\n\n\n\n\nlog_FlatAprt\n\n\n-0.1646088\n\n\n\n\nlog_ComEstResTranform\n\n\n-0.1638404\n\n\n\n\nlog_MedianHousePrice\n\n\n-0.0843802\n\n\n\n\nlog_BurglaryRPDTranform\n\n\n0.1726291\n\n\n\n\nlog_AvHholdSz\n\n\n0.1747301\n\n\n\n\nlog_CentralHealingPercent\n\n\n0.2435584\n\n\n\n\nlog_RobberyRPDTranform\n\n\nNA\n\n\n\n\n\n\n\n\n\nmod_burglary <- lm(log_RobberyRPDTranform ~ log_total_robberies  + log_total_burglaries + log_FlatAprt + log_CentralHealingPercent + log_fullTimeStudents, data = feat_transform_df)\nsummary(mod_burglary)\n\n\nCall:\nlm(formula = log_RobberyRPDTranform ~ log_total_robberies + log_total_burglaries + \n    log_FlatAprt + log_CentralHealingPercent + log_fullTimeStudents, \n    data = feat_transform_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95406 -0.04243 -0.00535  0.04616  0.31826 \n\nCoefficients:\n                           Estimate Std. Error t value        Pr(>|t|)    \n(Intercept)               -5.041998   1.396500  -3.610        0.000321 ***\nlog_total_robberies       -0.030763   0.005495  -5.598 0.0000000282078 ***\nlog_total_burglaries      -0.066724   0.009757  -6.838 0.0000000000141 ***\nlog_FlatAprt               0.029516   0.005148   5.733 0.0000000131343 ***\nlog_CentralHealingPercent  1.302973   0.301809   4.317 0.0000174188038 ***\nlog_fullTimeStudents      -0.001894   0.006809  -0.278        0.780980    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08921 on 973 degrees of freedom\nMultiple R-squared:  0.2103,    Adjusted R-squared:  0.2062 \nF-statistic: 51.81 on 5 and 973 DF,  p-value: < 0.00000000000000022\n\n\nThis is a notably better fit than our burglary model, with our R2 suggesting we now account for over 20% of the variance. The nature of our predictors is also quite different: while we still see a negative relationship with historic crime (with areas of high historic crime experiencing larger relative decreases), there is a positive relationship with both the presence of apartments and central heating.\nI suspect some of these features are correlates of deprivation, so I want to create a quick scatter of three - for now we’ll do it against median house price, which is definitely deprivation correlated.\n\nheating <- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_CentralHealingPercent)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\napartments <- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_FlatAprt)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\ncomest <- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_ComEstResTranform)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\n\n\nggarrange(heating, apartments, comest, ncol=3, nrow=1 )\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nWhile there does appear to be a relationship with some of these, it isn’t strong - this suggests the factor’s we have identified are significant not because of their association with deprivation and poverty, but because of what they mean about the specific characteristics of the area.\n\n\nRandom Forests\nUnlike regression, random forest doesn’t really on any specific type of association - instead, we rely on computing power, repetition and iteration to capture the very best predictor for our variable, in any combination.\nThere are risks to this method: our sample size is smaller than I’d like, and this may lead to over-fit of outlier MSOAs.\nIt does mean we don’t need to worry about transformations or correlations: we can return to our original dataset, and let the model identify the strongest predictors.\n\n# we remove rows where our main error is na or missing\nrf_msoa_matrix <- drop_na(msoa_matrix_numeric, RPDRobberyShifted)\n\n#then we repeat for any columns where value asre missing\nclean_rf_matrix <- rf_msoa_matrix[ , colSums(is.na(rf_msoa_matrix)) == 0]\n\n\n#then we drop out any values that directly predict our error.\n\ndrop<- c(\"burglaryActual\",\"burglaryError\",\"burglaryPercentError\",\"burglaryPredicted\",\"robberyActual\",\"robberyPredicted\",\"robberyError\",\"robberyPercentError\", \"RPDBurglaryShifted\", \"RPDRobberyShifted\")\n\n#remove out selected columns\ndata<- clean_rf_matrix[,!(names(clean_rf_matrix) %in% drop)]\n\n\n#automatically remove white space and make names r compatbiel\nnames(clean_rf_matrix)<-make.names(names(clean_rf_matrix),unique = TRUE)\n\ndrop<- c(\"burglaryActual\",\"burglaryError\",\"robberyActual\",\"robberyError\",\"RPDBurglary\",\"RPDRobbery\",\"robberyActualShifted\",\"robberyPredictedShifted\",\"burglaryActualShifted\",\"burglaryPredictedShifted\", \"burglaryPredicted\", \"burglaryPercentError\", \"robberyPredicted\", \"robberyPercentError\")\n#drop<- c(\"burglaryPercentError\",\"burglaryPredicted\",\"robberyPredicted\",\"robberyPercentError\")\n\ndata<- clean_rf_matrix[,!(names(clean_rf_matrix) %in% drop)]\n\n\nnames(data)<- make.names(names(data),unique = TRUE)\nhead(data)\n\n\n\n\n\n\n\nUsualRes\n\n\nHholdRes\n\n\nComEstRes\n\n\nPopDen\n\n\nHholds\n\n\nAvHholdSz\n\n\ntotal_robberies\n\n\ntotal_burglaries\n\n\nRPDRobberyShifted\n\n\nRPDburglaryShifted\n\n\nAge.Structure..2011.Census..All.Ages\n\n\nAge.Structure..2011.Census..0.15\n\n\nAge.Structure..2011.Census..16.29\n\n\nAge.Structure..2011.Census..30.44\n\n\nAge.Structure..2011.Census..45.64\n\n\nAge.Structure..2011.Census..65.\n\n\nAge.Structure..2011.Census..Working.age\n\n\nMid.year.Estimate.totals.All.Ages.2002\n\n\nMid.year.Estimate.totals.All.Ages.2003\n\n\nMid.year.Estimate.totals.All.Ages.2004\n\n\nMid.year.Estimate.totals.All.Ages.2005\n\n\nMid.year.Estimate.totals.All.Ages.2006\n\n\nMid.year.Estimate.totals.All.Ages.2007\n\n\nMid.year.Estimate.totals.All.Ages.2008\n\n\nMid.year.Estimate.totals.All.Ages.2009\n\n\nMid.year.Estimate.totals.All.Ages.2010\n\n\nMid.year.Estimate.totals.All.Ages.2011\n\n\nMid.year.Estimate.totals.All.Ages.2012\n\n\nMid.year.Estimates.2012..by.age…0.to.14\n\n\nMid.year.Estimates.2012..by.age…15.64\n\n\nMid.year.Estimates.2012..by.age…65.\n\n\nMid.year.Estimates.2012..by.age.0.4\n\n\nMid.year.Estimates.2012..by.age.5.9\n\n\nMid.year.Estimates.2012..by.age.10.14\n\n\nMid.year.Estimates.2012..by.age.15.19\n\n\nMid.year.Estimates.2012..by.age.20.24\n\n\nMid.year.Estimates.2012..by.age.25.29\n\n\nMid.year.Estimates.2012..by.age.30.34\n\n\nMid.year.Estimates.2012..by.age.35.39\n\n\nMid.year.Estimates.2012..by.age.40.44\n\n\nMid.year.Estimates.2012..by.age.45.49\n\n\nMid.year.Estimates.2012..by.age.50.54\n\n\nMid.year.Estimates.2012..by.age.55.59\n\n\nMid.year.Estimates.2012..by.age.60.64\n\n\nMid.year.Estimates.2012..by.age.65.69\n\n\nMid.year.Estimates.2012..by.age.70.74\n\n\nMid.year.Estimates.2012..by.age.75.79\n\n\nMid.year.Estimates.2012..by.age.80.84\n\n\nMid.year.Estimates.2012..by.age.85.89\n\n\nMid.year.Estimates.2012..by.age.90.\n\n\nHouseholds..2011..All.Households\n\n\nHousehold.Composition..2011..Numbers.Couple.household.with.dependent.children\n\n\nHousehold.Composition..2011..Numbers.Couple.household.without.dependent.children\n\n\nHousehold.Composition..2011..Numbers.Lone.parent.household\n\n\nHousehold.Composition..2011..Numbers.One.person.household\n\n\nHousehold.Composition..2011..Numbers.Other.household.Types\n\n\nHousehold.Composition..2011..Percentages.Couple.household.with.dependent.children\n\n\nHousehold.Composition..2011..Percentages.Couple.household.without.dependent.children\n\n\nHousehold.Composition..2011..Percentages.Lone.parent.household\n\n\nHousehold.Composition..2011..Percentages.One.person.household\n\n\nHousehold.Composition..2011..Percentages.Other.household.Types\n\n\nEthnic.Group..2011.Census..White\n\n\nEthnic.Group..2011.Census..Mixed.multiple.ethnic.groups\n\n\nEthnic.Group..2011.Census..Asian.Asian.British\n\n\nEthnic.Group..2011.Census..Black.African.Caribbean.Black.British\n\n\nEthnic.Group..2011.Census..Other.ethnic.group\n\n\nEthnic.Group..2011.Census..BAME\n\n\nEthnic.Group..2011.Census..White….\n\n\nEthnic.Group..2011.Census..Mixed.multiple.ethnic.groups….\n\n\nEthnic.Group..2011.Census..Asian.Asian.British….\n\n\nEthnic.Group..2011.Census..Black.African.Caribbean.Black.British….\n\n\nEthnic.Group..2011.Census..Other.ethnic.group….\n\n\nEthnic.Group..2011.Census..BAME….\n\n\nCountry.of.Birth..2011..United.Kingdom\n\n\nCountry.of.Birth..2011..Not.United.Kingdom\n\n\nCountry.of.Birth..2011..United.Kingdom….\n\n\nCountry.of.Birth..2011..Not.United.Kingdom….\n\n\nHousehold.Language..2011..At.least.one.person.aged.16.and.over.in.household.has.English.as.a.main.language\n\n\nHousehold.Language..2011..No.people.in.household.have.English.as.a.main.language\n\n\nHousehold.Language..2011….of.people.aged.16.and.over.in.household.have.English.as.a.main.language\n\n\nHousehold.Language..2011….of.households.where.no.people.in.household.have.English.as.a.main.language\n\n\nReligion..2011..Christian\n\n\nReligion..2011..Buddhist\n\n\nReligion..2011..Hindu\n\n\nReligion..2011..Jewish\n\n\nReligion..2011..Muslim\n\n\nReligion..2011..Sikh\n\n\nReligion..2011..Other.religion\n\n\nReligion..2011..No.religion\n\n\nReligion..2011..Religion.not.stated\n\n\nReligion..2011..Christian….\n\n\nReligion..2011..Buddhist….\n\n\nReligion..2011..Hindu….\n\n\nReligion..2011..Jewish….\n\n\nReligion..2011..Muslim….\n\n\nReligion..2011..Sikh….\n\n\nReligion..2011..Other.religion….\n\n\nReligion..2011..No.religion….\n\n\nReligion..2011..Religion.not.stated….\n\n\nTenure..2011..Owned..Owned.outright\n\n\nTenure..2011..Owned..Owned.with.a.mortgage.or.loan\n\n\nTenure..2011..Social.rented\n\n\nTenure..2011..Private.rented\n\n\nTenure..2011..Owned..Owned.outright….\n\n\nTenure..2011..Owned..Owned.with.a.mortgage.or.loan….\n\n\nTenure..2011..Social.rented….\n\n\nTenure..2011..Private.rented….\n\n\nDwelling.type..2011..Household.spaces.with.at.least.one.usual.resident\n\n\nDwelling.type..2011..Household.spaces.with.no.usual.residents\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Detached\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Semi.detached\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Terraced..including.end.terrace.\n\n\nDwelling.type..2011..Flat..maisonette.or.apartment\n\n\nDwelling.type..2011..Household.spaces.with.at.least.one.usual.resident….\n\n\nDwelling.type..2011..Household.spaces.with.no.usual.residents….\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Detached….\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Semi.detached….\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Terraced..including.end.terrace…..\n\n\nDwelling.type..2011..Flat..maisonette.or.apartment….\n\n\nLand.Area.Hectares\n\n\nPopulation.Density.Persons.per.hectare..2012.\n\n\nHouse.Prices.Median.House.Price…..2005\n\n\nHouse.Prices.Median.House.Price…..2006\n\n\nHouse.Prices.Median.House.Price…..2007\n\n\nHouse.Prices.Median.House.Price…..2008\n\n\nHouse.Prices.Median.House.Price…..2009\n\n\nHouse.Prices.Median.House.Price…..2010\n\n\nHouse.Prices.Median.House.Price…..2011\n\n\nHouse.Prices.Median.House.Price…..2012\n\n\nHouse.Prices.Median.House.Price…..2013..p.\n\n\nHouse.Prices.Sales.2005\n\n\nHouse.Prices.Sales.2006\n\n\nHouse.Prices.Sales.2007\n\n\nHouse.Prices.Sales.2008\n\n\nHouse.Prices.Sales.2009\n\n\nHouse.Prices.Sales.2010\n\n\nHouse.Prices.Sales.2011…129\n\n\nHouse.Prices.Sales.2011…130\n\n\nHouse.Prices.Sales.2013.p.\n\n\nQualifications..2011.Census..No.qualifications\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Level.1.qualifications\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Level.2.qualifications\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Apprenticeship\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Level.3.qualifications\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Level.4.qualifications.and.above\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Other.qualifications\n\n\nQualifications..2011.Census..Schoolchildren.and.full.time.students..Age.18.and.over\n\n\nEconomic.Activity..2011.Census..Economically.active..Total\n\n\nEconomic.Activity..2011.Census..Economically.active..Unemployed\n\n\nEconomic.Activity..2011.Census..Economically.inactive..Total\n\n\nEconomic.Activity..2011.Census..Economically.active..\n\n\nEconomic.Activity..2011.Census..Unemployment.Rate\n\n\nEconomic.Activity..2011.Census..Economically.inactive..\n\n\nAdults.in.Employment..2011.Census..No.adults.in.employment.in.household..With.dependent.children\n\n\nAdults.in.Employment..2011.Census….of.households.with.no.adults.in.employment..With.dependent.children\n\n\nHousehold.Income.Estimates..2011.12..Total.Mean.Annual.Household.Income….\n\n\nHousehold.Income.Estimates..2011.12..Total.Median.Annual.Household.Income….\n\n\nIncome.Deprivation..2010….living.in.income.deprived.households.reliant.on.means.tested.benefit\n\n\nIncome.Deprivation..2010….of.people.aged.over.60.who.live.in.pension.credit.households\n\n\nLone.Parents..2011.Census..All.lone.parent.housholds.with.dependent.children\n\n\nLone.Parents..2011.Census..Lone.parents.not.in.employment\n\n\nLone.Parents..2011.Census..Lone.parent.not.in.employment..\n\n\nCentral.Heating..2011.Census..Households.with.central.heating….\n\n\nHealth..2011.Census..Day.to.day.activities.limited.a.lot\n\n\nHealth..2011.Census..Day.to.day.activities.limited.a.little\n\n\nHealth..2011.Census..Day.to.day.activities.not.limited\n\n\nHealth..2011.Census..Day.to.day.activities.limited.a.lot….\n\n\nHealth..2011.Census..Day.to.day.activities.limited.a.little….\n\n\nHealth..2011.Census..Day.to.day.activities.not.limited….\n\n\nHealth..2011.Census..Very.good.health\n\n\nHealth..2011.Census..Good.health\n\n\nHealth..2011.Census..Fair.health\n\n\nHealth..2011.Census..Bad.health\n\n\nHealth..2011.Census..Very.bad.health\n\n\nHealth..2011.Census..Very.good.health….\n\n\nHealth..2011.Census..Good.health….\n\n\nHealth..2011.Census..Fair.health….\n\n\nHealth..2011.Census..Bad.health….\n\n\nHealth..2011.Census..Very.bad.health….\n\n\nLow.Birth.Weight.Births..2007.2011..Low.Birth.Weight.Births….\n\n\nLow.Birth.Weight.Births..2007.2011..LCL…Lower.confidence.limit\n\n\nLow.Birth.Weight.Births..2007.2011..UCL…Upper.confidence.limit\n\n\nObesity.Percentage.of.the.population.aged.16..with.a.BMI.of.30…modelled.estimate..2006.2008\n\n\nIncidence.of.Cancer.All\n\n\nIncidence.of.Cancer.Breast.Cancer\n\n\nLife.Expectancy.Males\n\n\nLife.Expectancy.Females\n\n\nCar.or.van.availability..2011.Census..No.cars.or.vans.in.household\n\n\nCar.or.van.availability..2011.Census..1.car.or.van.in.household\n\n\nCar.or.van.availability..2011.Census..2.cars.or.vans.in.household\n\n\nCar.or.van.availability..2011.Census..3.cars.or.vans.in.household\n\n\nCar.or.van.availability..2011.Census..4.or.more.cars.or.vans.in.household\n\n\nCar.or.van.availability..2011.Census..Sum.of.all.cars.or.vans.in.the.area\n\n\nCar.or.van.availability..2011.Census..No.cars.or.vans.in.household….\n\n\nCar.or.van.availability..2011.Census..1.car.or.van.in.household….\n\n\nCar.or.van.availability..2011.Census..2.cars.or.vans.in.household….\n\n\nCar.or.van.availability..2011.Census..3.cars.or.vans.in.household….\n\n\nCar.or.van.availability..2011.Census..4.or.more.cars.or.vans.in.household….\n\n\nCar.or.van.availability..2011.Census..Cars.per.household\n\n\nRoad.Casualties.2010.Fatal\n\n\nRoad.Casualties.2010.Serious\n\n\nRoad.Casualties.2010.Slight\n\n\nRoad.Casualties.2010.2010.Total\n\n\nRoad.Casualties.2011.Fatal\n\n\nRoad.Casualties.2011.Serious\n\n\nRoad.Casualties.2011.Slight\n\n\nRoad.Casualties.2011.2011.Total\n\n\nRoad.Casualties.2012.Fatal\n\n\nRoad.Casualties.2012.Serious\n\n\nRoad.Casualties.2012.Slight\n\n\nRoad.Casualties.2012.2012.Total\n\n\n\n\n\n\n7375\n\n\n7187\n\n\n188\n\n\n25.5\n\n\n4385\n\n\n1.6\n\n\n53\n\n\n58\n\n\n0.0601204\n\n\n-0.1219796\n\n\n7375\n\n\n620\n\n\n1665\n\n\n2045\n\n\n2010\n\n\n1035\n\n\n5720\n\n\n7280\n\n\n7115\n\n\n7118\n\n\n7131\n\n\n7254\n\n\n7607\n\n\n7429\n\n\n7472\n\n\n7338\n\n\n7412\n\n\n7604\n\n\n8.771699\n\n\n76.68332\n\n\n14.54498\n\n\n297\n\n\n205\n\n\n165\n\n\n231\n\n\n495\n\n\n949\n\n\n826\n\n\n622\n\n\n663\n\n\n598\n\n\n504\n\n\n470\n\n\n473\n\n\n363\n\n\n263\n\n\n192\n\n\n155\n\n\n86\n\n\n47\n\n\n4385\n\n\n306\n\n\n927\n\n\n153\n\n\n2472\n\n\n527\n\n\n6.978335\n\n\n21.14025\n\n\n3.489168\n\n\n56.37400\n\n\n12.01824\n\n\n5799\n\n\n289\n\n\n940\n\n\n193\n\n\n154\n\n\n1576\n\n\n78.63051\n\n\n3.918644\n\n\n12.745763\n\n\n2.616949\n\n\n2.0881356\n\n\n21.36949\n\n\n4670\n\n\n2705\n\n\n63.32203\n\n\n36.67797\n\n\n3825\n\n\n560\n\n\n87.22919\n\n\n12.770810\n\n\n3344\n\n\n92\n\n\n145\n\n\n166\n\n\n409\n\n\n18\n\n\n28\n\n\n2522\n\n\n651\n\n\n45.3\n\n\n1.2\n\n\n2.0\n\n\n2.3\n\n\n5.5\n\n\n0.2\n\n\n0.4\n\n\n34.2\n\n\n8.8\n\n\n1093\n\n\n762\n\n\n725\n\n\n1573\n\n\n24.9\n\n\n17.4\n\n\n16.5\n\n\n35.9\n\n\n4385\n\n\n1145\n\n\n22\n\n\n12\n\n\n80\n\n\n5416\n\n\n79.3\n\n\n20.7\n\n\n0.4\n\n\n0.2\n\n\n1.4\n\n\n98.0\n\n\n289.78\n\n\n26.24060\n\n\n310000\n\n\n341000\n\n\n412500\n\n\n365000.0\n\n\n410000\n\n\n450000\n\n\n465000\n\n\n485000\n\n\n595000\n\n\n303\n\n\n295\n\n\n268\n\n\n141\n\n\n157\n\n\n235\n\n\n256\n\n\n195\n\n\n353\n\n\n454\n\n\n291\n\n\n445\n\n\n47\n\n\n484\n\n\n4618\n\n\n416\n\n\n422\n\n\n4972\n\n\n187\n\n\n1335\n\n\n78.83304\n\n\n3.761062\n\n\n21.16696\n\n\n38\n\n\n0.9\n\n\n59728.48\n\n\n46788.30\n\n\n5.2\n\n\n9.9\n\n\n91\n\n\n22\n\n\n24.2\n\n\n95.7\n\n\n328\n\n\n520\n\n\n6527\n\n\n4.447458\n\n\n7.050847\n\n\n88.50169\n\n\n4112\n\n\n2374\n\n\n643\n\n\n190\n\n\n56\n\n\n55.75593\n\n\n32.18983\n\n\n8.718644\n\n\n2.576271\n\n\n0.759322\n\n\n4.2\n\n\n2.5\n\n\n7.1\n\n\n13.7\n\n\n76.8\n\n\n90.9\n\n\n83.6\n\n\n88.4\n\n\n3043\n\n\n1100\n\n\n173\n\n\n51\n\n\n18\n\n\n1692\n\n\n69.4\n\n\n25.1\n\n\n3.9\n\n\n1.2\n\n\n0.4\n\n\n0.3858609\n\n\n1\n\n\n39\n\n\n334\n\n\n374\n\n\n0\n\n\n46\n\n\n359\n\n\n405\n\n\n2\n\n\n51\n\n\n361\n\n\n414\n\n\n\n\n6775\n\n\n6724\n\n\n51\n\n\n31.3\n\n\n2713\n\n\n2.5\n\n\n53\n\n\n136\n\n\n-0.0223392\n\n\n0.3489691\n\n\n6775\n\n\n1751\n\n\n1277\n\n\n1388\n\n\n1258\n\n\n1101\n\n\n3923\n\n\n6333\n\n\n6312\n\n\n6329\n\n\n6341\n\n\n6330\n\n\n6323\n\n\n6369\n\n\n6570\n\n\n6636\n\n\n6783\n\n\n6853\n\n\n25.113089\n\n\n58.73340\n\n\n16.15351\n\n\n652\n\n\n607\n\n\n462\n\n\n458\n\n\n399\n\n\n468\n\n\n466\n\n\n466\n\n\n461\n\n\n450\n\n\n347\n\n\n254\n\n\n256\n\n\n254\n\n\n206\n\n\n215\n\n\n201\n\n\n137\n\n\n94\n\n\n2713\n\n\n491\n\n\n366\n\n\n597\n\n\n814\n\n\n445\n\n\n18.098046\n\n\n13.49060\n\n\n22.005160\n\n\n30.00369\n\n\n16.40251\n\n\n4403\n\n\n330\n\n\n820\n\n\n1133\n\n\n89\n\n\n2372\n\n\n64.98893\n\n\n4.870849\n\n\n12.103321\n\n\n16.723247\n\n\n1.3136531\n\n\n35.01107\n\n\n5159\n\n\n1616\n\n\n76.14760\n\n\n23.85240\n\n\n2459\n\n\n254\n\n\n90.63767\n\n\n9.362329\n\n\n3975\n\n\n19\n\n\n174\n\n\n27\n\n\n591\n\n\n122\n\n\n16\n\n\n1417\n\n\n434\n\n\n58.7\n\n\n0.3\n\n\n2.6\n\n\n0.4\n\n\n8.7\n\n\n1.8\n\n\n0.2\n\n\n20.9\n\n\n6.4\n\n\n596\n\n\n663\n\n\n1133\n\n\n269\n\n\n22.0\n\n\n24.4\n\n\n41.8\n\n\n9.9\n\n\n2713\n\n\n82\n\n\n99\n\n\n744\n\n\n865\n\n\n1087\n\n\n97.1\n\n\n2.9\n\n\n3.5\n\n\n26.6\n\n\n30.9\n\n\n38.9\n\n\n216.15\n\n\n31.70483\n\n\n168500\n\n\n180000\n\n\n187500\n\n\n197500.0\n\n\n190000\n\n\n173000\n\n\n185000\n\n\n182250\n\n\n190000\n\n\n81\n\n\n100\n\n\n100\n\n\n68\n\n\n45\n\n\n61\n\n\n51\n\n\n42\n\n\n61\n\n\n1623\n\n\n789\n\n\n706\n\n\n118\n\n\n479\n\n\n914\n\n\n395\n\n\n272\n\n\n2847\n\n\n335\n\n\n1513\n\n\n65.29817\n\n\n11.766772\n\n\n34.70183\n\n\n319\n\n\n11.8\n\n\n31788.19\n\n\n27058.70\n\n\n31.0\n\n\n27.5\n\n\n445\n\n\n249\n\n\n56.0\n\n\n97.5\n\n\n707\n\n\n678\n\n\n5390\n\n\n10.435424\n\n\n10.007380\n\n\n79.55720\n\n\n2933\n\n\n2288\n\n\n1059\n\n\n389\n\n\n106\n\n\n43.29151\n\n\n33.77122\n\n\n15.630996\n\n\n5.741697\n\n\n1.564576\n\n\n10.6\n\n\n8.4\n\n\n13.3\n\n\n29.8\n\n\n100.7\n\n\n93.3\n\n\n78.0\n\n\n80.1\n\n\n1020\n\n\n1186\n\n\n424\n\n\n66\n\n\n17\n\n\n2305\n\n\n37.6\n\n\n43.7\n\n\n15.6\n\n\n2.4\n\n\n0.6\n\n\n0.8496130\n\n\n0\n\n\n0\n\n\n18\n\n\n18\n\n\n0\n\n\n2\n\n\n16\n\n\n18\n\n\n0\n\n\n1\n\n\n15\n\n\n16\n\n\n\n\n10045\n\n\n10033\n\n\n12\n\n\n46.9\n\n\n3834\n\n\n2.6\n\n\n124\n\n\n239\n\n\n0.0526430\n\n\n-0.0218569\n\n\n10045\n\n\n2247\n\n\n1959\n\n\n2300\n\n\n2259\n\n\n1280\n\n\n6518\n\n\n9236\n\n\n9252\n\n\n9155\n\n\n9072\n\n\n9144\n\n\n9227\n\n\n9564\n\n\n9914\n\n\n10042\n\n\n10088\n\n\n10218\n\n\n21.442552\n\n\n65.81523\n\n\n12.74222\n\n\n849\n\n\n739\n\n\n603\n\n\n615\n\n\n686\n\n\n804\n\n\n818\n\n\n743\n\n\n770\n\n\n713\n\n\n624\n\n\n526\n\n\n426\n\n\n372\n\n\n277\n\n\n258\n\n\n198\n\n\n124\n\n\n73\n\n\n3834\n\n\n776\n\n\n730\n\n\n589\n\n\n1039\n\n\n700\n\n\n20.239958\n\n\n19.04017\n\n\n15.362546\n\n\n27.09963\n\n\n18.25769\n\n\n5486\n\n\n433\n\n\n2284\n\n\n1618\n\n\n224\n\n\n4559\n\n\n54.61424\n\n\n4.310602\n\n\n22.737680\n\n\n16.107516\n\n\n2.2299652\n\n\n45.38576\n\n\n7193\n\n\n2852\n\n\n71.60777\n\n\n28.39223\n\n\n3431\n\n\n403\n\n\n89.48878\n\n\n10.511215\n\n\n5475\n\n\n46\n\n\n476\n\n\n42\n\n\n1351\n\n\n430\n\n\n34\n\n\n1514\n\n\n677\n\n\n54.5\n\n\n0.5\n\n\n4.7\n\n\n0.4\n\n\n13.4\n\n\n4.3\n\n\n0.3\n\n\n15.1\n\n\n6.7\n\n\n1028\n\n\n1473\n\n\n446\n\n\n830\n\n\n26.8\n\n\n38.4\n\n\n11.6\n\n\n21.6\n\n\n3834\n\n\n110\n\n\n161\n\n\n936\n\n\n1591\n\n\n1256\n\n\n97.2\n\n\n2.8\n\n\n4.1\n\n\n23.7\n\n\n40.3\n\n\n31.8\n\n\n214.15\n\n\n47.71422\n\n\n185000\n\n\n197750\n\n\n220000\n\n\n225000.0\n\n\n188500\n\n\n215000\n\n\n200000\n\n\n205500\n\n\n237000\n\n\n203\n\n\n232\n\n\n295\n\n\n100\n\n\n80\n\n\n97\n\n\n89\n\n\n114\n\n\n98\n\n\n1778\n\n\n1210\n\n\n1236\n\n\n169\n\n\n847\n\n\n1829\n\n\n729\n\n\n442\n\n\n5038\n\n\n459\n\n\n2111\n\n\n70.47139\n\n\n9.110758\n\n\n29.52861\n\n\n268\n\n\n7.0\n\n\n43356.93\n\n\n36834.53\n\n\n18.9\n\n\n21.2\n\n\n408\n\n\n199\n\n\n48.8\n\n\n96.5\n\n\n792\n\n\n810\n\n\n8443\n\n\n7.884520\n\n\n8.063713\n\n\n84.05177\n\n\n4566\n\n\n3633\n\n\n1325\n\n\n406\n\n\n115\n\n\n45.45545\n\n\n36.16725\n\n\n13.190642\n\n\n4.041812\n\n\n1.144848\n\n\n7.8\n\n\n6.1\n\n\n9.9\n\n\n28.3\n\n\n91.4\n\n\n107.3\n\n\n80.2\n\n\n85.6\n\n\n1196\n\n\n1753\n\n\n691\n\n\n155\n\n\n39\n\n\n3766\n\n\n31.2\n\n\n45.7\n\n\n18.0\n\n\n4.0\n\n\n1.0\n\n\n0.9822640\n\n\n0\n\n\n3\n\n\n34\n\n\n37\n\n\n1\n\n\n4\n\n\n40\n\n\n45\n\n\n0\n\n\n3\n\n\n47\n\n\n50\n\n\n\n\n6182\n\n\n5937\n\n\n245\n\n\n24.8\n\n\n2318\n\n\n2.6\n\n\n30\n\n\n81\n\n\n0.0257532\n\n\n0.1381492\n\n\n6182\n\n\n1196\n\n\n1277\n\n\n1154\n\n\n1543\n\n\n1012\n\n\n3974\n\n\n6208\n\n\n6159\n\n\n6163\n\n\n6152\n\n\n5997\n\n\n6005\n\n\n6084\n\n\n6268\n\n\n6237\n\n\n6185\n\n\n6308\n\n\n18.246671\n\n\n65.82118\n\n\n15.93215\n\n\n409\n\n\n364\n\n\n378\n\n\n516\n\n\n472\n\n\n435\n\n\n363\n\n\n399\n\n\n388\n\n\n463\n\n\n459\n\n\n354\n\n\n303\n\n\n226\n\n\n238\n\n\n194\n\n\n172\n\n\n106\n\n\n69\n\n\n2318\n\n\n508\n\n\n524\n\n\n322\n\n\n609\n\n\n355\n\n\n21.915444\n\n\n22.60569\n\n\n13.891286\n\n\n26.27265\n\n\n15.31493\n\n\n5006\n\n\n186\n\n\n313\n\n\n649\n\n\n28\n\n\n1176\n\n\n80.97703\n\n\n3.008735\n\n\n5.063086\n\n\n10.498221\n\n\n0.4529279\n\n\n19.02297\n\n\n5292\n\n\n890\n\n\n85.60336\n\n\n14.39664\n\n\n2214\n\n\n104\n\n\n95.51337\n\n\n4.486626\n\n\n4070\n\n\n21\n\n\n34\n\n\n23\n\n\n234\n\n\n10\n\n\n18\n\n\n1373\n\n\n399\n\n\n65.8\n\n\n0.3\n\n\n0.5\n\n\n0.4\n\n\n3.8\n\n\n0.2\n\n\n0.3\n\n\n22.2\n\n\n6.5\n\n\n718\n\n\n969\n\n\n371\n\n\n228\n\n\n31.0\n\n\n41.8\n\n\n16.0\n\n\n9.8\n\n\n2318\n\n\n45\n\n\n92\n\n\n858\n\n\n1094\n\n\n314\n\n\n98.1\n\n\n1.9\n\n\n3.9\n\n\n36.3\n\n\n46.3\n\n\n13.2\n\n\n249.28\n\n\n25.30488\n\n\n181000\n\n\n182000\n\n\n212000\n\n\n208997.5\n\n\n182000\n\n\n192000\n\n\n193750\n\n\n205000\n\n\n210000\n\n\n93\n\n\n131\n\n\n125\n\n\n66\n\n\n54\n\n\n61\n\n\n72\n\n\n58\n\n\n67\n\n\n1502\n\n\n800\n\n\n825\n\n\n163\n\n\n539\n\n\n891\n\n\n266\n\n\n215\n\n\n3187\n\n\n296\n\n\n1251\n\n\n71.81163\n\n\n9.287731\n\n\n28.18837\n\n\n122\n\n\n5.3\n\n\n46701.44\n\n\n39668.21\n\n\n15.8\n\n\n21.3\n\n\n206\n\n\n87\n\n\n42.2\n\n\n98.5\n\n\n586\n\n\n547\n\n\n5049\n\n\n9.479133\n\n\n8.848269\n\n\n81.67260\n\n\n2857\n\n\n2086\n\n\n861\n\n\n302\n\n\n76\n\n\n46.21482\n\n\n33.74313\n\n\n13.927532\n\n\n4.885150\n\n\n1.229376\n\n\n5.8\n\n\n3.9\n\n\n8.6\n\n\n26.9\n\n\n96.1\n\n\n95.5\n\n\n77.9\n\n\n80.7\n\n\n556\n\n\n1085\n\n\n515\n\n\n128\n\n\n34\n\n\n2650\n\n\n24.0\n\n\n46.8\n\n\n22.2\n\n\n5.5\n\n\n1.5\n\n\n1.1432269\n\n\n0\n\n\n1\n\n\n13\n\n\n14\n\n\n0\n\n\n2\n\n\n7\n\n\n9\n\n\n0\n\n\n2\n\n\n5\n\n\n7\n\n\n\n\n8562\n\n\n8562\n\n\n0\n\n\n72.1\n\n\n3183\n\n\n2.7\n\n\n87\n\n\n161\n\n\n-0.1490559\n\n\n-0.0107732\n\n\n8562\n\n\n2200\n\n\n1592\n\n\n1995\n\n\n1829\n\n\n946\n\n\n5416\n\n\n7919\n\n\n7922\n\n\n7882\n\n\n7887\n\n\n7917\n\n\n7916\n\n\n8025\n\n\n8317\n\n\n8519\n\n\n8588\n\n\n8660\n\n\n24.237875\n\n\n64.57275\n\n\n11.18938\n\n\n783\n\n\n692\n\n\n624\n\n\n657\n\n\n525\n\n\n608\n\n\n616\n\n\n643\n\n\n673\n\n\n656\n\n\n508\n\n\n386\n\n\n320\n\n\n321\n\n\n202\n\n\n194\n\n\n127\n\n\n90\n\n\n35\n\n\n3183\n\n\n691\n\n\n583\n\n\n593\n\n\n808\n\n\n508\n\n\n21.709080\n\n\n18.31605\n\n\n18.630223\n\n\n25.38486\n\n\n15.95979\n\n\n5674\n\n\n313\n\n\n1050\n\n\n1445\n\n\n80\n\n\n2888\n\n\n66.26956\n\n\n3.655688\n\n\n12.263490\n\n\n16.876898\n\n\n0.9343611\n\n\n33.73044\n\n\n6425\n\n\n2137\n\n\n75.04088\n\n\n24.95912\n\n\n2868\n\n\n315\n\n\n90.10368\n\n\n9.896324\n\n\n4986\n\n\n28\n\n\n138\n\n\n35\n\n\n762\n\n\n166\n\n\n13\n\n\n1816\n\n\n618\n\n\n58.2\n\n\n0.3\n\n\n1.6\n\n\n0.4\n\n\n8.9\n\n\n1.9\n\n\n0.2\n\n\n21.2\n\n\n7.2\n\n\n711\n\n\n1146\n\n\n793\n\n\n482\n\n\n22.3\n\n\n36.0\n\n\n24.9\n\n\n15.1\n\n\n3183\n\n\n89\n\n\n136\n\n\n622\n\n\n2141\n\n\n373\n\n\n97.3\n\n\n2.7\n\n\n4.2\n\n\n19.0\n\n\n65.4\n\n\n11.4\n\n\n118.81\n\n\n72.88949\n\n\n162250\n\n\n170000\n\n\n185000\n\n\n200000.0\n\n\n166250\n\n\n169000\n\n\n165000\n\n\n167000\n\n\n180000\n\n\n152\n\n\n168\n\n\n184\n\n\n73\n\n\n60\n\n\n58\n\n\n81\n\n\n59\n\n\n75\n\n\n1839\n\n\n1026\n\n\n1038\n\n\n160\n\n\n653\n\n\n1119\n\n\n527\n\n\n333\n\n\n4052\n\n\n394\n\n\n1881\n\n\n68.29597\n\n\n9.723593\n\n\n31.70403\n\n\n307\n\n\n9.6\n\n\n34293.82\n\n\n29155.68\n\n\n22.9\n\n\n25.3\n\n\n442\n\n\n231\n\n\n52.3\n\n\n96.3\n\n\n713\n\n\n722\n\n\n7127\n\n\n8.327494\n\n\n8.432609\n\n\n83.23990\n\n\n4089\n\n\n2811\n\n\n1134\n\n\n412\n\n\n116\n\n\n47.75753\n\n\n32.83111\n\n\n13.244569\n\n\n4.811960\n\n\n1.354824\n\n\n7.7\n\n\n6.0\n\n\n9.9\n\n\n29.7\n\n\n110.0\n\n\n106.1\n\n\n76.8\n\n\n79.9\n\n\n1080\n\n\n1423\n\n\n551\n\n\n109\n\n\n20\n\n\n2937\n\n\n33.9\n\n\n44.7\n\n\n17.3\n\n\n3.4\n\n\n0.6\n\n\n0.9227144\n\n\n1\n\n\n5\n\n\n24\n\n\n30\n\n\n0\n\n\n2\n\n\n27\n\n\n29\n\n\n0\n\n\n1\n\n\n24\n\n\n25\n\n\n\n\n8791\n\n\n8672\n\n\n119\n\n\n50.6\n\n\n3441\n\n\n2.5\n\n\n96\n\n\n136\n\n\n0.0702553\n\n\n-0.1401784\n\n\n8791\n\n\n2388\n\n\n1765\n\n\n1867\n\n\n1736\n\n\n1035\n\n\n5368\n\n\n7806\n\n\n7726\n\n\n7771\n\n\n7820\n\n\n7877\n\n\n7974\n\n\n8145\n\n\n8394\n\n\n8571\n\n\n8823\n\n\n9076\n\n\n26.586602\n\n\n62.02071\n\n\n11.39268\n\n\n957\n\n\n792\n\n\n664\n\n\n633\n\n\n642\n\n\n686\n\n\n634\n\n\n651\n\n\n596\n\n\n610\n\n\n483\n\n\n398\n\n\n296\n\n\n242\n\n\n207\n\n\n180\n\n\n181\n\n\n139\n\n\n85\n\n\n3441\n\n\n643\n\n\n489\n\n\n776\n\n\n1064\n\n\n469\n\n\n18.686428\n\n\n14.21099\n\n\n22.551584\n\n\n30.92124\n\n\n13.62976\n\n\n5906\n\n\n307\n\n\n526\n\n\n1997\n\n\n55\n\n\n2885\n\n\n67.18235\n\n\n3.492208\n\n\n5.983392\n\n\n22.716414\n\n\n0.6256399\n\n\n32.81765\n\n\n6658\n\n\n2133\n\n\n75.73655\n\n\n24.26345\n\n\n3117\n\n\n324\n\n\n90.58413\n\n\n9.415867\n\n\n5409\n\n\n22\n\n\n67\n\n\n13\n\n\n577\n\n\n19\n\n\n24\n\n\n2148\n\n\n512\n\n\n61.5\n\n\n0.3\n\n\n0.8\n\n\n0.1\n\n\n6.6\n\n\n0.2\n\n\n0.3\n\n\n24.4\n\n\n5.8\n\n\n558\n\n\n821\n\n\n1663\n\n\n333\n\n\n16.2\n\n\n23.9\n\n\n48.3\n\n\n9.7\n\n\n3441\n\n\n93\n\n\n82\n\n\n761\n\n\n1219\n\n\n1471\n\n\n97.4\n\n\n2.6\n\n\n2.3\n\n\n21.5\n\n\n34.5\n\n\n41.6\n\n\n173.58\n\n\n52.28713\n\n\n165000\n\n\n165000\n\n\n187000\n\n\n199000.0\n\n\n155000\n\n\n165000\n\n\n153750\n\n\n172500\n\n\n168500\n\n\n112\n\n\n133\n\n\n139\n\n\n39\n\n\n40\n\n\n75\n\n\n76\n\n\n48\n\n\n55\n\n\n2057\n\n\n1065\n\n\n1001\n\n\n166\n\n\n556\n\n\n1129\n\n\n429\n\n\n402\n\n\n3905\n\n\n511\n\n\n1893\n\n\n67.35081\n\n\n13.085788\n\n\n32.64919\n\n\n437\n\n\n12.7\n\n\n29975.83\n\n\n25568.60\n\n\n33.3\n\n\n28.9\n\n\n602\n\n\n345\n\n\n57.3\n\n\n98.1\n\n\n828\n\n\n752\n\n\n7211\n\n\n9.418724\n\n\n8.554203\n\n\n82.02707\n\n\n3996\n\n\n3015\n\n\n1252\n\n\n407\n\n\n121\n\n\n45.45558\n\n\n34.29644\n\n\n14.241838\n\n\n4.629735\n\n\n1.376408\n\n\n6.8\n\n\n5.3\n\n\n8.8\n\n\n29.0\n\n\n117.0\n\n\n80.1\n\n\n75.7\n\n\n79.5\n\n\n1496\n\n\n1444\n\n\n419\n\n\n63\n\n\n19\n\n\n2549\n\n\n43.5\n\n\n42.0\n\n\n12.2\n\n\n1.8\n\n\n0.6\n\n\n0.7407730\n\n\n0\n\n\n2\n\n\n18\n\n\n20\n\n\n1\n\n\n1\n\n\n18\n\n\n20\n\n\n0\n\n\n2\n\n\n25\n\n\n27\n\n\n\n\n\n\n\n\nWe start with modelling our relative rate of burglary shift. We divide our sample into a training set which we’ll use to train the model, and a test set we’ll use to verify accuracy and validity.\n\nset.seed(123)\n\nsample = sample.split(data$RPDburglaryShifted, SplitRatio = 0.7)\ntrain = subset(data, sample == TRUE)\ntest  = subset(data, sample == FALSE)\n\n\nrf_burglary <- randomForest(\n  RPDburglaryShifted ~ .,\n  data=train, \n  importance=TRUE\n)\n\nsummary(rf_burglary)\n\n                Length Class  Mode     \ncall              4    -none- call     \ntype              1    -none- character\npredicted       685    -none- numeric  \nmse             500    -none- numeric  \nrsq             500    -none- numeric  \noob.times       685    -none- numeric  \nimportance      420    -none- numeric  \nimportanceSD    210    -none- numeric  \nlocalImportance   0    -none- NULL     \nproximity         0    -none- NULL     \nntree             1    -none- numeric  \nmtry              1    -none- numeric  \nforest           11    -none- list     \ncoefs             0    -none- NULL     \ny               685    -none- numeric  \ntest              0    -none- NULL     \ninbag             0    -none- NULL     \nterms             3    terms  call     \n\n\n\n#calculate our predictions and a  rmse\nprediction <-predict(rf_burglary, test)\nMetrics::rmse(test$RPDburglaryShifted, prediction)\n\n[1] 0.2054765\n\n\nWhile machine learning models were once considered opaque and difficult to interpret, several libraries now offer functionality to explain predictions. Here I use DALEX to do just this.\n\nrf_explainer_burglary <- DALEX::explain(rf_burglary, data=train, y= train$RPDburglaryShifted)\n\nPreparation of a new explainer is initiated\n  -> model label       :  randomForest  (  default  )\n  -> data              :  685  rows  211  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  685  values \n  -> predict function  :  yhat.randomForest  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package randomForest , ver. 4.7.1.1 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  -0.5274977 , mean =  -0.07802522 , max =  0.6606534  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -0.2851655 , mean =  -0.001568916 , max =  0.6340126  \n  A new explainer has been created!  \n\nrf_perf_burg <- model_performance(rf_explainer_burglary)\nrf_perf_burg\n\nMeasures for:  regression\nmse        : 0.006624875 \nrmse       : 0.08139334 \nr2         : 0.8398785 \nmad        : 0.04636179\n\nResiduals:\n          0%          10%          20%          30%          40%          50% \n-0.285165546 -0.093485342 -0.059400738 -0.040833227 -0.024804195 -0.007423066 \n         60%          70%          80%          90%         100% \n 0.009744147  0.032325289  0.054260825  0.089461048  0.634012592 \n\n\nWe can see that our model significantly outperforms our best linear models: the R2 suggests that almost 85% of the variance is correctly interpreted, and our rmse (root mean squared error) is around 0.2 on our test set.\nThis model would be ill-suited to prediction or operationalising -it is a default forecast with no hyper-parameter tuning, and no consideration of error rates - but using tools like DALEX, we can identify which predictors the model identifies as most important.\n\nmodel_parts_burg <-model_parts(rf_explainer_burglary)\n\n\nplot(model_parts_burg, max_vars=25)\n\n\n\n\nThe three most important features our model highlights are: - the age composition of the population - the number of residents which are in commercial property - the historic number of burglaries\nThis seems to corroborate our previous regression model. To further unpick these trends, we can use Partial Dependence Plots to identify how the model prediction shifts with these values.\n\npdp_b <- model_profile(rf_explainer_burglary)\n\n\nplot(pdp_b, variables=\"total_burglaries\")\n\n\n\n\nWe still see a strong association between a high number of historic, and a large reduction during the lockdown period\n\nplot(pdp_b, variables= \"Religion..2011..Christian\")\n\n\n\n\n\nhist(data$Religion..2011..Christian)\n\n\n\n\n\nplot(pdp_b, variables=\"ComEstRes\")\n\n\n\n\n\nhist(msoa_matrix_numeric$ComEstRes)\n\n\n\n\nBy combining the distribution of commercial residents by MSOA with our PDP, we can see that those MSOAs that are most densely populated by commercial residents see the smallest “covid decrease”(suggesting that those MSOAs that are very heavily residential saw the sharpest drops).\n\nplot(pdp_b, variables=\"House.Prices.Sales.2008\")\n\n\n\n\nFinally, we can see that those areas that experienced the highest volume of house sales experienced the lowest relative decrease in burglary.\nThis highlights the importance of identifying correlates in RF models - especially in a dataset where features are highly interlinked, association does not imply causation, and we should be wary of over-interpreting.\nWe can now repeat our process for our robbery shift.\n\nset.seed(123)\n\n\nsample = sample.split(data$RPDRobberyShifted, SplitRatio = 0.75)\ntrain = subset(data, sample == TRUE)\ntest  = subset(data, sample == FALSE)\n\n\nrf_robbery <- randomForest(\n  RPDRobberyShifted ~ .,\n  data=train, \n  importance=TRUE\n)\n\nsummary(rf_robbery)\n\n                Length Class  Mode     \ncall              4    -none- call     \ntype              1    -none- character\npredicted       734    -none- numeric  \nmse             500    -none- numeric  \nrsq             500    -none- numeric  \noob.times       734    -none- numeric  \nimportance      420    -none- numeric  \nimportanceSD    210    -none- numeric  \nlocalImportance   0    -none- NULL     \nproximity         0    -none- NULL     \nntree             1    -none- numeric  \nmtry              1    -none- numeric  \nforest           11    -none- list     \ncoefs             0    -none- NULL     \ny               734    -none- numeric  \ntest              0    -none- NULL     \ninbag             0    -none- NULL     \nterms             3    terms  call     \n\n\nWe’ve now trained a model. Let’s now use the DALEX library to understand it, and see how it performs.\n\nrf_explainer_robbery <- DALEX::explain(rf_robbery, data=train, y= train$RPDRobberyShifted)\n\nPreparation of a new explainer is initiated\n  -> model label       :  randomForest  (  default  )\n  -> data              :  734  rows  211  cols \n  -> data              :  tibble converted into a data.frame \n  -> target variable   :  734  values \n  -> predict function  :  yhat.randomForest  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package randomForest , ver. 4.7.1.1 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  -0.9908618 , mean =  -0.04880848 , max =  0.3886733  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -0.4640489 , mean =  -0.0007951117 , max =  0.2909289  \n  A new explainer has been created!  \n\nrf_perf_rob <- model_performance(rf_explainer_robbery)\nrf_perf_rob\n\nMeasures for:  regression\nmse        : 0.003630323 \nrmse       : 0.06025216 \nr2         : 0.872715 \nmad        : 0.02818358\n\nResiduals:\n          0%          10%          20%          30%          40%          50% \n-0.464048863 -0.059133188 -0.037257312 -0.023376075 -0.014244755 -0.004866538 \n         60%          70%          80%          90%         100% \n 0.005089387  0.019736886  0.034999616  0.059437002  0.290928866 \n\n\n\nmodel_parts_rob <-model_parts(rf_explainer_robbery)\n\n\nplot(model_parts_rob, max_vars=25)\n\n\n\n\n\npdp_rob <- model_profile(rf_explainer_robbery)\n\n\nplot(pdp_rob, variables=\"total_robberies\")\n\n\n\n\n\nplot(pdp_rob, variables=\"total_burglaries\")\n\n\n\n\nThe effect of historic crime effects again appears like a reliable predictor of a robbery covid shift: those MSOAs with the highest number of burglaries and robberies see strong decreases in robbery (though the association with burglary is not clear cut, suggesting other interaction effects may be driving this)\n\nplot(pdp_rob, variables=\"Road.Casualties.2011.2011.Total\")\n\n\n\n\nRoad casualties is another strong relationship. This could be a proxy for deprivation, but I suggest this is more down to geographic features - road casualties are probably rarer in denser urban environments, most likely to be affected by lockdown.\n\nplot(pdp_rob, variables=\"Ethnic.Group..2011.Census..Other.ethnic.group....\")\n\n\n\n\n\nhist(data$Ethnic.Group..2011.Census..Other.ethnic.group....)\n\n\n\n\nFinally, we see a demographic predictor linked to “other ethnic group”. I’m not clear how to interpret this, but it suggests that those MSOAs that are most diverse (and have the largest representation by these ethnic groups) experienced the strongest decreases in robbery during lockdown.\nTogether, these analyses suggest that the crime drop for robbery and burglary during national lockdown was significantly affected by distinct local factors. For both offences, historical crime trends play a role, with high crime areas experiencing a relatively larger drop in both burglary and robbery.\nBeyond that, the drivers vary for each offence type: burglary was driven by the composition of the residential population, with heavily residential areas, and areas with a relatively “stable” population as measured by low housing sales also saw larger decrease - this is likely due to the increased number of empty residential properties.\nRobbery decreases conversely, are well associated with high numbers of road casualties, as well as the ethnic makeup of the local population - this is likely due to an association with denser, more urban areas, with lower street speeds, and a possible link to deprivation, whereby minority population were least able to work from home, and were likely to still present as available targets for robbery."
  },
  {
    "objectID": "posts/migrated_to_quarto/index.html",
    "href": "posts/migrated_to_quarto/index.html",
    "title": "I’ve migrated to Quarto!",
    "section": "",
    "text": "Well, isn’t this all fancy and posh! After much back and forth over a decent blogging platform, I think I’ve finally found a decent, well maintained home … meet Quarto!\nSo, what did I want from a blogging platform, and why did it take so long to get here?\n\nEasily maintained, static pages (no paying for fancy maintained services1)\nNative(ish) support for Python Jupyter notebooks and RMarkdown\nSupport for both technical writing (functions, code, citations) and interactive elements\nMake it pretty!\n\nSo, does it work? Is it pretty? It’s gorgeous.\n\n\n\nGorgeous pictures! With Alt Text!\n\n\nI’ve tried a variety of options over the past few years (notably Pelican, Nikola and Hugo) but none of them have quite fit the bill\n\n\nI eventually settled on Nikola because of how consistenly it handled Jupyter, but found RMarkdown support a pain, and it wasn’t quite as well maintained and documented as I would have liked.\nYou’ve probably also noted some of the niftier other features of Quarto sprinkled through this, especially for technical writing… callouts, citations, bibliographies ahoy!\n\n\n\n\n\n\nCheck out a Callout!\n\n\n\n\n\nI’ve been using Obsidian to manage my personal notes for a few months, and being able to make notes gorgeous and functional has been a real pleasure…now I can share that with everyone else!\n\n\n\nWith native support for interactive Jupyter widgets, I’m looking forward to seeing just how many plots and maps I can show off… It’s a really promising start! I haven’t got anything too much to show for now, so instead check out this nifty, automatically generated plot.\n\n\n\n\nflowchart LR\n  A[Interactive plot?] --> B(Excited audience...)\n  B --> C[Excitement!]\n  B --> D[Confusion!]\n\n\n\n\n\nHopefully this is the first post of many on my new and improved blog, and I hope you enjoy it!\n\n\n\n\nFootnotes\n\n\nThere are plenty of fancy companies that probably advertise on your favourite podcast or Youtube video, but I wanted to keep it simple.↩︎"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "",
    "text": "Amongst stuffing my face with wine and cheese, I’ve used this Christmas break to learn more about geospatial modelling in Python.\nThis blog post is largely intended for my reference and to act as a useful example for others…as such, it may be messy! I’ll try and tidy it into a Medium post in the coming weeks.\nSpace is an often disregarded dimension of modelling within policing research. As per Tobler’s first law of geography, “everything is related to everything else, but near things are more related than distant things”, and this is probably especially true of crime, that tenders to cluster in both time and space…treating your models as not having distinct physical locations that influence how they behave is likely to miss crucial information.\nNearly all of the below is adapted from a fantastic work in progress book, Geographic Data Science with PySAL and the PyData Stack - I’ve found it hugely helpful, and the code examples are very approachable. I’d also recommend browsing the Pysal documentation.\nAll of the below is based on public data: - Police recorded crime and searches from January 2019 through October 2020 (data.police.uk) - London MSOA geographic and demographic data (MOPAC)\nThe key libraries used are: - Standard Python libraries as included in Anaconda (statsmodels, pandas, sklearn, seaborn) - Geopandas - allowing you to read, write, and plot spatial data - Pysal - a collection of modules for geospatial data science"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-data",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-data",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Spatial Data",
    "text": "Spatial Data\nWe begin by importing our spatial border data. Spatial coordinates are just coordinates, so without understanding what those coordinates mean (for instance, where you are in a city, or in the world, at what altitude, etc), they’re essentially points on a chart.\nFor us, this is provided by the Mayor’s Office for Policing and Crime, and also conveniently contains some area characteristics. We use Geopandas to read the file.\nGeospatial modelling relies on assigning events to a unit of space. You could theoretically make this as detailed as possible - for instance, meter squares - but given we’re going to analyse how our units are interconnected, that’s probably not computationally feasible (if everything is connected to everything, you’re going to need a really big computer). You’ll need to reach a suitable compromise. Helpfully, the UK government provides various statistical units, including border coordinates, for download. Lower Super Output Areas (LSOAs) and Middle Super Output Areas (MSOAs) contain populations of between 5,000 and 7,200, and as such should be (partly) comparable.\nGeospatial data will use a specific Coordinate Reference System, or CRS which will affect how your data is processed. Make sure you’re using the right one - worldwide data that assumes it is on a sphere will behave very differently to data from a specific country on a flat plane.\nUK policing data often uses National Grid Easting/Northing coordinates, rather than the more common Lat/Longs. Daata.Police.uk comes pre-processed into lat/long coordinates. Thankfully, whichever you have, geopandas can easily convert (or “re-project”) to another system.\n\nmsoas = geopandas.read_file(\"statistical-gis-boundaries-london/MapInfo/MSOA_2011_London_gen_MHW.tab\")\nmsoas = msoas.to_crs(epsg=4326)\n\nC:\\Users\\Admin\\Anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:422: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n  for feature in features_lst:\n\n\nOur data on both stop and search and robberies was manually downloaded from data.police.uk, and extracted into our working folder. Given this is then returned into a folder per month, I have written a series of helper functions to read the files, assign them to an MSOA, and combine them into a total per MSOA.\nTo assign to an MSOA, we use a “spatial join”: like a normal table join (think vlookup in Excel), this connects elements from one table, to elements from another, via a common column. In our case, the common column is the geographic location: which MSOA is our search/crime located in.\nCrime data from data.police.uk is separated into “major” crime types, which is very helpful for anonymisation (we can’t figure out who the victim was if we don’t know specific crime types), but does mean that all violent and sexual crime is agglomerated into one - given I think it’s unlikely searches deter sexual offences, that’s unhelpful. As such, we’ll focus on robbery, which is disaggregated. This isn’t ideal, but robbery remains a serious, violent, high priority crime, and as such you’d hope they are connected, and robbery might in fact be a rough proxy for overall violence.\n\ndef read_and_add(file_path):\n    df = pd.read_csv(file_path)\n    robberies = df[df[\"Crime type\"] == 'Robbery']\n    \n    #we create a geopandas object that uses the lat/long coordinates, using the appropriate CRS code.\n    robberies = geopandas.GeoDataFrame(robberies, geometry=geopandas.points_from_xy(robberies.Longitude, robberies.Latitude), crs=\"epsg:4326\")\n    detected_mask = robberies[\"Last outcome category\"].isin(positive_outcomes)\n    \n    # identify which crimes resulted in a detected outcome \n    robberies.loc[detected_mask, \"Detected\"] = 1\n    detected_mask = robberies[\"Last outcome category\"].isin(positive_outcomes)\n\n    detected_df = robberies[detected_mask]\n    robberies.loc[detected_mask, \"Detected\"] = 1\n\n    dfsjoin_r = geopandas.sjoin(msoas, robberies) #Spatial join Points to polygons\n    dfsjoin_d = geopandas.sjoin(msoas, detected_df) #Spatial join Points to polygons\n\n    #aggregate to a pivot table that will count our outputs.\n    pivot_r = pd.pivot_table(dfsjoin_r,index='MSOA11CD',fill_value=0,columns='Crime type',aggfunc={'Crime type':len})\n    pivot_d = pd.pivot_table(dfsjoin_d,index='MSOA11CD',fill_value=0,columns='Crime type',aggfunc={'Crime type':len})\n    all_pivot = pivot_r.join(pivot_d, rsuffix=\"detected\").reset_index()\n    return all_pivot.fillna(0)\n\n\ndef read_and_add_search(file_path):\n    df = pd.read_csv(file_path)\n    df = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.Longitude, df.Latitude), crs=\"epsg:4326\")\n    dfsjoin = geopandas.sjoin(msoas, df) #Spatial join Points to polygons\n    pivot_d = dfsjoin.groupby([\"MSOA11CD\"])['Object of search'].count()\n\n    #pivot_d = pd.pivot_table(dfsjoin,index='MSOA11CD',fill_value=0,columns='Object of search', aggfunc=len)\n    \n    return pd.DataFrame(pivot_d.fillna(0)).reset_index()\n\n\ndef merge_dfs(list_of_df):\n    initial_df = list_of_df[0]\n    remaining = list_of_df[1:]\n    for pivot in remaining:\n        initial_df.groupby(\"MSOA11CD\", as_index=False).sum()\n        initial_df = pivot.merge(initial_df.groupby(\"MSOA11CD\", as_index=False).sum(), how=\"outer\")\n    #initial_df.columns=[\"MSOA11CD\",\"Robbery\",\"Detected\"]\n    print (\"dataframe length is \" + str(initial_df.shape[0]))\n    return initial_df\n\n\n#we iterate over all files and subfolders in our data directory, and use the appropriate helper functions based on their last characters.\nlist_of_pivot = []\nlist_of_searches = []\nrootdir = os.getcwd()\n\n\nfor subdir, dirs, files in os.walk(\"data\"):\n    for file in files:\n        filepath = subdir + os.sep + file\n\n        if filepath.endswith(\"street.csv\"):\n            list_of_pivot.append(read_and_add(filepath))\n            \n        if filepath.endswith(\"search.csv\"):\n            list_of_searches.append(read_and_add_search(filepath))\n\n#now we concatenate all our individual dataframes into a single one\nprint(\"aggregating robbery\")\nfinal_pivot = merge_dfs(list_of_pivot)\n\nprint(\"aggregating searches\")\nfinal_search = merge_dfs(list_of_searches)\n\naggregating robbery\n\n\nC:\\Users\\Admin\\Anaconda3\\envs\\geospatial\\lib\\site-packages\\pandas\\core\\generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n\n\ndataframe length is 1705\naggregating searches\ndataframe length is 1953\n\n\n\n#finally, merge those into a single pivot with the sum of all incidents during our data, by MSOA.\nfinal_pivot.columns=[\"MSOA11CD\",\"Robbery\",\"Detected\"]\nfinal_pivot = final_pivot.groupby(\"MSOA11CD\").sum()\n\nfinal_search = final_search.groupby(\"MSOA11CD\").sum()\n\nfinal_join = final_search.join(final_pivot).reset_index().copy()\nfinal_join.columns=[\"MSOA11CD\", \"search_count\", \"robbery_count\",\"detected_robbery\"]\n\n\nfinal_join\n\n\n\n\n\n  \n    \n      \n      MSOA11CD\n      search_count\n      robbery_count\n      detected_robbery\n    \n  \n  \n    \n      0\n      E02000001\n      268\n      37\n      0.0\n    \n    \n      1\n      E02000002\n      173\n      38\n      2.0\n    \n    \n      2\n      E02000003\n      294\n      115\n      7.0\n    \n    \n      3\n      E02000004\n      216\n      19\n      0.0\n    \n    \n      4\n      E02000005\n      132\n      61\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      978\n      E02006927\n      581\n      68\n      7.0\n    \n    \n      979\n      E02006928\n      638\n      89\n      5.0\n    \n    \n      980\n      E02006929\n      1373\n      146\n      7.0\n    \n    \n      981\n      E02006930\n      168\n      56\n      2.0\n    \n    \n      982\n      E02006931\n      393\n      105\n      5.0\n    \n  \n\n983 rows × 4 columns\n\n\n\n\nfinal_join.sum()\n\nMSOA11CD            E02000001E02000002E02000003E02000004E02000005E...\nsearch_count                                                   477668\nrobbery_count                                                   60093\ndetected_robbery                                                 3054\ndtype: object\n\n\nOur final file then, contains nearly 500,000 searches and just over 60,000 robberies (of which a suspect was found in around 3,000) across London’s 938 MSOAs.\n\nmsoas[\"MSOA11CD\"] = msoas[\"MSOA11CD\"].astype(\"string\")\nfinal_join[\"MSOA11CD\"] = final_join[\"MSOA11CD\"].astype(\"string\")\n\n\nmsoas = msoas.merge(final_join, on=[\"MSOA11CD\"])\n\nWe then re-combine this with our geometry file, before calculating a proportion of robbery detected figure, and a search per robbery rate - the final table is below.\n\nmsoas[\"robbery_solve\"] = msoas[\"detected_robbery\"] / msoas[\"robbery_count\"]\nmsoas[\"search_rate\"] = msoas[\"search_count\"] / msoas[\"robbery_count\"]\n\n\nmsoas.index = msoas[\"MSOA11NM\"]\n\nThe data is now processed. Now is probably a good time to write this to a file to retrieve it in the future.\n\nmsoas.to_file(\"msoas.shp\")\n\nmsoas\n\n\n\n\n\n  \n    \n      \n      MSOA11CD\n      MSOA11NM\n      LAD11CD\n      LAD11NM\n      RGN11CD\n      RGN11NM\n      UsualRes\n      HholdRes\n      ComEstRes\n      PopDen\n      Hholds\n      AvHholdSz\n      geometry\n      search_count\n      robbery_count\n      detected_robbery\n      robbery_solve\n      search_rate\n    \n    \n      MSOA11NM\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      City of London 001\n      E02000001\n      City of London 001\n      E09000001\n      City of London\n      E12000007\n      London\n      7375\n      7187\n      188\n      25.5\n      4385\n      1.6\n      MULTIPOLYGON (((-0.09676 51.52325, -0.09644 51...\n      268\n      37\n      0.0\n      0.000000\n      7.243243\n    \n    \n      Barking and Dagenham 001\n      E02000002\n      Barking and Dagenham 001\n      E09000002\n      Barking and Dagenham\n      E12000007\n      London\n      6775\n      6724\n      51\n      31.3\n      2713\n      2.5\n      POLYGON ((0.14811 51.59678, 0.14809 51.59640, ...\n      173\n      38\n      2.0\n      0.052632\n      4.552632\n    \n    \n      Barking and Dagenham 002\n      E02000003\n      Barking and Dagenham 002\n      E09000002\n      Barking and Dagenham\n      E12000007\n      London\n      10045\n      10033\n      12\n      46.9\n      3834\n      2.6\n      POLYGON ((0.15065 51.58306, 0.14841 51.58075, ...\n      294\n      115\n      7.0\n      0.060870\n      2.556522\n    \n    \n      Barking and Dagenham 003\n      E02000004\n      Barking and Dagenham 003\n      E09000002\n      Barking and Dagenham\n      E12000007\n      London\n      6182\n      5937\n      245\n      24.8\n      2318\n      2.6\n      POLYGON ((0.18511 51.56480, 0.18403 51.56391, ...\n      216\n      19\n      0.0\n      0.000000\n      11.368421\n    \n    \n      Barking and Dagenham 004\n      E02000005\n      Barking and Dagenham 004\n      E09000002\n      Barking and Dagenham\n      E12000007\n      London\n      8562\n      8562\n      0\n      72.1\n      3183\n      2.7\n      POLYGON ((0.14990 51.56807, 0.15078 51.56778, ...\n      132\n      61\n      1.0\n      0.016393\n      2.163934\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Greenwich 034\n      E02006927\n      Greenwich 034\n      E09000011\n      Greenwich\n      E12000007\n      London\n      8315\n      8241\n      74\n      33.0\n      3338\n      2.5\n      POLYGON ((0.02900 51.46779, 0.02995 51.46592, ...\n      581\n      68\n      7.0\n      0.102941\n      8.544118\n    \n    \n      Greenwich 035\n      E02006928\n      Greenwich 035\n      E09000011\n      Greenwich\n      E12000007\n      London\n      7341\n      6410\n      931\n      136.0\n      2977\n      2.2\n      MULTIPOLYGON (((-0.00961 51.48366, -0.00979 51...\n      638\n      89\n      5.0\n      0.056180\n      7.168539\n    \n    \n      Greenwich 036\n      E02006929\n      Greenwich 036\n      E09000011\n      Greenwich\n      E12000007\n      London\n      7490\n      7489\n      1\n      29.4\n      3333\n      2.2\n      POLYGON ((0.01619 51.49578, 0.01854 51.49498, ...\n      1373\n      146\n      7.0\n      0.047945\n      9.404110\n    \n    \n      Greenwich 037\n      E02006930\n      Greenwich 037\n      E09000011\n      Greenwich\n      E12000007\n      London\n      6561\n      6557\n      4\n      75.6\n      2876\n      2.3\n      POLYGON ((0.00866 51.48917, 0.00837 51.48877, ...\n      168\n      56\n      2.0\n      0.035714\n      3.000000\n    \n    \n      Greenwich 038\n      E02006931\n      Greenwich 038\n      E09000011\n      Greenwich\n      E12000007\n      London\n      9186\n      8973\n      213\n      46.1\n      4113\n      2.2\n      POLYGON ((-0.00201 51.48155, -0.00136 51.48145...\n      393\n      105\n      5.0\n      0.047619\n      3.742857\n    \n  \n\n983 rows × 18 columns"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#analysis",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#analysis",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Analysis",
    "text": "Analysis\nLet’s start exploring our data. Before doing anything, we’ll use Pysal to create “spatial weights” - this let’s us understand our objects in space, and how they interact. There are a myriad of different methods, but we’ll just pick the 8 nearest MSOAs in space.\n\n# Generate W from the GeoDataFrame\nw = weights.distance.KNN.from_dataframe(msoas, k=8)\nw.neighbors['City of London 001']\n\n['Islington 023',\n 'Southwark 002',\n 'Islington 022',\n 'Hackney 027',\n 'Hackney 026',\n 'Southwark 006',\n 'Southwark 003',\n 'Southwark 009']\n\n\n\n# Row-standardization\nw.transform = 'R'\n\nWe also create a spatial “lag” for our key values - this is one of the most straightforward measures of spatial relationships, and captures the products and weights of our value in neighbouring observations. Essentially, it’s the value of our metric, weighted by the value of the metric in neighbourhing observations - so clusters will expect to be high, while a single high value surrounded by low values will be diminished.\n\nmsoas['w_search_count'] = weights.spatial_lag.lag_spatial(w, msoas['search_count'])\nmsoas['w_robbery_count'] = weights.spatial_lag.lag_spatial(w, msoas['robbery_count'])\nmsoas['w_rate'] = weights.spatial_lag.lag_spatial(w, msoas['search_rate'])\n\n\nLet’s start by mapping out our key crime and search figures, and seeing if anything stands out.\n\nf, axs = plt.subplots(nrows=2, ncols=2, figsize=(20, 12))\nmsoas.plot(column='search_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[0,0]\n       )\n\naxs[0,0].title.set_text('Search Count')\n\nmsoas.plot(column='robbery_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[0,1]\n       )\n\naxs[0,1].title.set_text('Robbery Count')\n\nmsoas.plot(column='robbery_solve', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[1,0]\n       )\n\naxs[1,0].title.set_text('Detected Robbery Proportion')\n\nmsoas.plot(column='search_rate', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[1,1]\n       )\n\n#contextily pulls a background tile from online providers\naxs[1,1].title.set_text('Search per Robbery')\nfor ax in axs.reshape(-1):\n        contextily.add_basemap(ax, \n                           crs=msoas.crs, \n                           source=contextily.providers.Stamen.TerrainBackground)\n\n#ax1.set_axis_off()\n#ax2.set_axis_off()\n\n\n\n\nThere are a few apparently spatial trends at play: - Searches and robberies seem most common in central London, though searches also seem to cluster on the Western area of the city - The proportion of robberies that are detected seems to be scaterred pretty randomly - The rate of search per robbery seems to form a “U” shape around the soutehrn side of the city, with the Northern edge standing out as potentially “under-policed”\nLet’s ignore the spatial dimension for a second, and just create a correlation heatmap. This should let us identify any commonalities between our measured characteristics\n\ncorr_cols = ['UsualRes', 'HholdRes', 'ComEstRes', 'PopDen', 'Hholds', 'AvHholdSz',\n             'search_count', 'robbery_count',\n       'robbery_solve', 'search_rate',\n             'w_search_count', 'w_robbery_count','w_rate']\n\nsns.set_theme(style=\"white\")\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n\nplt.figure(figsize=(16, 6))\n# define the mask to set the values in the upper triangle to True\nmask = np.triu(np.ones_like(msoas[corr_cols].corr(), dtype=np.bool))\nheatmap = sns.heatmap(msoas[corr_cols].corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap=cmap)\n\n\n\n\nThe good news is that search and robbery counts are closely correlated: those MSOAs where most robberies occur also see the most searches.\nThere is also a weak positive correlation (0.3) between commercial households and robberies - this is probably due to some combination of high footfall and affuluence.\nThere is also a negative relationship between the spatial lag of robberies, and the average household size - I’m not quite sure how to interpret this. Potentially those areas with lots of focused large households are large nexuses of robbery?\nLet’s go a bit further by building a scatter plot between robbery and search counts.\n\nsns.scatterplot(x=\"search_count\", y=\"robbery_count\", data=msoas)\n\n<AxesSubplot:xlabel='search_count', ylabel='robbery_count'>\n\n\n\n\n\nIt’s a bit of a mess. Most of our robberies are focused on the lower angle, with a few huge outliers. This might benefit from a log transformaton - keeping the key characteristics the same, but normalising our distribution and making the relationship linear.\n\nsns.scatterplot(x=np.log(msoas[\"search_count\"]), y=np.log(msoas[\"robbery_count\"]), data=msoas)\n\n<AxesSubplot:xlabel='search_count', ylabel='robbery_count'>\n\n\n\n\n\nAs planned, a lot better - this suggests a linear relationship between both variables, and they should now be normally distributed, allowing us to model them.\n\nsns.displot(x=np.log(msoas[\"search_rate\"]), kde=True)\n\n<seaborn.axisgrid.FacetGrid at 0x1c46af71e08>"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-structures",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-structures",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Spatial Structures",
    "text": "Spatial Structures\nTo identify whether geography plays a role in how our values change, we’ll contrast the values to their spatial lag - the local value, impacted by their neighbours. This should smooth out any outliers, and let us get a general value per area, rather than by MSOA.\n\nf, axs = plt.subplots(nrows=3, ncols=2, figsize=(20, 12))\nmsoas.plot(column='search_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[0,0]\n       )\n\naxs[0,0].title.set_text('Search Count')\n\nmsoas.plot(column='w_search_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[0,1]\n       )\n\naxs[0,1].title.set_text('Spatial Lag - Search Count')\n\nmsoas.plot(column='robbery_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[1,0]\n       )\n\naxs[1,0].title.set_text('Robbery Count')\n\nmsoas.plot(column='w_robbery_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[1,1]\n       )\n\naxs[1,1].title.set_text('Spatial Lag - Robbery Count')\n\nmsoas.plot(column='search_rate', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[2,0]\n       )\n\naxs[2,0].title.set_text('Search per Robbery')\n\nmsoas.plot(column='w_rate', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[2,1]\n       )\n\naxs[2,1].title.set_text('Spatial Lag - Search Rate')\nfor ax in axs.reshape(-1):\n        contextily.add_basemap(ax, \n                           crs=msoas.crs, \n                           source=contextily.providers.Stamen.TerrainBackground)\n\n#ax1.set_axis_off()\n#ax2.set_axis_off()\n\n\n\n\nOur spatial weights enable us to clearly identy trends in space: - the search count and robbery count is now far more focused on centralon London, with the exception of a few pockets - the north of London cluster stands out more distinctly as a location where the rate of search per robbery is distinctively lower"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#clustering",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#clustering",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Clustering",
    "text": "Clustering\nAs an alternative method for exploring our data, we’ll use some machine learning techniques to undertake some clustering. We’ll be using K Means clustering (which focuses on the mean of all our values, and picks the most similar other MSOAs) - given we’re now focusing on modelling, we’ll use the log values.\n\nmsoas['ln_search_count'] = np.log(msoas['search_count'])\nmsoas['ln_robbery_count'] = np.log(msoas['robbery_count'])\nmsoas['ln_rate'] = np.log(msoas['search_rate'])\n\nmsoas['w_search_count'] = weights.spatial_lag.lag_spatial(w, msoas['ln_search_count'])\nmsoas['w_robbery_count'] = weights.spatial_lag.lag_spatial(w, msoas['w_robbery_count'])\nmsoas['w_rate'] = weights.spatial_lag.lag_spatial(w, msoas['w_rate'])\n\nw = weights.distance.KNN.from_dataframe(msoas, k=8)\n\n# Row-standardization\nw.transform = 'R'\n\n\nWe’ll also include the demographic information provided by MOPAC (household numbers and populations), and use the KMeans Clustering implementation from sklearn. A number of clusters must be specified - I’ve gone for 5 on this occasion. We’ll then map them.\nEssentially, we’re trying to aggregate MSOAs into 5 distinct groups, based on our crime and demographic characteristics.\n\ncluster_variables = ['UsualRes', 'HholdRes', 'ComEstRes', 'PopDen', 'Hholds', 'AvHholdSz', \n       'robbery_solve', 'w_search_count', 'w_robbery_count',\n       'w_rate', 'ln_search_count', 'ln_robbery_count', 'ln_rate']\n\n# Initialise KMeans instance\nkmeans = KMeans(n_clusters=5)\n\n# Set the seed for reproducibility\nnumpy.random.seed(1234)\n# Run K-Means algorithm\nk5cls = kmeans.fit(msoas[cluster_variables])\n\n# Assign labels into a column\nmsoas['k5cls'] = k5cls.labels_\n# Setup figure and ax\nf, ax = plt.subplots(1, figsize=(14, 9))\n# Plot unique values choropleth including a legend and with no boundary lines\nmsoas.plot(column='k5cls', categorical=True, legend=True, linewidth=0, ax=ax)\n# Remove axis\nax.set_axis_off()\n# Keep axes proportionate\nplt.axis('equal')\n# Add title\nplt.title(r'Geodemographic Clusters (k-means, $k=5$)')\n\ncontextily.add_basemap(ax, crs=msoas.crs, \n                           source=contextily.providers.Stamen.TerrainBackground)\n# Display the map\nplt.show()\n\n\n\n\nInterestingly, there are no clear patterns here: cluster membership is spread out throughout London. That said, we still don’t know what these actually mean - to understand that, we’ll visualise the distributions of our characteristics in each cluster.\n\n# Index db on cluster ID\ntidy_db = msoas.set_index('k5cls')\n# Keep only variables used for clustering\ntidy_db = tidy_db[cluster_variables]\n# Stack column names into a column, obtaining \n# a \"long\" version of the dataset\ntidy_db = tidy_db.stack()\n# Take indices into proper columns\ntidy_db = tidy_db.reset_index()\n# Rename column names\ntidy_db = tidy_db.rename(columns={\n                        'level_1': 'Attribute', \n                        0: 'Values'})\n\n# Setup the facets\nfacets = seaborn.FacetGrid(data=tidy_db, col='Attribute', hue='k5cls', \\\n                  sharey=False, sharex=False, aspect=2, col_wrap=3)\n# Build the plot from `sns.kdeplot`\n_ = facets.map(seaborn.kdeplot, 'Values', shade=True).add_legend()\n\n\n\n\nThe defining metric seems to be density, but some other trends stand out: the densest group (3) also sees the largest proportion of robberies and searches, while conversely group 2 is the very opposite."
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-autocorrelation",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-autocorrelation",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\nWe’re now quite happy there are some spatial relationships in our data: where you are in London matters. It appears that an MSOA in the South and North of London might expect differnet numbers of searches and robberies, even were all there other characteristics the same. We can test this by checking for spatial auto-correlation in our data, the primary metric of which is Moran’s I.\n\nmoran = esda.moran.Moran(msoas['search_count'], w)\n\nmoran.I\n\n0.28837097453822225\n\n\nOur Moran’s I shows weak, positive autocorrelation accross our entire dataset: an MSOA with a high search count will generally be next to similar MSOAs.\n\nmoran.p_sim\n\n0.001\n\n\nThe P statistic suggests this Moran’s I is statistically different to what we would expect if it were random (at P<0.05) - we reject the hypothesis that there is no spatial dimension.\n\nplot_moran(moran);"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#regression-modelling",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#regression-modelling",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Regression Modelling",
    "text": "Regression Modelling\nFinally, we’re explore regression models. We’ll build a series of models, each attempting to predict the number of searches in an MSOA, based on the number of robberies and the population density. We’ll start by simple OLS models, and then attempt variations that account for spatial weights.\n\nfrom pysal.model import spreg\nfrom pysal.lib import weights\nfrom pysal.explore import esda\nfrom scipy import stats\nimport statsmodels.formula.api as sm\nimport numpy\nimport pandas\nimport geopandas\nimport matplotlib.pyplot as plt\nimport seaborn\nimport patsy\n\n\nOLS\nFirst, we run a simple log OLS model, using the Pysal spreg implementation (you could also get identical results using StatsModels or SKLearn). This doesn’t use any spatial modelling at all. I’m using Patsy to transform our Dataframe appropriately.\n\ny, X =  patsy.dmatrices(\"np.log(search_count) ~ np.log(robbery_count) + np.log(PopDen)\", msoas, return_type=\"matrix\")\n\n\ny = np.array(y)\nX = np.array(X)\n\n\nm1 = spreg.OLS(y, X,\n                name_y='Log Search Count', name_x=[\"Constant\",\"Log Robbery Count\",\"Log Pop Density\"])\n\n\nprint(m1.summary)\n\nREGRESSION\n----------\nSUMMARY OF OUTPUT: ORDINARY LEAST SQUARES\n-----------------------------------------\nData set            :     unknown\nWeights matrix      :        None\nDependent Variable  :Log Search Count                Number of Observations:         983\nMean dependent var  :      5.7488                Number of Variables   :           3\nS.D. dependent var  :      0.9163                Degrees of Freedom    :         980\nR-squared           :      0.4181\nAdjusted R-squared  :      0.4169\nSum squared residual:     479.795                F-statistic           :    352.0632\nSigma-square        :       0.490                Prob(F-statistic)     :  5.987e-116\nS.E. of regression  :       0.700                Log likelihood        :   -1042.289\nSigma-square ML     :       0.488                Akaike info criterion :    2090.577\nS.E of regression ML:      0.6986                Schwarz criterion     :    2105.249\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     t-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT       3.1161071       0.1416924      21.9920574       0.0000000\n   Log Robbery Count       0.5845225       0.0260850      22.4083679       0.0000000\n     Log Pop Density       0.1141935       0.0357637       3.1929958       0.0014529\n------------------------------------------------------------------------------------\nWarning: Variable(s) ['Constant'] removed for being constant.\n\nREGRESSION DIAGNOSTICS\nMULTICOLLINEARITY CONDITION NUMBER           15.197\n\nTEST ON NORMALITY OF ERRORS\nTEST                             DF        VALUE           PROB\nJarque-Bera                       2           4.305           0.1162\n\nDIAGNOSTICS FOR HETEROSKEDASTICITY\nRANDOM COEFFICIENTS\nTEST                             DF        VALUE           PROB\nBreusch-Pagan test                2           4.046           0.1322\nKoenker-Bassett test              2           3.543           0.1700\n================================ END OF REPORT =====================================\n\n\nThe model is actually pretty good - the R2 suggests we account for over 40% of the variance, and each of our variables are significant. This is what we’d expect given the correlation between robbery and search counts.\n\n\nExogeneous Spatial Effects Model\nNow, let’s use a model including the spatial lag of our exogenous variables. We use the same OLS implementation, and just add the weights as a variable.\n\ny, X =  patsy.dmatrices(\"np.log(search_count) ~ np.log(robbery_count) + np.log(PopDen) + np.log(w_robbery_count)\", msoas, return_type=\"matrix\")\n\n\ny = np.array(y)\nX = np.array(X)\n\n\nm2 = spreg.OLS(y, X,\n                name_y='Log Search Count', name_x=[\"Constant\",\"Log Robbery Count\",\"Log Pop Density\",\"Robbery Spatial Weights\"])\n\n\nprint(m2.summary)\n\nREGRESSION\n----------\nSUMMARY OF OUTPUT: ORDINARY LEAST SQUARES\n-----------------------------------------\nData set            :     unknown\nWeights matrix      :        None\nDependent Variable  :Log Search Count                Number of Observations:         983\nMean dependent var  :      5.7488                Number of Variables   :           4\nS.D. dependent var  :      0.9163                Degrees of Freedom    :         979\nR-squared           :      0.4188\nAdjusted R-squared  :      0.4170\nSum squared residual:     479.214                F-statistic           :    235.1495\nSigma-square        :       0.489                Prob(F-statistic)     :   7.02e-115\nS.E. of regression  :       0.700                Log likelihood        :   -1041.693\nSigma-square ML     :       0.488                Akaike info criterion :    2091.386\nS.E of regression ML:      0.6982                Schwarz criterion     :    2110.948\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     t-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT       2.9955315       0.1797701      16.6631230       0.0000000\n   Log Robbery Count       0.5587104       0.0352339      15.8571653       0.0000000\n     Log Pop Density       0.0987403       0.0384697       2.5666998       0.0104148\nRobbery Spatial Weights       0.0711112       0.0652601       1.0896585       0.2761318\n------------------------------------------------------------------------------------\nWarning: Variable(s) ['Constant'] removed for being constant.\n\nREGRESSION DIAGNOSTICS\nMULTICOLLINEARITY CONDITION NUMBER           27.105\n\nTEST ON NORMALITY OF ERRORS\nTEST                             DF        VALUE           PROB\nJarque-Bera                       2           4.293           0.1169\n\nDIAGNOSTICS FOR HETEROSKEDASTICITY\nRANDOM COEFFICIENTS\nTEST                             DF        VALUE           PROB\nBreusch-Pagan test                3           5.101           0.1645\nKoenker-Bassett test              3           4.506           0.2118\n================================ END OF REPORT =====================================\n\n\nInterestingly, adding a spatial lag does not improve our model at all - the lag is not statistically significant.\n\n\nSpatial Error Models\nInstead of adding a spatial lag, we add a spatial error - effectively assuming we’re slightly off, in a consistent way, because of our spatial characteristics, and accounting for it.\n\ny, X =  patsy.dmatrices(\"np.log(search_count) ~ 0 + np.log(robbery_count) + np.log(PopDen)\", msoas, return_type=\"matrix\")\ny = np.array(y)\nX = np.array(X)\n\nm6 = spreg.GM_Error_Het(y, X, \n                           w=w, name_y='Log Search Count', name_x=[\"Log Robbery Count\",\"Log Pop Density\"])\nprint(m6.summary)\n\nREGRESSION\n----------\nSUMMARY OF OUTPUT: SPATIALLY WEIGHTED LEAST SQUARES (HET)\n---------------------------------------------------------\nData set            :     unknown\nWeights matrix      :     unknown\nDependent Variable  :Log Search Count                Number of Observations:         983\nMean dependent var  :      5.7488                Number of Variables   :           3\nS.D. dependent var  :      0.9163                Degrees of Freedom    :         980\nPseudo R-squared    :      0.4134\nN. of iterations    :           1                Step1c computed       :          No\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     z-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT       3.5341582       0.2141206      16.5054581       0.0000000\n   Log Robbery Count       0.5823628       0.0365549      15.9311915       0.0000000\n     Log Pop Density       0.0124371       0.0397069       0.3132221       0.7541119\n              lambda       0.6810127       0.0298280      22.8313537       0.0000000\n------------------------------------------------------------------------------------\n================================ END OF REPORT =====================================\n\n\nInterestingly, while our model seems to be just as accurate, population density is no longer significant - this suggests density may be acting as a proxy for our spatial characteristics.\n\n\nSpatial Lag Model\nFinally, we’ll test a model that includes the spatial lag of our dependent variable - the search count varies based on nearby search counts. That breaks the assumptions of traditional OLS regression - we’re essentially forcing the search count to be on either side of the equation, meaning both will be correlated. Pysal has a specific implementation using two stage least squares to tackle these issues.\n\ny, X =  patsy.dmatrices(\"np.log(search_count) ~ np.log(robbery_count) + np.log(PopDen)\", msoas, return_type=\"matrix\")\ny = np.array(y)\nX = np.array(X)\n\nm7 = spreg.GM_Lag(y, X, w=w, name_y='Log of Search Count', name_x=[\"Constant\",\"Log Robbery Count\",\"Log Pop Density\"])\nprint(m7.summary)\n\nREGRESSION\n----------\nSUMMARY OF OUTPUT: SPATIAL TWO STAGE LEAST SQUARES\n--------------------------------------------------\nData set            :     unknown\nWeights matrix      :     unknown\nDependent Variable  :Log of Search Count                Number of Observations:         983\nMean dependent var  :      5.7488                Number of Variables   :           4\nS.D. dependent var  :      0.9163                Degrees of Freedom    :         979\nPseudo R-squared    :      0.4584\nSpatial Pseudo R-squared:  0.4175\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     z-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT       2.7219978       0.3420296       7.9583682       0.0000000\n   Log Robbery Count       0.5614888       0.0311151      18.0455524       0.0000000\n     Log Pop Density       0.0818749       0.0430088       1.9036779       0.0569522\nW_Log of Search Count       0.1066105       0.0848248       1.2568327       0.2088142\n------------------------------------------------------------------------------------\nInstrumented: W_Log of Search Count\nInstruments: W_Log Pop Density, W_Log Robbery Count\nWarning: Variable(s) ['Constant'] removed for being constant.\n================================ END OF REPORT =====================================\n\n\nThis is a our best model yet. Note that density is once again no longer significant, but our model has improved somewhat - this seems the best fit yet.\n\ny, X =  patsy.dmatrices(\"np.log(search_count) ~ 0 + np.log(robbery_count)\", msoas, return_type=\"matrix\")\ny = np.array(y)\nX = np.array(X)\n\nm7 = spreg.GM_Lag(y, X, w=w, name_y='Log of Search Count', name_x=[\"Log Robbery Count\"])\nprint(m7.summary)\n\nREGRESSION\n----------\nSUMMARY OF OUTPUT: SPATIAL TWO STAGE LEAST SQUARES\n--------------------------------------------------\nData set            :     unknown\nWeights matrix      :     unknown\nDependent Variable  :Log of Search Count                Number of Observations:         983\nMean dependent var  :      5.7488                Number of Variables   :           3\nS.D. dependent var  :      0.9163                Degrees of Freedom    :         980\nPseudo R-squared    :      0.4544\nSpatial Pseudo R-squared:  0.4130\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     z-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT       2.9856130       0.3464531       8.6176549       0.0000000\n   Log Robbery Count       0.5879825       0.0320598      18.3401657       0.0000000\nW_Log of Search Count       0.1040083       0.0724840       1.4349150       0.1513113\n------------------------------------------------------------------------------------\nInstrumented: W_Log of Search Count\nInstruments: W_Log Robbery Count\n================================ END OF REPORT =====================================\n\n\nBased on the pseudo-R2, this is our best model, though I’m not sure whether it’s directly comparable, and the spatial term is not significant.\nThat said, we’ve now created a few simple models, using various techniques. We could do an awful lot more to improve these: I suspect accounting for more local characteristics might help (such as presence of tube stations), as well as distance from central London.\nI’ve quite enjoyed learning this, and it’s been far more approachable than I expected - I’ll keep expanding on this and clean it up in the future!"
  },
  {
    "objectID": "posts/burglary_attendance/index.html",
    "href": "posts/burglary_attendance/index.html",
    "title": "What’s Happened to Burglary, and does Attending Help?",
    "section": "",
    "text": "It’s nearly 2023, and in the true spirit of Christmas, I’ve used the cheese/wine filled nowhere time between the holidays to pick up an analytical side-project that’s been irking me for awhile… what’s happened to burglary, and why are we suddenly so excited about mandatory attendance? It’s a thorny question, so I figured I’d document my analysis here… All(ish) of the code is available, so this will hopefully be a useful tutorial for others. As usual, keep in mind this is a blog post, not an academic article - this analysis is fuelled by post-Christmas cheese, and peer reviewed by me after a glass of cherry, so if you want to rely on any of it, replicate it yourself first and make sure it’s right!\nThis took somewhat longer than I expected (hooray for Christmas breaks), so this will be a series in three parts:\nSo come with me on an analytical adventure through time, as we cast our minds back to the halcyon days of 2015. The pound is worth $1.5 dollars again, Boris Johnson is Mayor of London, and we’re all still fondly thinking back to how great the London Olympics were.\nAcross policing, the effects of austerity are starting to be acutely felt - having briefly been protected, chief officers are tightening their fiscal belt: swathes of policing real estate are sold off to protect the front-line, which in some places is starting to rely on volunteers and goodwill to avoid looking rather thin.\nMeanwhile, crime is changing: traditional crimes like burglary and violence continue their to fall, leading the soon-to-be Prime Minister Theresa May to tell the Police Federation to “stop crying wolf” about cuts while demand shrinks, but policing certainly isn’t feeling it, as an increase in “high-harm”, complex offences more than made up for any shortfall.\nIn the face of these conflicting pressures, policing did something that seemed perfectly reasonable, announcing that officers may not attend every single burglary.\nLess than a decade later, we’re now on a very different course. Confidence in policing, which had previously seemed immutable, is down. Investigative performance has seemingly falling off a cliff. And so the NPCC has made the opposite commitment, pledging that every home burglary will now see police attendance, after forces who tested the approach claimed huge “dramatic” crime reductions.\nBut has performance really declined that quickly, and is attendance to blame? I’ll use this post to explore open police data from the last decade, and use data from pilot forces to see just how dramatic the effect is: just what’s happened to burglary, and will mandatory attendance help?"
  },
  {
    "objectID": "posts/burglary_attendance/index.html#whats-happened-to-burglary",
    "href": "posts/burglary_attendance/index.html#whats-happened-to-burglary",
    "title": "What’s Happened to Burglary, and does Attending Help?",
    "section": "What’s Happened to Burglary?",
    "text": "What’s Happened to Burglary?\nWe’ll start by exploring how burglary has changed over time - for the purpose of this analysis, we’re focusing on domestic burglary (eg, non including commercial properties), and we’ll use both police recorded crime data (eg, crimes reported to police) and the Crime Survey of England and Wales (a national survey to measure total crime) to compare trends.\n\n\n\n\n\n\nOn Data Quality\n\n\n\n\n\nWhile the UK benefits from crime data that is comparatively clean (I do not envy our American cousins, let alone most of our colleagues in the rest of the world), this is less true for both historical data (eg, going back more than a few years) or the Crime Survey data, which isn’t readily available to the general public: the data is spread through various archive files, websites and reports, and there really isn’t a “single source of truth” for crime counts over time.\nThe data for this analysis is a few hacked togther files: - Crime outcomes and crime counts by quarter, manually concatenated from the Police Open Data Tables - Crime Survey burglary counts per year, from the most recent ONS crime and justice quarterly\nI’m afraid that does mean this analysis isn’t quite as reproducible as I’d like - I had written some code to scrape the Home Office data page and aggregate all the outcomes data, but it’s such a mishmash of ODS,XLSX and various formats that everything fell over and I did it manually. That said, hopefully it’s not too much of a pain to reproduce if you feel the need to.\nA few caveats are worth considering:\n\nThe Crime Survey can’t always be easily compared to reported crime: police crime data is legal accounting, while the crime survey is asking the general public what happened. Domestic Burglary should be comparable, but this is all a little experimental.\nReproducibility and data cleaning: good analysis should be reproducible - you should be able to run it the same way, with new data. This isn’t true here because of my ugly cleaning.\nOutcomes aren’t instant: Crimes take time to solve, and so the most recent outcomes data will probably change. I’ve used outcomes per quarter, which should work, but the most recent data is likely to change.\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport patsy\nimport statsmodels.api as sm\nimport plotly.io as pio\nfrom linearmodels import PanelOLS\n\npio.templates.default = \"plotly_dark\"\n\n\npolice_crime_df = pd.read_csv(\"../data/external/police_recorded_crime_combined.csv\")\n\n\nyearly_police_df = police_crime_df[['Financial Year','Number of Offences']].groupby('Financial Year').sum().reset_index()\nyearly_police_df['year'] = range(2002,2023,1)\nyearly_police_df = yearly_police_df.rename(columns={\"Number of Offences\":'police'})\n\ncsew_count = pd.read_csv(\"../data/external/csew_burglary_trends.csv\").rename(columns={'Domestic burglary':'csew'})\n\n\n\ncsew_count['year'] = range(2002,2021,1)\ncsew_count['csew'] = csew_count['csew']*1000\n\ncomparison_df = csew_count.merge(yearly_police_df,how='left',on='year').rename(columns={'csew':'Crime Survey',\n                                                                                        'police':'Police Reported'})\n\ncomparison_df[['year','Crime Survey','Police Reported']].set_index('year')\n\n\n\n\n\n\n\n\n  \n    \n      \n      Crime Survey\n      Police Reported\n    \n    \n      year\n      \n      \n    \n  \n  \n    \n      2002\n      1423000\n      437583\n    \n    \n      2003\n      1357000\n      402345\n    \n    \n      2004\n      1310000\n      321507\n    \n    \n      2005\n      1059000\n      300517\n    \n    \n      2006\n      1025000\n      292260\n    \n    \n      2007\n      1006000\n      280696\n    \n    \n      2008\n      960000\n      284431\n    \n    \n      2009\n      992000\n      268606\n    \n    \n      2010\n      917000\n      258165\n    \n    \n      2011\n      1033000\n      245312\n    \n    \n      2012\n      922000\n      227276\n    \n    \n      2013\n      887000\n      211988\n    \n    \n      2014\n      781000\n      196554\n    \n    \n      2015\n      784000\n      194700\n    \n    \n      2016\n      697000\n      206051\n    \n    \n      2017\n      650000\n      309867\n    \n    \n      2018\n      691000\n      295556\n    \n    \n      2019\n      699000\n      268715\n    \n    \n      2020\n      582000\n      196214\n    \n  \n\n\n\n\nThe Crime Survey data is the best measure for long term crime trends - as it isn’t affected by reporting or policing practice. Taken in isolation, the most obvious takeaway here is that burglary is lower than it’s every been. If you acccept the Peelian view that “the test of police efficiency is the absence of crime and disorder”, policing is doing an excellent job. So why is police reported crime comparatively high, and why are perceptions of police performance seemingly so low?\n\n\nCode\ntidy_yearly = comparison_df[['year','Crime Survey','Police Reported']].melt(id_vars='year')\n\nfig = px.line(tidy_yearly, x='year', y='value', color='variable',color_discrete_sequence=['red','blue'])\n\nfig.update_layout(\n    title_text=\"Domestic Burglaries, Police Reported and Crime Survey\",\n        legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n        y=-0.2),\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Year\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Burglaries\")\n\nfig.show()\n\n\n\n                                                \n\n\nComparing Crime Survey to Police Recorded crime counts is generally considered bad form: while a police officer might always know the difference between a robbery, a burglary, and an affray to enable a theft, that’s not something most of the public worries about.\nThankfully, that shouldn’t be a problem for domestic burglary - most people know when they’ve been burgled! That means we can produce a ratio of crime survey burglaries to police burglaries, and obtain a rough estimate of what proportion of burglaries are reported to police over time.\n\n\nCode\ncomparison_df['Proportion of Burglaries Reported'] = comparison_df['Police Reported'] / comparison_df['Crime Survey'] * 100\n\ntidy_yearly = comparison_df[['year','Crime Survey','Police Reported','Proportion of Burglaries Reported']].melt(id_vars='year')\n\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\ncsew_burglary = tidy_yearly[tidy_yearly['variable'] == 'Crime Survey']\npolice_burglary = tidy_yearly[tidy_yearly['variable'] == 'Police Reported']\nreported_ratio = tidy_yearly[tidy_yearly['variable'] == 'Proportion of Burglaries Reported']\n\n\n\nfig.add_trace(\n    go.Scatter(x=csew_burglary['year'], y=csew_burglary['value'], name=\"Crime Survey\", line=dict(color='red')),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=police_burglary['year'], y=police_burglary['value'], name=\"Police Reported\", line=dict(color='blue')),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=reported_ratio['year'], y=reported_ratio['value'], name=\"% Reported\", line=dict(color='grey')),\n    secondary_y=True,\n)\n\n# Add figure title\nfig.update_layout(\n    title_text=\"Crime Survey Burglaries per Year and Proportion Reported\",\n    legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n        y=-0.2),\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Year\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Burglaries\", secondary_y=False, color='black')\nfig.update_yaxes(title_text=\"Proportion of Burglaries Reported\", secondary_y=True, showgrid=False, color='grey')\n\n\nfig.show()\n\n\n\n                                                \n\n\nI’d once thought burglary reporting would be consistent - it’s an emotive crime, with a well established insurance industry - and yet we definitely see variation over time.\nNotice that dramatic spike in 2015? That’s the aftermath of the 2014 police crime recording scandal, which led to police crime statistics losing their designation as a national statistic, and accurate crime recording becoming a key metric for the policing regulator. To dig into this issue in more depth, I’d really recommend this blog by Gavin Hales.\nWhat does this tell us about burglary in England & Wales over the last decade though? Two things:\n\nDomestic Burglaries are probably rarer than ever (and that’s not down to COVID)\nThat’s not consistently reflected in police recorded crime - the likelihood of reporting seems to change over time"
  },
  {
    "objectID": "posts/burglary_attendance/index.html#how-many-burglars-are-we-catching",
    "href": "posts/burglary_attendance/index.html#how-many-burglars-are-we-catching",
    "title": "What’s Happened to Burglary, and does Attending Help?",
    "section": "How Many Burglars are we Catching?",
    "text": "How Many Burglars are we Catching?\nSo there aren’t many burglaries…so have we gotten better at catching the rest?\nWell…we’re certainly not catching any more than we used to. Whether you use charges (eg, burglars being sent to court) or “detections” (eg, including cautions, fines and similar), there are far fewer, thought it looks like that trend started long before 2015.\n\n\nCode\ndetections = pd.read_csv('../data/interim/burglary_detections.csv',index_col=0)\ndetections\n\nfig = px.line(detections, x='year', y='detections')\n\nfig.update_xaxes(title_text=\"Year\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Police Detections\", range=[0,60000])\n\n\nfig.update_layout(\n    title_text=\"Police Detections by Year\"\n)\n\nfig.show()\n\n\n\n                                                \n\n\nThat’s not hugely surprising: there are fewer burglars around to be caught. What’s perhaps more important is how many burglars we’re catching as a proportion of those remaining burglaries we know about.\nI’ve visualised that below: on the left axis, all police reported burglaries (in blue), and how many lead to a positive outcome (in red), and on the right axis, the ratio of the two: an estimate of the proportion of police burglaries that are “solved”.\n\n\nCode\nyearly_df = detections.merge(comparison_df, how='left', on='year')\nyearly_df['detected_csew_ratio'] = yearly_df['detections'] / yearly_df['Crime Survey'] * 100\nyearly_df['detected_police_ratio'] = yearly_df['detections'] / yearly_df['Police Reported'] * 100\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['Police Reported'], name=\"Police Reported\", fillcolor='blue'),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['detections'], name=\"Police Detections\",  fillcolor='red'),\n    secondary_y=False,\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['detected_police_ratio'], name=\"Poportion of Police Reported Burglaries Detected\", fillcolor='green', visible=True),\n    secondary_y=True,\n)\n\n\n# Add figure title\nfig.update_layout(\n    title_text=\"Crime Survey Burglaries per Year and Proportion Reported\",\n    legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n        y=-0.2),\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Year\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Burglaries\", secondary_y=False)\nfig.update_yaxes(title_text=\"Proportion of Burglaries Solved\",range=[0,20], secondary_y=True, showgrid=False, color='green')\n\n\nfig.show()\n\n\n\n                                                \n\n\nIt doesn’t look good: performance drops sharply around the time we stopped attending all burglaries in 2015.\nOf course, plenty of other things happened in 2015: for instance, we know there was a radical shift in recording standards, where police recorded and invesigated thousands of burglaries which they would have previously not been aware of. So how do we measure how police performance changed irrespective of those recording changes? We can replicate that chart, but instead of using police recorded burglaries, we use all burglaries, as reported by the Crime Survey. rather than just police reported burglaries.\nThe below chart does exactly that - click on each of the buttons to see how the baseline figure affects perceived investigatory performance.\n\n\nCode\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['Police Reported'], name=\"Police Reported\", fillcolor='blue'),\n    secondary_y=False,\n)\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['detections'], name=\"Police Detections\", fillcolor='red'),\n    secondary_y=False,\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['detected_police_ratio'], name=\"Poportion of Police Reported Burglaries Detected\", fillcolor='green', visible=False),\n    secondary_y=True,\n)\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['detected_csew_ratio'], name=\"Poportion of Crime Survey Burglaries Detected\", fillcolor='black', visible=False),\n    secondary_y=True,\n)\n\n# Add figure title\nfig.update_layout(\n    title_text=\"Crime Survey Burglaries per Year and Proportion Reported\"\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Year\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Burglaries\", secondary_y=False)\nfig.update_yaxes(title_text=\"Proportion of Burglaries Solved\",range=[0,20], secondary_y=True, showgrid=False, color='green')\n\n\nfig.update_layout(\n    legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n        y=-0.2),\n    updatemenus=[\n        dict(\n            type=\"buttons\",\n            direction=\"right\",\n            active=0,\n            x=1,\n            y=1.2,\n            bgcolor='grey',\n            bordercolor='white',\n            font=dict(color='white'),\n            showactive=False,\n            buttons=list([\n                dict(label=\"As Proportion of Police Reported\",\n                     method=\"update\",\n                     args=[{\"visible\": [True, True, True,False]}]),\n                dict(label=\"As Proportion of all Burglaries\",\n                     method=\"update\",\n                     args=[{\"visible\": [True, True, False,True]}])\n            ]),\n        )\n    ])\n\nfig.show()\n\n\n\n                                                \n\n\nSuddenly that sharp drop in 2015 is a lot less convincing. Looking at all burglaries - rather than just those reported to police - the proportion being solved has been going down, but that started happening long before 2015. Around 5% of all burglaries in the crime survey were “solved” in 2010, but by 2015, that had already dropped to around 2.5%, compared to just over 2% in 2020.\nWhat’s the lesson here? The perception of a recent, short term drop in police investigative performance are somewhat overstated. Yes, policing is catching fewer burglars than ever, but burglaries are rarer than ever - those that remain are less likely to be caught than they used to be, but that’s a trend that started long before 2015 (it might even have started as early as 2008).\nWhat explains that trend? We really don’t know. It’s possible that those few remaining burglars are the most persistent and sophisiticated: maybe opportunistic drug users desperate for a fix who were willing to smash your front window in a decade ago have been replaced by professional teams who will do a whole set of flats in minutes. We can’t really tell with the data we’ve got here, but it’s probably not as simple as policing just not trying hard enough."
  },
  {
    "objectID": "posts/burglary_attendance/index.html#will-mandatory-attendance-help",
    "href": "posts/burglary_attendance/index.html#will-mandatory-attendance-help",
    "title": "What’s Happened to Burglary, and does Attending Help?",
    "section": "Will Mandatory Attendance Help?",
    "text": "Will Mandatory Attendance Help?\nGiven the above, will mandatory attendance actually reduce burglary? As far as I can tell, nobody has properly checked. The Telegraph suggests that burglaries “could fall by 60pc if officers visited every victim”, but given nobody has really rigorously tested the impact of burglary attendance, I’m not totally convinced. Leicestershire attempted to conduct a randomised control trial in 2015, but following press uproar the whole thing was shelved.\nThere is some good research that suggests it will make some difference: rapid attendance to crimes probably does help solve them, and finding a suspect is the most effective way of solving a burglary. If we randomly assigned some burglaries to be attended, and some to not, those we did attend would be solved more often.\nThat’s different to mandatory attendance though: where even if the responding officer thinks there isn’t any point (for example, if the burglary was many months ago, or very unlikely to be solved following a phone report), they’ll attend anyway. So how have some reporters calculcated that policy could cut the number of offences by half?\nUnhelpfully, the researchers for the Mail and the Telegraph haven’t been very open with sharing their methodology, but it looks like they’ve looked at those forces who already have a mandatory attendance policy, and tried to predict what would happen if that had been expanded nationally.\nThree forces in particular are repeatedly mentioned as having introduced mandatory attendance prior to the NPCC mandate:\n\nNorthamptonshire since April 2019, though it’s worth noting these are dedicated burglary teams rather than attendance in isolation\nBedfordshire Police using the codename “Operation Maze”, though again, it seems paired with dedicated burglary teams, and may in fact be attendance by forensics staff rather than police offiers. The earliest records are in early 2020 but it’s not clear at all this involves any mandatory attendance, rather than just increased resourcing.\nGreater Manchester Police since July 2021\n\nThe key question then is did the introduction of mandatory attendance in these three forces make any difference to burglary investigations?\nTo do that, we’ll calculate: - What was the detection rate for burglary in all forces? - How did the detection rate change after the introduction of mandatory attendance? - How did other, similar forces fare during the same period?\nThere are real limitations to doing this with public data: we are limited to quarter by quarter analysis, and we don’t actually know exactly when these policies started (or, for that matter, exactly what they include) or who else might have been doing something similar, but we can still try and identify what performance boost (if any) mandatory attendance made for these forces.\n\n\nCode\ntheft_df = pd.read_excel(\"../data/external/burglary_outcomes_combined.ods\", engine=\"odf\")\nqs = theft_df['Financial year'].str.slice(0,4) + \"-Q\" + theft_df['Financial quarter'].astype('str')\n\ntheft_df['date'] = pd.PeriodIndex(qs, freq='Q').to_timestamp()\ntheft_df = theft_df.sort_values(by='date',ascending=False)\n\nburglary_df = theft_df[theft_df['Offence Subgroup'] == 'Domestic burglary'].rename(columns={'Force Name':'Force'})\nburglary_df['Force'] = burglary_df['Force'].astype('string')\nburglary_df\n\n\n\n\n\n\n  \n    \n      \n      Financial year\n      Financial quarter\n      Force\n      Offence Description\n      Offence Group\n      Offence Subgroup\n      Offence Code\n      Offence code expired\n      Outcome Description\n      Outcome Group\n      Outcome Type\n      Force outcomes for offences recorded in quarter\n      Force outcomes recorded in quarter\n      date\n    \n  \n  \n    \n      160811\n      2021/22\n      4\n      British Transport Police\n      Attempted Burglary Residential\n      Theft offences\n      Domestic burglary\n      28F\n      NaN\n      Community Resolution\n      Out-of-court (informal)\n      8\n      0.0\n      0.0\n      2021-10-01\n    \n    \n      164083\n      2021/22\n      4\n      Hampshire\n      Attempted Distraction Burglary Residential\n      Theft offences\n      Domestic burglary\n      28H\n      NaN\n      Taken into consideration\n      Taken into consideration\n      4\n      0.0\n      0.0\n      2021-10-01\n    \n    \n      164081\n      2021/22\n      4\n      Hampshire\n      Attempted Distraction Burglary Residential\n      Theft offences\n      Domestic burglary\n      28H\n      NaN\n      Diversionary, educational or intervention acti...\n      Diversionary, educational or intervention acti...\n      22\n      0.0\n      0.0\n      2021-10-01\n    \n    \n      164080\n      2021/22\n      4\n      Hampshire\n      Attempted Distraction Burglary Residential\n      Theft offences\n      Domestic burglary\n      28H\n      NaN\n      Further investigation to support formal action...\n      Further investigation to support formal action...\n      21\n      0.0\n      0.0\n      2021-10-01\n    \n    \n      164079\n      2021/22\n      4\n      Hampshire\n      Attempted Distraction Burglary Residential\n      Theft offences\n      Domestic burglary\n      28H\n      NaN\n      Responsibility for further investigation trans...\n      Responsibility for further investigation trans...\n      20\n      0.0\n      0.0\n      2021-10-01\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      49689\n      2017/18\n      1\n      Northamptonshire\n      Attempted Burglary Residential\n      Theft offences\n      Domestic burglary\n      28F\n      NaN\n      Further investigation to support formal action...\n      Further investigation to support formal action...\n      21\n      1.0\n      0.0\n      2017-01-01\n    \n    \n      49688\n      2017/18\n      1\n      Northamptonshire\n      Attempted Burglary Residential\n      Theft offences\n      Domestic burglary\n      28F\n      NaN\n      Responsibility for further investigation trans...\n      Responsibility for further investigation trans...\n      20\n      1.0\n      1.0\n      2017-01-01\n    \n    \n      49687\n      2017/18\n      1\n      Northamptonshire\n      Attempted Burglary Residential\n      Theft offences\n      Domestic burglary\n      28F\n      NaN\n      Caution – youths\n      Out-of-court (formal)\n      2\n      0.0\n      0.0\n      2017-01-01\n    \n    \n      49686\n      2017/18\n      1\n      Northamptonshire\n      Attempted Burglary Residential\n      Theft offences\n      Domestic burglary\n      28F\n      NaN\n      Investigation complete – no suspect identified\n      Investigation complete – no suspect identified\n      18\n      114.0\n      104.0\n      2017-01-01\n    \n    \n      50576\n      2017/18\n      1\n      South Wales\n      Distraction Burglary Residential\n      Theft offences\n      Domestic burglary\n      28G\n      NaN\n      Community Resolution\n      Out-of-court (informal)\n      8\n      0.0\n      0.0\n      2017-01-01\n    \n  \n\n212960 rows × 14 columns\n\n\n\nGiven we’re looking at quarterly data, we’ll generate two temporal variables for our data:\n\nA running variable (running_var), which measures the number of quarters that have passed since the start of our dataset\nA quarter “categorical variable”, checking to see if there is any specific trend for burglary detection whether you’re in quarter 1, 2, 3 or 4\n\n\n\nCode\n\ntime_series = theft_df[['date','Financial quarter']].drop_duplicates().sort_values(by=['date']).reset_index(drop=True).reset_index().rename(columns={'index':'running_var'})\ntime_series['quarter'] = time_series['Financial quarter'].astype('string')\ntime_series = time_series.drop(columns=['Financial quarter'])\ntime_series\n\n\n\n\n\n\n  \n    \n      \n      running_var\n      date\n      quarter\n    \n  \n  \n    \n      0\n      0\n      2017-01-01\n      1\n    \n    \n      1\n      1\n      2017-04-01\n      2\n    \n    \n      2\n      2\n      2017-07-01\n      3\n    \n    \n      3\n      3\n      2017-10-01\n      4\n    \n    \n      4\n      4\n      2018-01-01\n      1\n    \n    \n      5\n      5\n      2018-04-01\n      2\n    \n    \n      6\n      6\n      2018-07-01\n      3\n    \n    \n      7\n      7\n      2018-10-01\n      4\n    \n    \n      8\n      8\n      2019-01-01\n      1\n    \n    \n      9\n      9\n      2019-04-01\n      2\n    \n    \n      10\n      10\n      2019-07-01\n      3\n    \n    \n      11\n      11\n      2019-10-01\n      4\n    \n    \n      12\n      12\n      2020-01-01\n      1\n    \n    \n      13\n      13\n      2020-04-01\n      2\n    \n    \n      14\n      14\n      2020-07-01\n      3\n    \n    \n      15\n      15\n      2020-10-01\n      4\n    \n    \n      16\n      16\n      2021-01-01\n      1\n    \n    \n      17\n      17\n      2021-04-01\n      2\n    \n    \n      18\n      18\n      2021-07-01\n      3\n    \n    \n      19\n      19\n      2021-10-01\n      4\n    \n  \n\n\n\n\nFor each force, we then have a count of burglaries, and a ratio of how many solved burglaries (eg, burglary investigations where a burglary is identified and dealt with by police) we have that quarter.\n\n\nCode\ntotal_q_offences = burglary_df[['date','Force','Force outcomes for offences recorded in quarter']].groupby(['date','Force']).sum().reset_index()\ntotal_q_offences['total_offences'] = pd.to_numeric(total_q_offences['Force outcomes for offences recorded in quarter'],errors='coerce')\n\npositive_outcomes = ['Taken into consideration',\n 'Charged/Summonsed',\n 'Community Resolution',\n 'Cannabis/Khat Warning',\n 'Penalty Notices for Disorder',\n 'Caution – adults',\n 'Diversionary, educational or intervention activity, resulting from the crime report, has been undertaken and it is not in the public interest to take any further action.',\n 'Caution – youths']\n\nburglary_df['is_detected'] = burglary_df['Outcome Description'].isin(positive_outcomes)\n\npositive_burglary_df = burglary_df[burglary_df['is_detected']]\n\ntotal_detected_offences = positive_burglary_df[['date','Force','Force outcomes for offences recorded in quarter']].groupby(['date','Force']).sum().reset_index()\ntotal_detected_offences['total_detected'] = pd.to_numeric(total_detected_offences['Force outcomes for offences recorded in quarter'],errors='coerce')\n\nforce_comparison = total_q_offences[['date','Force','total_offences']].merge(total_detected_offences, how='left',on=['date','Force']).drop(columns=['Force outcomes for offences recorded in quarter'])\nforce_comparison['detected_rate'] = force_comparison['total_detected'] / force_comparison['total_offences']\nforce_comparison['detected_rate']  = force_comparison['detected_rate'].fillna(0)\nforce_comparison\n\n\n\n\n\n\n  \n    \n      \n      date\n      Force\n      total_offences\n      total_detected\n      detected_rate\n    \n  \n  \n    \n      0\n      2017-01-01\n      Avon and Somerset\n      2151.0\n      112.0\n      0.052069\n    \n    \n      1\n      2017-01-01\n      Bedfordshire\n      1142.0\n      120.0\n      0.105079\n    \n    \n      2\n      2017-01-01\n      British Transport Police\n      0.0\n      0.0\n      0.000000\n    \n    \n      3\n      2017-01-01\n      Cambridgeshire\n      1109.0\n      99.0\n      0.089270\n    \n    \n      4\n      2017-01-01\n      Cheshire\n      790.0\n      80.0\n      0.101266\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      875\n      2021-10-01\n      Warwickshire\n      376.0\n      8.0\n      0.021277\n    \n    \n      876\n      2021-10-01\n      West Mercia\n      857.0\n      8.0\n      0.009335\n    \n    \n      877\n      2021-10-01\n      West Midlands\n      4061.0\n      93.0\n      0.022901\n    \n    \n      878\n      2021-10-01\n      West Yorkshire\n      2406.0\n      81.0\n      0.033666\n    \n    \n      879\n      2021-10-01\n      Wiltshire\n      265.0\n      8.0\n      0.030189\n    \n  \n\n880 rows × 5 columns\n\n\n\nNow, we identify our “treated” forces, and the period in which the policy was in place. This isn’t exactly clean, but based on the newspaper articles and press releases, I’ve picked out:\n\nBedfordshire from 2020 onwards\nGMP from July 2021 onwards\nNorthamptonshire from April 2019 onwards\nand of course, nationwide from October 2022\n\nWith those dates, we’ll then build our completed dataset of quarter by quarter burglary detection rates, for each force, and whether or not a mandatory attendance policy was in place in that force/quarter.\n\n\nCode\nnorthamptonshire = (force_comparison['Force'].str.contains('Northamptonshire')) & (force_comparison['date'] >= '2019-04-01')\nbedfordshire = (force_comparison['Force'].str.contains('Bedfordshire')) & (force_comparison['date'] >= '2020-01-01')\nmanchester = (force_comparison['Force'].str.contains('Manchester')) & (force_comparison['date'] >= '2020-07-01')\neveryone = (force_comparison['date'] >= '2022-10-01')\n\n\nforce_comparison['mandatory_attendance'] = northamptonshire | bedfordshire | manchester | everyone\nforce_comparison\n\n\n\n\n\n\n  \n    \n      \n      date\n      Force\n      total_offences\n      total_detected\n      detected_rate\n      mandatory_attendance\n    \n  \n  \n    \n      0\n      2017-01-01\n      Avon and Somerset\n      2151.0\n      112.0\n      0.052069\n      False\n    \n    \n      1\n      2017-01-01\n      Bedfordshire\n      1142.0\n      120.0\n      0.105079\n      False\n    \n    \n      2\n      2017-01-01\n      British Transport Police\n      0.0\n      0.0\n      0.000000\n      False\n    \n    \n      3\n      2017-01-01\n      Cambridgeshire\n      1109.0\n      99.0\n      0.089270\n      False\n    \n    \n      4\n      2017-01-01\n      Cheshire\n      790.0\n      80.0\n      0.101266\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      875\n      2021-10-01\n      Warwickshire\n      376.0\n      8.0\n      0.021277\n      False\n    \n    \n      876\n      2021-10-01\n      West Mercia\n      857.0\n      8.0\n      0.009335\n      False\n    \n    \n      877\n      2021-10-01\n      West Midlands\n      4061.0\n      93.0\n      0.022901\n      False\n    \n    \n      878\n      2021-10-01\n      West Yorkshire\n      2406.0\n      81.0\n      0.033666\n      False\n    \n    \n      879\n      2021-10-01\n      Wiltshire\n      265.0\n      8.0\n      0.030189\n      False\n    \n  \n\n880 rows × 6 columns\n\n\n\nOne limitation of working by quarters is that we’re quite limited in numbers! Good inference in statistical modelling relies on detecting “variance” - eg, having enough data to identify both our effect, and general background levels, and separating out the two. Given GMP has only been running for 6 quarters, we’ll focus on the other two forces for this analysis.\n\n\nCode\nforce_comparison[['Force','mandatory_attendance']].groupby('Force').sum().sort_values(by='mandatory_attendance', ascending=False).rename(columns={\"mandatory_attendance\":'Quarters of Mandatory Attendance'})\n\n\n\n\n\n\n  \n    \n      \n      Quarters of Mandatory Attendance\n    \n    \n      Force\n      \n    \n  \n  \n    \n      Northamptonshire\n      11\n    \n    \n      Bedfordshire\n      8\n    \n    \n      Greater Manchester\n      6\n    \n    \n      South Wales\n      0\n    \n    \n      Merseyside\n      0\n    \n    \n      Metropolitan Police\n      0\n    \n    \n      Norfolk\n      0\n    \n    \n      North Wales\n      0\n    \n    \n      North Yorkshire\n      0\n    \n    \n      Northumbria\n      0\n    \n    \n      Nottinghamshire\n      0\n    \n    \n      Avon and Somerset\n      0\n    \n    \n      London, City of\n      0\n    \n    \n      Staffordshire\n      0\n    \n    \n      Suffolk\n      0\n    \n    \n      Surrey\n      0\n    \n    \n      Sussex\n      0\n    \n    \n      Thames Valley\n      0\n    \n    \n      Warwickshire\n      0\n    \n    \n      West Mercia\n      0\n    \n    \n      West Midlands\n      0\n    \n    \n      West Yorkshire\n      0\n    \n    \n      South Yorkshire\n      0\n    \n    \n      Lincolnshire\n      0\n    \n    \n      Leicestershire\n      0\n    \n    \n      Lancashire\n      0\n    \n    \n      British Transport Police\n      0\n    \n    \n      Cambridgeshire\n      0\n    \n    \n      Cheshire\n      0\n    \n    \n      Cleveland\n      0\n    \n    \n      Cumbria\n      0\n    \n    \n      Derbyshire\n      0\n    \n    \n      Devon and Cornwall\n      0\n    \n    \n      Dorset\n      0\n    \n    \n      Durham\n      0\n    \n    \n      Dyfed-Powys\n      0\n    \n    \n      Essex\n      0\n    \n    \n      Gloucestershire\n      0\n    \n    \n      Gwent\n      0\n    \n    \n      Hampshire\n      0\n    \n    \n      Hertfordshire\n      0\n    \n    \n      Humberside\n      0\n    \n    \n      Kent\n      0\n    \n    \n      Wiltshire\n      0\n    \n  \n\n\n\n\nNow we can start combining our data together, and seeing if we can observe any meaningful difference after the intervention takes place.\nFor meaningful comparisons, we’ll seek to compare forces to other similar forces - there’s not much point in comparing GMP to Durham. We could do that with our existing data - eg, looking at forces with similar numbers of burglaries - but helpfully, the policing regulator HMIFRS already produces “Most Similar Forces” groups, which should let us match up forces based on size, demographics and other factors relevant to performance. We’ll seperate out our group A (Bedfordshire and their group) from group B (Northamptonshire’s), though notice Kent is in both - unhelpfully, HMIC’s groups aren’t exclusive, though that’s not too much of a problem.\n\n\nCode\ngroup_a = [\"Bedfordshire\",\n\"Leicestershire\",\n\"Nottinghamshire\",\n\"Hertfordshire\",\n\"Kent\",\n\"Hampshire\",\n\"Essex\",\n\"South Yorkshire\"]\n\ngroup_b = [\"Northamptonshire\",\n           \"Cheshire\",\n\"Derbyshire\",\n\"Staffordshire\",\n\"Kent\",\n\"Avon and Somerset\",\n\"Essex\",\n\"Nottinghamshire\"]\n\nmsf_groups = force_comparison[['Force']].drop_duplicates().reset_index(drop=True)\nmsf_groups['group_A'] = msf_groups['Force'].isin(group_a)\nmsf_groups['group_B'] = msf_groups['Force'].isin(group_b)\nmsf_groups\n\n\n\n\n\n\n  \n    \n      \n      Force\n      group_A\n      group_B\n    \n  \n  \n    \n      0\n      Avon and Somerset\n      False\n      True\n    \n    \n      1\n      Bedfordshire\n      True\n      False\n    \n    \n      2\n      British Transport Police\n      False\n      False\n    \n    \n      3\n      Cambridgeshire\n      False\n      False\n    \n    \n      4\n      Cheshire\n      False\n      True\n    \n    \n      5\n      Cleveland\n      False\n      False\n    \n    \n      6\n      Cumbria\n      False\n      False\n    \n    \n      7\n      Derbyshire\n      False\n      True\n    \n    \n      8\n      Devon and Cornwall\n      False\n      False\n    \n    \n      9\n      Dorset\n      False\n      False\n    \n    \n      10\n      Durham\n      False\n      False\n    \n    \n      11\n      Dyfed-Powys\n      False\n      False\n    \n    \n      12\n      Essex\n      True\n      True\n    \n    \n      13\n      Gloucestershire\n      False\n      False\n    \n    \n      14\n      Greater Manchester\n      False\n      False\n    \n    \n      15\n      Gwent\n      False\n      False\n    \n    \n      16\n      Hampshire\n      True\n      False\n    \n    \n      17\n      Hertfordshire\n      True\n      False\n    \n    \n      18\n      Humberside\n      False\n      False\n    \n    \n      19\n      Kent\n      True\n      True\n    \n    \n      20\n      Lancashire\n      False\n      False\n    \n    \n      21\n      Leicestershire\n      True\n      False\n    \n    \n      22\n      Lincolnshire\n      False\n      False\n    \n    \n      23\n      London, City of\n      False\n      False\n    \n    \n      24\n      Merseyside\n      False\n      False\n    \n    \n      25\n      Metropolitan Police\n      False\n      False\n    \n    \n      26\n      Norfolk\n      False\n      False\n    \n    \n      27\n      North Wales\n      False\n      False\n    \n    \n      28\n      North Yorkshire\n      False\n      False\n    \n    \n      29\n      Northamptonshire\n      False\n      True\n    \n    \n      30\n      Northumbria\n      False\n      False\n    \n    \n      31\n      Nottinghamshire\n      True\n      True\n    \n    \n      32\n      South Wales\n      False\n      False\n    \n    \n      33\n      South Yorkshire\n      True\n      False\n    \n    \n      34\n      Staffordshire\n      False\n      True\n    \n    \n      35\n      Suffolk\n      False\n      False\n    \n    \n      36\n      Surrey\n      False\n      False\n    \n    \n      37\n      Sussex\n      False\n      False\n    \n    \n      38\n      Thames Valley\n      False\n      False\n    \n    \n      39\n      Warwickshire\n      False\n      False\n    \n    \n      40\n      West Mercia\n      False\n      False\n    \n    \n      41\n      West Midlands\n      False\n      False\n    \n    \n      42\n      West Yorkshire\n      False\n      False\n    \n    \n      43\n      Wiltshire\n      False\n      False\n    \n  \n\n\n\n\nLet’s start with a simple visual observation: we look at the rate of detection per outcome for each force and group, with the start of mandatory attendance visible as the dashed vertical line.\n\n\nCode\nlinear_comparator_df = force_comparison.merge(msf_groups, how='right', on='Force')\nlinear_comparator_df = linear_comparator_df[linear_comparator_df['group_A'] | linear_comparator_df['group_B']].merge(time_series, how='left', on='date')\n\nlinear_comparator_df['detected_rate'] = linear_comparator_df['detected_rate']  * 100\n\ndf_a = linear_comparator_df[linear_comparator_df['group_A']]\ndf_b = linear_comparator_df[linear_comparator_df['group_B']]\n\nfig = make_subplots(rows=2, cols=1)\n\nfor force in df_a['Force'].unique():\n    force_df = df_a[df_a['Force'] == force]\n    if force == 'Bedfordshire':\n        fig.add_trace(\n            go.Scatter(\n            x=force_df['date'], y=force_df['detected_rate'], name=force, line=dict(color='red', width=4,\n                              dash='dash')),\n            row=1, col=1\n        )\n    else:\n        force_df = df_a[df_a['Force'] == force]\n        fig.add_trace(\n            go.Scatter(\n            x=force_df['date'], y=force_df['detected_rate'], name=force),\n            row=1, col=1\n        )\n\n\n\nfor force in df_b['Force'].unique():\n    force_df = df_b[df_b['Force'] == force]\n    if force == 'Northamptonshire':\n        fig.add_trace(\n            go.Scatter(\n            x=force_df['date'], y=force_df['detected_rate'], name=force, line=dict(color='blue', width=4,\n                              dash='dash')),\n            row=2, col=1\n        )\n    else:\n        fig.add_trace(\n            go.Scatter(\n            x=force_df['date'], y=force_df['detected_rate'], name=force),\n            row=2, col=1\n        )\n\n\nfig.add_vline(x='2020-01-01', row=1, col=1, line_dash=\"dash\")\nfig.add_vline(x='2019-04-01', row=2, col=1,line_dash=\"dash\")\n\nfig.update_xaxes(title_text=\"Date\")\nfig.update_layout(height=700, title_text=\"Detected Outcomes per Quarter and Force, and start of Mandatory Attendance\")\n\nfig.update_yaxes(title_text=\"Detected Rate\")\n\nfig.show()\n\n\n\n                                                \n\n\nIt looks, in a word, messy. There might be some sort of effect, but it certainly isn’t pronounced enough to visually observe. Instead, let’s use some statistical modelling to try and measure the average effect."
  },
  {
    "objectID": "posts/burglary_attendance/index.html#statistical-modelling",
    "href": "posts/burglary_attendance/index.html#statistical-modelling",
    "title": "What’s Happened to Burglary, and does Attending Help?",
    "section": "Statistical Modelling",
    "text": "Statistical Modelling\n\nLinear Models with Time Effects\nWith all our data, we now build a statistical model, predicting the detected outcome rate as a factor of force, their comparison group, their total offences, and the period in time.\n\n\nCode\nlinear_comparator_df['Force'] = linear_comparator_df['Force'].astype('category')\nlinear_comparator_df['quarter'] = linear_comparator_df['quarter'].astype('category')\nlinear_comparator_df['mandatory_attendance'] = linear_comparator_df['mandatory_attendance'].astype('int')\n\n\ny = linear_comparator_df['detected_rate']\n\nX = patsy.dmatrix(\"0 + C(Force) + total_offences + mandatory_attendance + group_A + group_B  + C(quarter) + running_var\", data=linear_comparator_df, return_type='dataframe')\n\n\nmodel = sm.OLS(y,X)\n\nresults = model.fit()\n\nresults.summary()\n\n\n\n\nOLS Regression Results\n\n  Dep. Variable:      detected_rate    R-squared:             0.551\n\n\n  Model:                   OLS         Adj. R-squared:        0.518\n\n\n  Method:             Least Squares    F-statistic:           16.44\n\n\n  Date:             Sat, 31 Dec 2022   Prob (F-statistic): 1.59e-32\n\n\n  Time:                 16:25:24       Log-Likelihood:      -426.89\n\n\n  No. Observations:         260        AIC:                   891.8\n\n\n  Df Residuals:             241        BIC:                   959.4\n\n\n  Df Model:                  18                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                                 coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  C(Force)[Avon and Somerset]     3.6335     0.530     6.861  0.000     2.590     4.677\n\n\n  C(Force)[Bedfordshire]          3.1250     0.322     9.720  0.000     2.492     3.758\n\n\n  C(Force)[Cheshire]              4.9698     0.291    17.099  0.000     4.397     5.542\n\n\n  C(Force)[Derbyshire]            1.3595     0.322     4.226  0.000     0.726     1.993\n\n\n  C(Force)[Essex]                -2.4339     0.274    -8.877  0.000    -2.974    -1.894\n\n\n  C(Force)[Hampshire]             2.8248     0.468     6.031  0.000     1.902     3.747\n\n\n  C(Force)[Hertfordshire]         1.4803     0.277     5.341  0.000     0.934     2.026\n\n\n  C(Force)[Kent]                 -3.5083     0.277   -12.671  0.000    -4.054    -2.963\n\n\n  C(Force)[Leicestershire]        2.8885     0.294     9.821  0.000     2.309     3.468\n\n\n  C(Force)[Northamptonshire]      0.6208     0.350     1.774  0.077    -0.068     1.310\n\n\n  C(Force)[Nottinghamshire]      -2.1424     0.419    -5.118  0.000    -2.967    -1.318\n\n\n  C(Force)[South Yorkshire]       3.5578     0.705     5.045  0.000     2.169     4.947\n\n\n  C(Force)[Staffordshire]         3.2984     0.292    11.288  0.000     2.723     3.874\n\n\n  group_A[T.True]                 5.7917     0.632     9.165  0.000     4.547     7.037\n\n\n  group_B[T.True]                 5.7974     0.455    12.730  0.000     4.900     6.694\n\n\n  C(quarter)[T.2]                -0.0319     0.231    -0.138  0.890    -0.487     0.423\n\n\n  C(quarter)[T.3]                -1.0204     0.259    -3.937  0.000    -1.531    -0.510\n\n\n  C(quarter)[T.4]                -1.0813     0.247    -4.370  0.000    -1.569    -0.594\n\n\n  total_offences                 -0.0013     0.000    -2.707  0.007    -0.002    -0.000\n\n\n  mandatory_attendance            0.7574     0.442     1.714  0.088    -0.113     1.628\n\n\n  running_var                    -0.1198     0.027    -4.481  0.000    -0.172    -0.067\n\n\n\n\n  Omnibus:        1.754   Durbin-Watson:         1.763\n\n\n  Prob(Omnibus):  0.416   Jarque-Bera (JB):      1.487\n\n\n  Skew:           0.174   Prob(JB):              0.475\n\n\n  Kurtosis:       3.128   Cond. No.           7.82e+19\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 8.11e-32. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\n\n\nOur model suggests that mandatory attendance may have had, on average, a small effect on performance, this was not significant - eg, it may have been down to sheer luck.\nLet’s go a bit further, and add a interaction effect between time and our force variable - eg, accounting for the fact that performance may vary differently over time force, by force.\n\n\nCode\ny = linear_comparator_df['detected_rate']\n\nX = patsy.dmatrix(\"C(Force)*running_var + total_offences + mandatory_attendance + group_A + group_B  + C(quarter)\", data=linear_comparator_df, return_type='dataframe')\n\n\nmodel = sm.OLS(y,X)\n\nresults = model.fit()\n\nresults.summary()\n\n\n\n\nOLS Regression Results\n\n  Dep. Variable:      detected_rate    R-squared:             0.616\n\n\n  Model:                   OLS         Adj. R-squared:        0.565\n\n\n  Method:             Least Squares    F-statistic:           12.22\n\n\n  Date:             Sat, 31 Dec 2022   Prob (F-statistic): 6.57e-33\n\n\n  Time:                 16:25:24       Log-Likelihood:      -406.76\n\n\n  No. Observations:         260        AIC:                   875.5\n\n\n  Df Residuals:             229        BIC:                   985.9\n\n\n  Df Model:                  30                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                                              coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                                    6.2968     0.805     7.827  0.000     4.712     7.882\n\n\n  C(Force)[T.Bedfordshire]                     1.6786     0.583     2.879  0.004     0.530     2.827\n\n\n  C(Force)[T.Cheshire]                         2.9897     0.959     3.118  0.002     1.101     4.879\n\n\n  C(Force)[T.Derbyshire]                      -1.9116     0.867    -2.205  0.028    -3.620    -0.204\n\n\n  C(Force)[T.Essex]                           -0.2824     0.500    -0.565  0.573    -1.268     0.703\n\n\n  C(Force)[T.Hampshire]                        2.2860     0.537     4.256  0.000     1.228     3.344\n\n\n  C(Force)[T.Hertfordshire]                   -0.2522     0.513    -0.492  0.623    -1.263     0.759\n\n\n  C(Force)[T.Kent]                            -1.8168     0.490    -3.704  0.000    -2.783    -0.850\n\n\n  C(Force)[T.Leicestershire]                  -0.3087     0.483    -0.639  0.523    -1.260     0.643\n\n\n  C(Force)[T.Northamptonshire]                -3.0584     0.941    -3.252  0.001    -4.912    -1.205\n\n\n  C(Force)[T.Nottinghamshire]                 -0.6398     0.620    -1.032  0.303    -1.862     0.582\n\n\n  C(Force)[T.South Yorkshire]                  0.8367     0.745     1.123  0.263    -0.632     2.305\n\n\n  C(Force)[T.Staffordshire]                    0.8295     0.902     0.919  0.359    -0.948     2.607\n\n\n  group_A[T.True]                              1.5015     0.429     3.496  0.001     0.655     2.348\n\n\n  group_B[T.True]                              2.0563     0.418     4.917  0.000     1.232     2.880\n\n\n  C(quarter)[T.2]                             -0.0358     0.220    -0.162  0.871    -0.470     0.399\n\n\n  C(quarter)[T.3]                             -1.0428     0.256    -4.077  0.000    -1.547    -0.539\n\n\n  C(quarter)[T.4]                             -1.1036     0.240    -4.601  0.000    -1.576    -0.631\n\n\n  running_var                                 -0.0171     0.056    -0.305  0.761    -0.128     0.093\n\n\n  C(Force)[T.Bedfordshire]:running_var        -0.1477     0.088    -1.670  0.096    -0.322     0.027\n\n\n  C(Force)[T.Cheshire]:running_var            -0.1680     0.070    -2.388  0.018    -0.307    -0.029\n\n\n  C(Force)[T.Derbyshire]:running_var          -0.0334     0.069    -0.487  0.627    -0.168     0.102\n\n\n  C(Force)[T.Essex]:running_var               -0.1593     0.069    -2.320  0.021    -0.295    -0.024\n\n\n  C(Force)[T.Hampshire]:running_var           -0.2698     0.068    -3.966  0.000    -0.404    -0.136\n\n\n  C(Force)[T.Hertfordshire]:running_var       -0.1377     0.068    -2.032  0.043    -0.271    -0.004\n\n\n  C(Force)[T.Kent]:running_var                -0.1106     0.068    -1.629  0.105    -0.244     0.023\n\n\n  C(Force)[T.Leicestershire]:running_var       0.0146     0.068     0.216  0.829    -0.119     0.148\n\n\n  C(Force)[T.Northamptonshire]:running_var     0.0375     0.091     0.413  0.680    -0.141     0.216\n\n\n  C(Force)[T.Nottinghamshire]:running_var     -0.0865     0.068    -1.273  0.204    -0.220     0.047\n\n\n  C(Force)[T.South Yorkshire]:running_var     -0.0445     0.068    -0.657  0.512    -0.178     0.089\n\n\n  C(Force)[T.Staffordshire]:running_var       -0.1167     0.068    -1.707  0.089    -0.251     0.018\n\n\n  total_offences                              -0.0012     0.001    -2.279  0.024    -0.002    -0.000\n\n\n  mandatory_attendance                         0.3016     0.777     0.388  0.698    -1.229     1.832\n\n\n\n\n  Omnibus:        3.734   Durbin-Watson:         2.028\n\n\n  Prob(Omnibus):  0.155   Jarque-Bera (JB):      3.443\n\n\n  Skew:           0.221   Prob(JB):              0.179\n\n\n  Kurtosis:       3.350   Cond. No.           4.44e+18\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 2.52e-29. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\n\n\nNo matter how we model it, we’re again not finding meaningful effects - mandatory attendance does seem to be associated with slightly higher detected rates, on average, but this isn’t statistically significant. Put simply, examining the “statistical noise” in the rest of our model, we can’t confidently say any increases aren’t just due to random luck.\nAs one final approach, we’ll use panel data using the LinearModels library - far from the best way to examine how effects change over time, but not a bad starter for ten.\n\n\nCode\npanel_data = force_comparison.set_index([\"Force\", \"date\"])\npanel_data\n\n\n\n\n\n\n  \n    \n      \n      \n      total_offences\n      total_detected\n      detected_rate\n      mandatory_attendance\n    \n    \n      Force\n      date\n      \n      \n      \n      \n    \n  \n  \n    \n      Avon and Somerset\n      2017-01-01\n      2151.0\n      112.0\n      0.052069\n      False\n    \n    \n      Bedfordshire\n      2017-01-01\n      1142.0\n      120.0\n      0.105079\n      False\n    \n    \n      British Transport Police\n      2017-01-01\n      0.0\n      0.0\n      0.000000\n      False\n    \n    \n      Cambridgeshire\n      2017-01-01\n      1109.0\n      99.0\n      0.089270\n      False\n    \n    \n      Cheshire\n      2017-01-01\n      790.0\n      80.0\n      0.101266\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      Warwickshire\n      2021-10-01\n      376.0\n      8.0\n      0.021277\n      False\n    \n    \n      West Mercia\n      2021-10-01\n      857.0\n      8.0\n      0.009335\n      False\n    \n    \n      West Midlands\n      2021-10-01\n      4061.0\n      93.0\n      0.022901\n      False\n    \n    \n      West Yorkshire\n      2021-10-01\n      2406.0\n      81.0\n      0.033666\n      False\n    \n    \n      Wiltshire\n      2021-10-01\n      265.0\n      8.0\n      0.030189\n      False\n    \n  \n\n880 rows × 4 columns\n\n\n\n\n\nCode\nmod = PanelOLS.from_formula(\n    \"detected_rate ~ 1 + mandatory_attendance + EntityEffects + TimeEffects\", data=panel_data\n)\nmod.fit()\n\n\n\n\nPanelOLS Estimation Summary\n\n  Dep. Variable:      detected_rate    R-squared:             0.0013 \n\n\n  Estimator:            PanelOLS       R-squared (Between):   -0.0444\n\n\n  No. Observations:        880         R-squared (Within):    -0.0013\n\n\n  Date:             Sat, Dec 31 2022   R-squared (Overall):   -0.0059\n\n\n  Time:                 16:25:24       Log-likelihood         1326.8 \n\n\n  Cov. Estimator:      Unadjusted                                    \n\n\n                                       F-statistic:           1.0290 \n\n\n  Entities:                44          P-value                0.3107 \n\n\n  Avg Obs:               20.000        Distribution:         F(1,816)\n\n\n  Min Obs:               20.000                                      \n\n\n  Max Obs:               20.000        F-statistic (robust):  1.0290 \n\n\n                                       P-value                0.3107 \n\n\n  Time periods:            20          Distribution:         F(1,816)\n\n\n  Avg Obs:               44.000                                      \n\n\n  Min Obs:               44.000                                      \n\n\n  Max Obs:               44.000                                      \n\n\n                                                                     \n\n\n\nParameter Estimates\n\n                       Parameter Std. Err. T-stat P-value Lower CI Upper CI\n\n\n  Intercept             0.0614    0.0019   31.890 0.0000   0.0576   0.0652 \n\n\n  mandatory_attendance  0.0155    0.0153   1.0144 0.3107   -0.0145  0.0456 \n\nF-test for Poolability: 2.3035P-value: 0.0000Distribution: F(62,816)Included effects: Entity, Timeid: 0x1ecad429930\n\n\nA similar pattern emerges - we see a small increase associated with mandatory attendance, but it’s non-significant - the increases are too small or too random to say the effect is associated due to attendance, rather than just random data noise.\nThat said, there are plenty of issues with our approach! We’ve only tried straightforward linear approaches, and given we’re relying on two forces and quarterly data, we’d struggle to detect small effects… hopefully we’ll notice something more stark when I come back to this in a few months for part 2, time series approaches!"
  },
  {
    "objectID": "posts/Copbot/explainer.html",
    "href": "posts/Copbot/explainer.html",
    "title": "Teaching OpenAI to assess risk, with CopBot!",
    "section": "",
    "text": "In case you’ve been living under a rock and missed all the recent excitement around ChatGPT, it works really, really well now… Like, “oh god that’s actual wizardry” well. The fact it would probably breeze through a Voight-Kampff test should probably worry me a bit, but more importantly, what cool stuff can we do with it?\nGiven using the tool in development is actually quite affordable, I thought I’d build a few prototypes for public safety use cases, and see how it performs… and after a few hours of work, “CopBot” is alive!\nReading, evaluating and prioritising risk is a core policing skill: from investigating crimes with piles of witness statements in dingy offices, to responding to life threatening incidents incidents at 2am on a rainy street, the key thread is you’ve got loads of risk, and you need to decide what to do first, taking into account decades worth of policing legislation and policy, and making sure your decision is justifiable when it inevitably goes wrong.\nSo can an AI learn to “speak police” and make vaguely convicing risk assessments? I scraped all the policing guidance I could find, fed it to an OpenAI powered model, and asked it to predict risk for missing people in an explainable way…and it kind of works! You can try out the prototype here.\nOf course, please don’t put any real personal data through it, or rely on it for actual work…it’s a weekend experiment, not an actual policing tool. For those who want to get into the detail, the project code is available here (built using Jupyter Notebooks with nbdev, which makes it easy to read and deserves it’s own blog post), and I thought I’d put together a post to explain the high level principles and document some thoughts."
  },
  {
    "objectID": "posts/Copbot/explainer.html#teaching-policing-to-ai",
    "href": "posts/Copbot/explainer.html#teaching-policing-to-ai",
    "title": "Teaching OpenAI to assess risk, with CopBot!",
    "section": "Teaching policing to AI",
    "text": "Teaching policing to AI\nYou’ve probably already played with the public version of ChatGPT, and hopefully understand the basic principles: the language model is trained on a huge corpus of publicly available text, aiming to answer questions in a helpful way…but not necessarily an operationally useful one, nor one that takes into account of legislation and best practice.\nIf you had all the relevant documentation to hand, and knew exactly which was most relevant, you could just feed it into your prompt - something like “answer this operation question, but consider this legislative text” - but how do you do that if you don’t know what’s relevant? If you want to teach it how to investigate missing people, where do you event start?\nThankfully, the College of Policing is wonderfully transparent, and shares all their Authorised Professional Practice in one place (though sadly not through an API). With a clever web crawler, you can quickly collect every page of guidance, as well as every other connected document.\nThe next stage is to convert all those documents into embeddings, turning them from text into numerical of their semantic meaning (according to the model). I’ve previously done those computations myself, using libraries like Huggingface or Spacy, but OpenAI provides all of their embeddings through a quite affordable API you can query.\nOnce you’ve converted all your all our documentation into embeddings, we can quickly calculate the distance between our question and each document, and that tells us which pieces from our corpus of of text is most closely associated to the question we’re asking!\nUnlike ChatGPT though, we want to limit our model to only answer questions from that corpus: if our documents don’t contain information about a certain topic, don’t just go and hallucinate a whole new answer. Then it’s just a matter of writing our question to extract the most meaningful documents linked to our question, feed them into our prompt, and ask OpenAI to complete the answer, unless it doesn’t know based off the documentation provided.\nSo how does it work? Well, let’s start by asking it some generic questions about policy.\n\nanswer_question(df, question=\"What are the most important factors to consider when searching for a missing person?\", debug=True)\n\nContext:\nVery detailed information and a lifestyle profile will be needed in high-risk cases consider taking a full statement from the person reporting the missing person as well as any other key individuals (for example, the last person to see them) conduct initial searches of relevant premises, the extent and nature of the search should be recorded (see Search) consider seizing electronic devices, computers, and other documentation, (for example, diaries, financial records and notes) and obtain details of usernames and passwords obtain photos of the missing person; these should ideally be current likeness of the missing person and obtained in a digital format obtain details of the individual’s mobile phone and if they have it with them; if the missing person has a mobile phone arrange for a TextSafe© to be sent by the charity, Missing People obtain details of any vehicles that they may have access to and place markers on relevant vehicles on the PNC without delay consider obtaining any physical evidence of identity such as fingerprints or DNA samples in accordance with Code of Practice (2009) Collection of Missing Persons Data. confirm if the person has taken their passport, (consider prompt circulation if it is deemed likely the individual may leave the country (This is particularly important where there are concerns that an individual has been radicalised and is intending to travel abroad, see National Ports Office – Heathrow) make all immediate relevant enquiries and take immediate actions in order to locate the missing person consider the need for specialist officers or resources, for example, force helicopters, dogs, financial investigation officers upload the missing person report circulate details of the missing person on local information systems and to relevant local partners, for example, hospitals, ambulance service, taxi and bus firms It is important for an individual (who has responsibilities/concerns for the missing person) to be identified who can act as the point of contact for the police. The police will agree with the individual/family when they will next be contacted. This person should be provided with a call reference number and given details of how to contact the police with any further information they have about the case or if they would like to receive an update. An assessment must be made of the level of support required for the family, residential worker or foster carer and consideration should be given to appointing a family liaison officer. Information should then be provided regarding additional organisations that may be able to assist or support them.\n\n###\n\nIt is also important to record the name and contact details of the person who gave that information and when this happened. Missing people may be at risk of harm resulting from factors such as: an inability to cope with weather conditions being the victim of violent crime risks relating to non-physical harm, for example, the people they are with, the places or circumstances they are in For further information see Mental Vulnerability and illness: Vulnerability assessment framework and Vulnerability-related risk guidelines.  Assessing risk levels and taking action The missing persons process chart and risk table can assist officers to assess the risk level and appropriate actions that should be assigned to each case. This is a framework to assist practitioners in making operational decisions. Each case requires individual assessment and decision-making. These resources have been developed based on professional expertise and practitioner experience and are available using the following links. Risk assessment table Missing persons process chart It is important to adopt an investigative approach to all reports, ensuring that assumptions are not made about the reasons for going missing. The importance and relevance of risk factors will depend on the circumstances of each case and require investigation to determine if there is a cause for concern. The approach should not be regarded as a mechanical one and police officers should be mindful that the risk assessment is subjective, and that just one factor alone may be considered important enough to prompt an urgent response. For this reason professional experience suggests that a numerical scoring system may not be the most appropriate method of identifying and communicating risk. A decision-making guide should be used to encourage consistency in the application of the process, however, it should only be used as a guide. Other grounds for suspicion, even if intuitive, can be registered by the investigator and officers should be supported in applying the National Decision Model when making these assessments. Accurate record keeping It is essential to accurately record the information received about a missing person. Information that may relate to any perceived risk should be captured at the time of reporting so that it can be used to inform any investigative enquires. This will also prevent duplication of questioning and report writing by investigation officers. Call handlers should be supported by their supervisors to ensure that sufficient time is given to each call to enable all the relevant information to be captured. Detailed missing person reports should be created and appropriate force recording systems and IT systems should be in place to support this process. It is important that the information used and rationale for the risk assessment is clearly recorded.\n\n###\n\n First published 21 November 2016  Updated 22 November 2016   Written by College of Policing  Missing persons quick reference guides  4 mins read   The primary consideration for the first responder is the safety of the missing person. Judgements made at this early stage may have a significant impact on the outcome of the investigation. The initial investigating officer (IIO) should: begin the investigation – identify places where the person might be, check information and assumptions, corroborate what they have been told, review the risk assessment, seek and secure evidence conduct appropriate searches – places where the missing person might be such as hospital, custody, friends and or relatives conduct appropriate intelligence checks – PNC, force intelligence systems, ViSOR, PND continually reassess the level of risk using the risk principles assess the level of support required for the missing person’s family, residential worker or foster carer as appropriate If it is suspected a serious crime has occurred or the individual is at significant risk of harm, the IIO should inform a supervising officer immediately. There are a number of actions that may be carried out by the IIO to ensure that sufficient information is gathered: Consider seizing electronic devices, computers, and other documentation, for example, diaries, financial records and notes and obtain details of usernames and passwords. Obtain photos of the missing person. These should ideally be current likeness of the missing person and obtained in a digital format. Obtain details of the individual’s mobile phone and if they have it with them. If they do, arrange for a TextSafe© to be sent by the Missing People charity. Obtain details of any vehicles to which the missing person may have access. Confirm if the person has taken their passport; consider prompt circulation if it is deemed likely the individual may leave the country (This is particularly important where there are concerns that an individual has been radicalised and is intending to travel abroad. See National Ports Office – Heathrow.) Upload the missing person report and place markers on relevant vehicles on the PNC without delay. Circulate details of the missing person on local information systems and to relevant local partners, for example, hospitals, ambulance service, taxi and bus firms. Consider obtaining any physical evidence of identity such as fingerprints or DNA samples (in accordance with Code of Practice (2009) Collection of Missing Persons Data).\n\n\n\n\n\n'The primary consideration for the first responder is the safety of the missing person. Judgements made at this early stage may have a significant impact on the outcome of the investigation. The initial investigating officer should begin the investigation by identifying places where the person might be, check information and assumptions, corroborate what they have been told, review the risk assessment, seek and secure evidence, conduct appropriate searches, conduct appropriate intelligence checks, continually reassess the level of risk using the risk principles, assess the level of support required for the missing person’s family, residential worker or foster carer as appropriate, consider seizing electronic devices, computers, and other documentation, obtain photos of the missing person, obtain details of the individual’s mobile phone, obtain'\n\n\nYou can see that as I’ve enabled debug mode on the function, it will start by printing the relevant documentation it has found (the context), before then giving its answer…which is actually pretty convicing! Let’s see what happens if I ask a question it can’t know the answer to.\n\nanswer_question(df, question=\"What day is it?\", debug=True)\n\nContext:\n.\n\n###\n\nIt’s also been raining heavily in the night and we have further calls about flooding in the road, so we ring Highways to inform them. I have a little smile to myself as I remember a call in the summer about cars stopping on the M11 because a mother duck and her ducklings were crossing the road. Lunchtime looms. I’m feeling hungry, but that disappears when I take a call from a 16-year-old male, who tells me that he can’t cope any more. He has cut himself with a knife but he doesn’t want to die. His sister has just had a baby. This goes on an emergency straight away, and officers are dispatched within three minutes. I have to talk to him about anything I can to distract him from his misery – luckily, I am good at small talk! Officers arrive and I feel relief as I can hang up the phone. COVID-19 has really affected Essex this year. People are low and weary. You can hear it in their voices. The number of mental health incidents has gone through the roof, and even the force control room team is quieter. Because of the onset of lockdown restrictions, more calls are coming in from the public reporting their neighbours for flouting the rules: 'We’re following the rules, why don’t they? What makes them think they are special?'  Gone are the past calls about drunken people leaving the pub. Instead, we have members of the public who are tired of being tied to the house and resentful of those who ignore the restrictions. After lunch, we receive a flurry of calls. There’s a domestic, involving a woman who tells me that ‘he didn’t mean to hit me, he loves me’. I spend time with this caller. There are three horses in the road. A driver has hit a dog and is upset, so I reassure him it wasn’t his fault. It gets busier. Essex is up and running but I am not. I feel tired but this is my job, so I make sure that nobody will hear it in my voice. Finally, it’s time to go home and hang up the headset for another day. I tend not to reflect on my day too much, so I can have some time to myself. There is no typical day in the control room.\n\n###\n\nThe vessel is identified as a rigid-hull inflatable boat (RHIB), which has four people on board wearing foul-weather gear and balaclavas. It appears to still have half a load of bales suspected to contain cannabis resin, wrapped in their distinctive blue and light brown hessian (approximately ¾ of a ton in total). Blue police beacons are engaged and the vessel has been repeatedly signalled to stop, but continues to carry out manoeuvres in an attempt to gain distance. The sea spray is cold and strikes the flesh like pins and needles. After a few minutes, which feel like an eternity, the pursuit is discontinued at the 3NM limits of territorial waters. CAD are informed and requested to inform Spanish Guardia Civil of the vessel’s last known speed and heading. No doubt it will return that night to attempt to unload its remaining cargo. I monitor the area and one of the crew observes something floating in the water. It is suspected to be a bale of cannabis resin, approximately 30-35kg in weight. It is retrieved and found to be in a good state with no marine growth. It is unclear whether it fell off the vessel recently pursued or belongs to a previous incident. The area is searched but nothing more is found. I return to GGMS, where I conduct a debrief. The bale is processed and conveyed to a police station for secure storage. I then complete the necessary paperwork, while the crew slip a zodiac out from the water, which was linked to the recovery of five North African migrants from the sea the previous day, and place the vessel on land. We continue with the mundane but necessary yard and vessel maintenance work. We grab a bite to eat, chat about the morning pursuit and joke about our recent mishaps. The work can be intense but we always manage to fill it with laughter. It’s now 2pm. The afternoon crew arrive. I give them a brief and handover. I then deploy on our training RHIB. As a qualified police instructor, I carry out powerboat training drills for the junior crew member. It reminds me of my early days at the helm and I enjoy passing on the knowledge. At 3.30pm, I return to base and carry out a debrief on the day’s activities. It’s the end of the shift. Let’s see what the next day brings – maybe another encounter with that RHIB.\n\n###\n\nChange to the order of the documents.\n\n###\n\n.police.uk news views.            News & views | College of Policing             Sorry, you need to enable JavaScript to visit this website.    Skip to content Jump to search         Menu      Secondary navigation About us News & views Contact us  Search Search     Main navigation Policing guidance Research Career & learning Support for forces Ethics     Breadcrumb Home           News & views            News & views         On this page         All news  Category: - Any -EventGoing equippedListicleBriefExplainerCase studyConsultationNewsViews  Sort by: Most recentLess recent       13 March 2023 Bursary scheme 2023 – applications now open   News  Applications for higher education funding are open and close on Monday 3 April.\n\n###\n\nUpdated 26 July 2022    Brief   UK and Switzerland – agreement on police cooperation  Treaty to strengthen police cooperation between law enforcement authorities in both countries Published 24 March 2022    Case study   Director of intelligence – a day in the life  There is no such thing as a typical day in any policing role and the director of intelligence is no different Published 30 July 2021    Case study   Head of intelligence analysis – a day in the life  A key senior role in the analysis side of the intelligence job family Published 30 July 2021    Case study   Intelligence manager – a day in the life  Overseeing the management, development and collection of intelligence from various sources Published 30 July 2021    Case study   Intelligence unit supervisor – a day in the life  Leading a team of intelligence officers to gather, develop and disseminate intelligence in support of local and national crime investigations Published 30 July 2021    Case study   Senior intelligence analyst – a day in the life  Managing an analytical team or a specific area of business within the analytical function Published 30 July 2021    Case study   Intelligence support officer – a day in the life  Providing information and data management and broad administrative support as part of an intelligence unit Published 30 July 2021    Case study   Intelligence officer – a day in the life  Managing dissemination of gathered intelligence to support reactive, proactive and/or crimes in action and providing advice on appropriate tactical options to support policing priorities Published 30 July 2021    Case study   Researcher in intelligence – a day in the life  Using a wide variety of sources to assess and evaluate information – they can then advise on the creation of intelligence products used to support decision-making at a strategic, tactical and/or operational level Published 30 July 2021    Case study   Intelligence analyst – a day in the life  Providing expertise through the development and use of analytical products to help make decisions at a strategic, tactical and operational level Published 30 July 2021    Case study   Using intelligence skills to target criminals ethically and proportionately  A role where it's important to keep on top of your continuing professional development and learn from research and analysis colleagues Published 5 November 2020     Looking for more on this topic? Try searching Intelligence       Was this page useful?  Yes  No   Do not provide personal information such as your name or email address in the feedback form.\n\n\n\n\n\n\"I don't know.\"\n\n\nSucess! While it does find a bunch of documentation relating to the current day in our corpus, it does identify that it doesn’t know what the day is now.\nLet’s try something a little bit more technical, and see if it can answer a few questions from the Sergeants’ and inspectors’ NPPF legal exams. I couldn’t find any official questions available publicly, here are two questions from an online guidance service.\n\nquestion = \"\"\" Officer Jennings is on his evening patrol. He is just about to finish for the day. As he walks down the street, he is approached by a man named Mark, who claims that he saw a man (named Steven) driving down a road not far from the location. Mark claims that he saw Steven drive into a cyclist, before driving off without stopping. Luckily, the cyclist was unharmed. The cyclist was named Kevin. Mark spoke to Kevin, and discovered that he is a 42 year old man, with a wife and two daughters.\n\nFifteen minutes later, Officer Jennings manages to stop the car being driven by Steven. He pulls him over to the side of the road, and orders him to step out of the car.\n\n \n\nReferring s.6 (5) of the Road Traffic Act 1988, is Officer Jennings within his legal rights to order that Steven takes a preliminary breath test?\n\nA – No. Officer Jennings has no right to tell Steven what he can and can’t do. He should never have stopped Steven in the first place.\n\nB – No. In order for Officer Jennings to do this, an accident must have happened. The fact that Officer Jennings suspects an accident has taken place, does not meet this requirement.\n\nC – Yes. However, the breath test must take place within or close to an area where the requirements for Steven to cooperate, can be imposed.\n\nD – Yes. Officer Jennings can tell Steven to do whatever he wants, as he’s a police officer.\"\"\"\n\nanswer_sergeant_exam_question(df,question)\n\n'B - No. In order for Officer Jennings to do this, an accident must have happened. The fact that Officer Jennings suspects an accident has taken place, does not meet this requirement.'\n\n\n\nquestion = \"\"\"Sarah is walking to work one morning, when she is approached from behind by Henry and Jacob.\n\n‘We won’t hurt you, as long as you give us the bag,’ Henry says.\n\n‘You’re not getting it!’ Sarah shouts.\n\nHenry grabs Sarah and holds a knife to her throat, whilst Jacob tries to snatch her bag.\n\nSarah fights with her attackers, and begins to run away. As the two men chase her, she trips and bangs her head on the pavement. She is taken to hospital and dies from head trauma.\n\nBased on the above information, which of the following options is correct?\n\nA – Jacob cannot be held accountable for the death of Sarah, as he simply tried to take her bag.\n\nB – Jacob and Henry will be charged with attempted robbery, but not in the death of Sarah.\n\nC – Jacob and Henry could be considered liable for the death of Sarah.\n\nD – Sarah’s death cannot be blamed on Henry and Jacob, as it was her choice to run away.\"\"\"\n\nanswer_sergeant_exam_question(df,question)\n\n'C - The answer is C because Jacob and Henry could be considered liable for the death of Sarah, as they were the ones who initiated the attack and chased her, which led to her tripping and hitting her head.'\n\n\nThe answer both should have been C, so that’s 50/50 for CopBot… not bad! You can see it’s referring to to relevant guidance, but sadly that doesn’t really help you pass a promotion exam (though I suspect it would do seriously well trained on a bank of questions instead)."
  },
  {
    "objectID": "posts/Copbot/explainer.html#so-does-it-work",
    "href": "posts/Copbot/explainer.html#so-does-it-work",
    "title": "Teaching OpenAI to assess risk, with CopBot!",
    "section": "So does it work?",
    "text": "So does it work?\nBefore we test our model on fictional missing scenarios, I made one last tweak: I’ve amended the prompt to explictly refer to identified risk factors, and return them in a given format - you can see how it works below.\n\nmargaret_risk_profile = \"\"\" Margaret is a 97 year old woman with severe dementia from Twickenham. She lives in supported accomodation, but regularly goes missing, as she walks out when left unsupervised.\n\nShe has been missing 6 hours, and it is now 2200.  It is getting dark, and staff are saying she is rarely missing this long\"\"\"\n\nmargaret_answer = machine_risk_assessment(margaret_risk_profile, df, debug=True)\nmargaret_answer\n\nQuestion:\n Margaret is a 97 year old woman with severe dementia from Twickenham. She lives in supported accomodation, but regularly goes missing, as she walks out when left unsupervised.\n\nShe has been missing 6 hours, and it is now 2200.  It is getting dark, and staff are saying she is rarely missing this long\nContext:\n.police.uk research projects maximizing effectiveness police scotland investigations when people living dementia go missing.            Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing | College of Policing             Sorry, you need to enable JavaScript to visit this website.    Skip to content Jump to search         Menu      Secondary navigation About us News & views Contact us  Search Search     Main navigation Policing guidance Research Career & learning Support for forces Ethics     Breadcrumb Home Research Research projects map           Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing            Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing         On this page     This research aims to explore the effectiveness of searches for people living with dementia who are reported as missing. Key details             Lead institution            Queen Margaret University             Principal researcher(s)            Alistair Shields [email protected]             Police region                   Scotland                    Level of research                   PhD                    Project start date            September 2018             Date due for completion            January 2023  Research context In Scotland annually there are approximately 530 missing person incidents reported to the police for people living with dementia. These incidents are emotionally distressing for the families and caregivers who do not know the whereabouts of the reported person. For the person living with dementia the consequences of being missing worsen with the passage of time. It has been suggested that when reported as missing the person travels toward a place orientated to their past. The knowledge of such locations to inform police investigations, when someone is reported as missing, is commonly not available. Aim For people living with dementia who are reported as missing to improve search effectiveness by better defining areas where police should search.\n\n###\n\n First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review – please see APP on international investigation for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child  Voice of the child practice briefing  Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children’s Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‘streetwise’ and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children may put themselves in danger because they may have been abused, neglected or rejected by their families or others and, as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to work with all the agencies and carers that have been in regular contact with the child as they may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\n\n###\n\nThreshold for referral of missing persons: the individual is a ‘repeat missing person’, (reported as missing three times in a rolling 90 day period) the individual has experienced, or is likely to experience significant harm for children, the parent or carer appears unable or unwilling to work to support and meet the needs of a child that has gone missing. The Protection Procedures also recommend that there is very close working between children's social care and policing to ensure that all investigations are undertaken efficiently and without duplication of effort. Where appropriate, a multi-agency meeting is convened after a child has been missing from home or care for more than seven days, or has been missing on more than three occasions in a twelve month period. While standard thresholds for referral may be useful, senior officers will want to be sure that a process is in place to ensure cases are recognised which may require a greater safeguarding response before the threshold has been reached. For example, individuals reported missing for the first time where significant risks have been identified should be referred immediately for multi-agency support, without requiring further reports. For further information see HM Government (2018) Working together to safeguard children: A guide to inter-agency working to safeguard and promote the welfare of children. Prevention and intervention strategies Collecting and analysing data about cases will help the police and other agencies to understand whether there are any patterns related to missing persons incidents. The results of routine data analysis should be shared to inform the development and review of prevention and intervention strategies. Missing Persons Coordinators are vital for this information analysis and sharing. Interventions might include, for example, the use of Child Abduction Warning Notices, or referrals to support services. It is important that such interventions take place in appropriate circumstances and are not used as a single response when a person is at risk of harm, but form one strand of a more comprehensive approach. Regular liaison between neighbourhood policing teams and children’s and adults’ care providers may enable relationships to be developed between police officers, the staff of care establishments, and individuals who are looked after. These relationships may then support effective police intervention and the speedy resolution of cases. For lessons from other cases, see IOPC Learning the Lesson. Understanding the reasons for going missing Understanding the reasons why an individual went missing may help to prevent future harm to those individuals. The officers in charge of local areas should have clear plans on how they intend to reduce the number of people who go missing in their area.\n\n###\n\nIt’s also been raining heavily in the night and we have further calls about flooding in the road, so we ring Highways to inform them. I have a little smile to myself as I remember a call in the summer about cars stopping on the M11 because a mother duck and her ducklings were crossing the road. Lunchtime looms. I’m feeling hungry, but that disappears when I take a call from a 16-year-old male, who tells me that he can’t cope any more. He has cut himself with a knife but he doesn’t want to die. His sister has just had a baby. This goes on an emergency straight away, and officers are dispatched within three minutes. I have to talk to him about anything I can to distract him from his misery – luckily, I am good at small talk! Officers arrive and I feel relief as I can hang up the phone. COVID-19 has really affected Essex this year. People are low and weary. You can hear it in their voices. The number of mental health incidents has gone through the roof, and even the force control room team is quieter. Because of the onset of lockdown restrictions, more calls are coming in from the public reporting their neighbours for flouting the rules: 'We’re following the rules, why don’t they? What makes them think they are special?'  Gone are the past calls about drunken people leaving the pub. Instead, we have members of the public who are tired of being tied to the house and resentful of those who ignore the restrictions. After lunch, we receive a flurry of calls. There’s a domestic, involving a woman who tells me that ‘he didn’t mean to hit me, he loves me’. I spend time with this caller. There are three horses in the road. A driver has hit a dog and is upset, so I reassure him it wasn’t his fault. It gets busier. Essex is up and running but I am not. I feel tired but this is my job, so I make sure that nobody will hear it in my voice. Finally, it’s time to go home and hang up the headset for another day. I tend not to reflect on my day too much, so I can have some time to myself. There is no typical day in the control room.\n\n\n\n\n\n('Graded as High risk, because of the below risk factors: \\n- Margaret is 97 years old and has severe dementia, making her more vulnerable to harm\\n- She has been missing for 6 hours, which is longer than usual, and it is now dark outside, increasing the risk of harm',\n \".police.uk research projects maximizing effectiveness police scotland investigations when people living dementia go missing.            Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing | College of Policing             Sorry, you need to enable JavaScript to visit this website.    Skip to content Jump to search         Menu      Secondary navigation About us News & views Contact us  Search Search     Main navigation Policing guidance Research Career & learning Support for forces Ethics     Breadcrumb Home Research Research projects map           Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing            Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing         On this page     This research aims to explore the effectiveness of searches for people living with dementia who are reported as missing. Key details             Lead institution            Queen Margaret University             Principal researcher(s)            Alistair Shields [email\\xa0protected]             Police region                   Scotland                    Level of research                   PhD                    Project start date            September 2018             Date due for completion            January 2023  Research context In Scotland annually there are approximately 530 missing person incidents reported to the police for people living with dementia. These incidents are emotionally distressing for the families and caregivers who do not know the whereabouts of the reported person. For the person living with dementia the consequences of being missing worsen with the passage of time. It has been suggested that when reported as missing the person travels toward a place orientated to their past. The knowledge of such locations to inform police investigations, when someone is reported as missing, is commonly not available. Aim For people living with dementia who are reported as missing to improve search effectiveness by better defining areas where police should search.\\n\\n###\\n\\n First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review – please see\\xa0APP\\xa0on international investigation\\xa0for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see\\xa0Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child\\xa0 Voice of the child practice briefing\\xa0 Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children’s Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‘streetwise’ and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children\\xa0may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,\\xa0as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to\\xa0work with all the agencies and carers\\xa0that\\xa0have been\\xa0in regular contact with the child as they\\xa0may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\\n\\n###\\n\\nThreshold for referral of missing persons: the individual is a ‘repeat missing person’, (reported as missing three times in a rolling 90 day period) the individual has experienced, or is likely to experience significant harm for children, the parent or carer appears unable or unwilling to work to support and meet the needs of a child that has gone missing. The\\xa0Protection Procedures\\xa0also recommend that there is very close working between children's social care and policing to ensure that all investigations are undertaken efficiently and without duplication of effort. Where appropriate, a multi-agency meeting is convened after a child has been missing from home or care for more than seven days, or has been missing on more than three occasions in a twelve month period. While standard thresholds for referral may be useful, senior officers will want to be sure that a process is in place to ensure cases are recognised which may require a greater safeguarding response before the threshold has been reached. For example, individuals reported missing for the first time where significant risks have been identified should be referred immediately for multi-agency support, without requiring further reports. For further information see\\xa0HM Government (2018) Working together to safeguard children: A guide to inter-agency working to safeguard and promote the welfare of children. Prevention and intervention strategies Collecting and analysing data about cases will help the police and other agencies to understand whether there are any patterns related to missing persons incidents. The results of routine data analysis should be shared to inform the development and review of prevention and intervention strategies. Missing Persons Coordinators are vital for this information analysis and sharing. Interventions might include, for example, the use of\\xa0Child Abduction Warning Notices, or referrals to support services. It is important that such interventions take place in appropriate circumstances and are not used as a single response when a person is at risk of harm, but form one strand of a more comprehensive approach. Regular liaison between neighbourhood policing teams and children’s and adults’ care providers may enable relationships to be developed between police officers, the staff of care establishments, and individuals who are looked after. These relationships may then support effective police intervention and the speedy resolution of cases. For lessons from other cases, see\\xa0IOPC\\xa0Learning the Lesson. Understanding the reasons for going missing Understanding the reasons why an individual went missing may help to prevent future harm to those individuals. The officers in charge of local areas should have clear plans on how they intend to reduce the number of people who go missing in their area.\\n\\n###\\n\\nIt’s also been raining heavily in the night and we have further calls about flooding in the road, so we ring Highways to inform them. I have a little smile to myself as I remember a call in the summer about cars stopping on the M11 because a mother duck and her ducklings were crossing the road. Lunchtime looms. I’m feeling hungry, but that disappears when I take a call from a 16-year-old male, who tells me that he can’t cope any more. He has cut himself with a knife but he doesn’t want to die. His sister has just had a baby. This goes on an emergency straight away, and officers are dispatched within three minutes. I have to talk to him about anything I can to distract him from his misery – luckily, I am good at small talk! Officers arrive and I feel relief as I can hang up the phone. COVID-19 has really affected Essex this year. People are low and weary. You can hear it in their voices. The number of mental health incidents has gone through the roof, and even the force control room team is quieter. Because of\\xa0the onset of lockdown restrictions, more calls are coming in from the public reporting their neighbours for flouting the rules: 'We’re following the rules, why don’t they? What makes them think they are special?'\\xa0 Gone are the past calls about drunken people leaving the pub. Instead, we have members of the public who are tired of being tied to the house and resentful of those who ignore the restrictions. After lunch, we receive a flurry of calls. There’s a domestic, involving a woman who tells me that ‘he didn’t mean to hit me, he loves me’. I spend time with this caller. There are three horses in the road. A driver has hit a dog and is upset, so I reassure him it wasn’t his fault. It gets busier. Essex is up and running but I am not. I feel tired but this is my job, so I make sure that nobody will hear it in my voice. Finally, it’s time to go home and hang up the headset for another day. I tend not to reflect on my day too much, so I can have some time to myself. There is no typical day in the control room.\")\n\n\n\nabout_james = \"\"\" \nJames is a 34 year old man, who was reported missing by his wife this evening as he has not returned home from work. It is now 2200, and she expected him home by 1900.\nShe says while he does go out for drinks after work sometimes, he has not been out this late before, and his phone is off.\nJames is in good health, there are no mental health concerns or other vulnerabilities. The weather is good, and his friend from work said he'd probably just gone out for drinks.\n\"\"\"\n\njames_answer = machine_risk_assessment(about_james, df, debug=True)\njames_answer\n\nQuestion:\n \nJames is a 34 year old man, who was reported missing by his wife this evening as he has not returned home from work. It is now 2200, and she expected him home by 1900.\nShe says while he does go out for drinks after work sometimes, he has not been out this late before, and his phone is off.\nJames is in good health, there are no mental health concerns or other vulnerabilities. The weather is good, and his friend from work said he'd probably just gone out for drinks.\n\nContext:\n First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review – please see APP on international investigation for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child  Voice of the child practice briefing  Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children’s Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‘streetwise’ and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children may put themselves in danger because they may have been abused, neglected or rejected by their families or others and, as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to work with all the agencies and carers that have been in regular contact with the child as they may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\n\n###\n\n First published 22 November 2016  Updated 16 February 2023   Latest changes  Written by College of Policing  Missing persons  11 mins read   Introduction Going missing should be treated as an indicator that the individual may be at risk of harm. The safeguarding of vulnerable people is paramount and a missing person report should be recognised as an opportunity to identify and address risks. The reasons for a person deciding to go missing may be complex and linked to a variety of social or family issues. Three key factors should be considered in a missing person investigation: protecting those at risk of harm minimising distress and ensuring high quality of service to the families and carers of missing persons prosecuting those who perpetrate harm or pose a risk of harm when this is appropriate and supported by evidence Support for law enforcement agencies Police investigators can contact the following specialists for advice and assistance in missing and unidentified person investigations. UK Missing Persons Unit (UKMPU) on 0800 234 6034 NCA Major Crime Investigative Support (MCIS) on 0345 000 5463 Definition of ‘missing’ Anyone whose whereabouts cannot be established will be considered as missing until located, and their well-being or otherwise confirmed. All reports of missing people sit within a continuum of risk from ‘no apparent risk (absent)’ through to high-risk cases that require immediate, intensive action.  Risk assessment and response The risk assessment table The following table should be used as a guide to an appropriate level of police response based on initial and on-going risk assessment in each case. Risk assessment should be guided by the College of Policing Risk principles, the National Decision Model and Police Code of Ethics. No apparent risk (absent) There is no apparent risk of harm to either the subject or the public. Actions to locate the subject and/or gather further information should be agreed with the informant and a latest review time set to reassess the risk. Low risk The risk of harm to the subject or the public is assessed as possible but minimal. Proportionate enquiries should be carried out to ensure that the individual has not come to harm. Medium risk The risk of harm to the subject or the public is assessed as likely but not serious. This category requires an active and measured response by the police and other agencies in order to trace the missing person and support the person reporting.\n\n###\n\nHigh risk The risk of serious harm to the subject or the public is assessed as very likely. This category almost always requires the immediate deployment of police resources – action may be delayed in exceptional circumstances, such as searching water or forested areas during hours of darkness. A member of the senior management team must be involved in the examination of initial lines of enquiry and approval of appropriate staffing levels. Such cases should lead to the appointment of an investigating officer (IO) and possibly an SIO, and a police search adviser (PolSA).              There should be a press/media strategy and/or close contact with outside agencies. Family support should be put in place where appropriate. The UKMPU should be notified of the case without undue delay. Children’s services must also be notified immediately if the person is under 18. Risk of serious harm has been defined as (Home Office 2002 and OASys 2006): A risk which is life threatening and/or traumatic, and from which recovery, whether physical or psychological, can be expected to be difficult or impossible. Where the risk cannot be accurately assessed without active investigation, appropriate lines of enquiry should be set to gather the required information to inform the risk assessment. The missing persons process chart  Joint responsibility The police are entitled to expect parents and carers, including staff acting in a parenting role in care homes, to accept normal parenting responsibilities and undertake reasonable actions to try and establish the whereabouts of the individual. Children who are breaching parental discipline should not be dealt with by police unless there are other risks. For example, a child who is late home from a party should not be regarded as missing until the parent or carer has undertaken enquiries to locate the child. Once those enquiries have been completed, it may be appropriate to record the child as missing and take actions set out in this APP. Parents or carers may need police support if they are very distressed, incapacitated or otherwise unable to undertake enquiries. In such circumstances, it may be appropriate to make a referral to the local authority so that the standard of care for the missing person can be reviewed. Individuals whose whereabouts are known will not be considered as missing, but may require other police activity in order to ensure their welfare. Police should consult their local public protection procedures to ensure an appropriate safeguarding response is provided.\n\n\n\n\n\n('Graded as Medium risk, because of the below risk factors:\\n- James has not returned home at the expected time, and his phone is off\\n- There is no indication of any mental health concerns or other vulnerabilities that could put him at risk',\n ' First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review – please see\\xa0APP\\xa0on international investigation\\xa0for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see\\xa0Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child\\xa0 Voice of the child practice briefing\\xa0 Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children’s Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‘streetwise’ and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children\\xa0may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,\\xa0as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to\\xa0work with all the agencies and carers\\xa0that\\xa0have been\\xa0in regular contact with the child as they\\xa0may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\\n\\n###\\n\\n First published 22 November 2016  Updated 16 February 2023   Latest changes  Written by College of Policing  Missing persons  11 mins read   Introduction Going missing should be treated as an indicator that the individual may be at risk of harm. The safeguarding of vulnerable people is paramount and a missing person report should be recognised as an opportunity to identify and address risks. The reasons for a person deciding to go missing may be complex and linked to a variety of social or family issues. Three key factors should be considered in a missing person investigation: protecting those at risk of harm minimising distress and ensuring high quality of service to the families and carers of missing persons prosecuting those who perpetrate harm or pose a risk of harm when this is appropriate and supported by evidence Support for law enforcement agencies Police investigators can contact the following specialists for advice and assistance in missing and unidentified person investigations. UK Missing Persons Unit (UKMPU) on 0800\\xa0234 6034 NCA\\xa0Major Crime Investigative Support (MCIS) on 0345 000 5463 Definition of ‘missing’ Anyone whose whereabouts cannot be established will be considered as missing until located, and their well-being or otherwise confirmed. All reports of missing people sit within a continuum of risk from ‘no apparent risk (absent)’ through to high-risk cases\\xa0that require immediate, intensive action.  Risk assessment and response The risk assessment table The following table should be used as a guide to\\xa0an appropriate level of police response based on initial and on-going risk assessment in each case. Risk assessment should be guided by the College of Policing\\xa0Risk principles,\\xa0the\\xa0National Decision Model\\xa0and Police\\xa0Code of Ethics. No apparent risk (absent) There is no apparent risk of harm to either the subject or the public. Actions to locate the subject and/or gather further information should be agreed with the informant and a latest review time set to reassess the risk. Low risk The risk of harm to the subject or the public is assessed as possible but minimal. Proportionate enquiries should be carried out to ensure that the individual has not come to harm. Medium risk The risk of harm to the subject or the public is assessed as likely but not serious. This category requires an active and measured response by the police and other agencies in order to trace the missing person and support the person reporting.\\n\\n###\\n\\nHigh risk The risk of serious harm to the subject or the public is assessed as very likely. This category almost always requires the immediate deployment of police resources – action may be delayed in exceptional circumstances, such as searching water or forested areas during hours of darkness. A member of the senior management team must be involved in the examination of initial lines of enquiry and approval of appropriate staffing levels. Such cases should lead to the appointment of an investigating officer (IO) and possibly an\\xa0SIO, and a police search adviser (PolSA). \\t\\t\\t\\xa0 There should be a press/media strategy and/or close contact with outside agencies. Family support should be put in place where appropriate. The UKMPU\\xa0should be notified of the case without undue delay. Children’s services must also be notified immediately if the person is under 18. Risk of serious harm has been defined as (Home Office 2002 and OASys 2006): A risk which is life threatening and/or traumatic, and from which recovery, whether physical or psychological, can be expected to be difficult or impossible. Where the risk cannot be accurately assessed without active investigation, appropriate lines of enquiry should be set to gather the required information to inform the risk assessment. The missing persons process chart  Joint responsibility The police are entitled to expect parents and carers, including staff acting in a parenting role in care homes, to accept normal parenting responsibilities and undertake reasonable actions to try and establish the whereabouts of the individual. Children who are breaching parental discipline should not be dealt with by police unless there are other risks. For example, a child who is late home from a party should not be regarded as missing until the parent or carer has undertaken enquiries to locate the child. Once those enquiries have been completed, it may be appropriate to record the child as missing and take actions set out in this\\xa0APP. Parents or carers may need police support if they are very distressed, incapacitated or otherwise unable to undertake enquiries. In such circumstances, it may be appropriate to make a referral to the local authority so that the standard of care for the missing person can be reviewed. Individuals whose whereabouts are known will not be considered as missing, but may require other police activity in order to ensure their welfare. Police should consult their local public protection procedures to ensure an appropriate safeguarding response is provided.')\n\n\n\nabout_yannik = \"\"\" Yannik is a 15 year old boy. He has recently been down, and was reported missing by his parents as he did not return home from school today.\n\nHis friends are worried he may be depressed, and when he apparently told one a few days ago 'if it doesn't get any better, I'm going to end it soon'\n\"\"\"\n\nyannik_answer = machine_risk_assessment(about_yannik, df, debug=True)\nyannik_answer\n\nQuestion:\n Yannik is a 15 year old boy. He has recently been down, and was reported missing by his parents as he did not return home from school today.\n\nHis friends are worried he may be depressed, and when he apparently told one a few days ago 'if it doesn't get any better, I'm going to end it soon'\n\nContext:\n First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review – please see APP on international investigation for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child  Voice of the child practice briefing  Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children’s Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‘streetwise’ and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children may put themselves in danger because they may have been abused, neglected or rejected by their families or others and, as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to work with all the agencies and carers that have been in regular contact with the child as they may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\n\n###\n\nIt’s also been raining heavily in the night and we have further calls about flooding in the road, so we ring Highways to inform them. I have a little smile to myself as I remember a call in the summer about cars stopping on the M11 because a mother duck and her ducklings were crossing the road. Lunchtime looms. I’m feeling hungry, but that disappears when I take a call from a 16-year-old male, who tells me that he can’t cope any more. He has cut himself with a knife but he doesn’t want to die. His sister has just had a baby. This goes on an emergency straight away, and officers are dispatched within three minutes. I have to talk to him about anything I can to distract him from his misery – luckily, I am good at small talk! Officers arrive and I feel relief as I can hang up the phone. COVID-19 has really affected Essex this year. People are low and weary. You can hear it in their voices. The number of mental health incidents has gone through the roof, and even the force control room team is quieter. Because of the onset of lockdown restrictions, more calls are coming in from the public reporting their neighbours for flouting the rules: 'We’re following the rules, why don’t they? What makes them think they are special?'  Gone are the past calls about drunken people leaving the pub. Instead, we have members of the public who are tired of being tied to the house and resentful of those who ignore the restrictions. After lunch, we receive a flurry of calls. There’s a domestic, involving a woman who tells me that ‘he didn’t mean to hit me, he loves me’. I spend time with this caller. There are three horses in the road. A driver has hit a dog and is upset, so I reassure him it wasn’t his fault. It gets busier. Essex is up and running but I am not. I feel tired but this is my job, so I make sure that nobody will hear it in my voice. Finally, it’s time to go home and hang up the headset for another day. I tend not to reflect on my day too much, so I can have some time to myself. There is no typical day in the control room.\n\n###\n\nPolice officers should ask the care home or local authority for details of the child’s risk assessment so that it can be taken into account during the investigation. Many forces are using the Philomena Protocol to guide their actions in relation to relevant cases involving children. A child, especially a teenager, is unlikely to share all information about their life with their parents or carers. Investigators should not overlook information from siblings, friends, associates, school teachers and others. The online activity of the child may also provide valuable additional information which parents and carers may not be aware of. For further information see: Children's views on being reported missing from care  No Place at Home - Risks facing children and young people who go missing from out of area placements Child Rescue Alert Child Rescue Alert (CRA) – (available to authorised users logged on to the restricted online College Learn) is a partnership between the police and the media which seeks public assistance when it is feared that a child may be at risk of serious harm. Assistance is sought via TV, radio, text messaging, social and digital media (including the internet) so that relevant information relating to the child, offender or any specified vehicle is passed on to the police. CRA focuses on the risk to the child, rather than whether or not an offence has taken place. The criteria for launching an alert is: The child is apparently under 18 years old There is a perception that the child is in imminent danger of serious harm or death There is sufficient information available to enable the public to assist police in locating the child Child Rescue Alert Activation Protocol (available to authorised users logged on to the restricted online College Learn) The CRA has been expanded to enable alerts to be disseminated by the charity Missing People. The system is managed by the National Crime Agency (NCA) and specialist advice is available 24/7 by contacting 0800 234 6034.\n\n###\n\nThis is particularly true to the Greater Manchester area. As a result, this research seeks to utilise this appeal in providing a platform for young people who have received a Threats to Life Notice and children to parents who have received a Threat to Life Notice, in order to steer them away from potential criminal activity and receive mentoring through football coaching, education and employability skills. Operationally, this would seek to provide tactical options in the short and long-term in instances where a young person (under the age of 18) is particularly vulnerable as either a victim or perpetrator of criminal behaviour, and potentially reduce the threat, risk and harm pertaining to such individuals. The key aims are: to develop the individuals in the cohort with regards to qualifications, personal wellbeing, skill development, employability and social action (part of their local community and their perception of their community) – this in turn will potentially reduce the impact of trauma and increase confidence and self-esteem of the cohort to reduce the effects of adverse childhood experiences through re-sensitisation and raising awareness that adverse childhood experiences should not be considered the norm to understand and improve the delivery and impact of Threats to Life on young people to inform police forces on the best possible process and follow-on actions needed when delivering Threats to Life Notices  Research methodology A cohort of 16 young people between the ages of 13 to 18 will be identified through GMP’s systems who have either directly received a Threat to Life Notice in the past 12 months, or are children to a parent(s) who have received a Threat to Life Notice in the past 12 months. Their details will then be given to the mentors at the Manchester United Youth Foundation who then make contact through post in the first instance to invite the young person into the scheme. More individuals will be invited into the scheme as it progresses to ensure a minimum of 16 participants at any given stage (catering for any refusals to be involved or disengagement).\n\n\n\n\n\n('Graded as High risk, because of the below risk factors: \\n- Yannik is a 15 year old boy who has been reported missing by his parents\\n- His friends are worried he may be depressed\\n- He has expressed suicidal ideation to one of his friends',\n \" First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review – please see\\xa0APP\\xa0on international investigation\\xa0for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see\\xa0Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child\\xa0 Voice of the child practice briefing\\xa0 Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children’s Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‘streetwise’ and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children\\xa0may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,\\xa0as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to\\xa0work with all the agencies and carers\\xa0that\\xa0have been\\xa0in regular contact with the child as they\\xa0may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\\n\\n###\\n\\nIt’s also been raining heavily in the night and we have further calls about flooding in the road, so we ring Highways to inform them. I have a little smile to myself as I remember a call in the summer about cars stopping on the M11 because a mother duck and her ducklings were crossing the road. Lunchtime looms. I’m feeling hungry, but that disappears when I take a call from a 16-year-old male, who tells me that he can’t cope any more. He has cut himself with a knife but he doesn’t want to die. His sister has just had a baby. This goes on an emergency straight away, and officers are dispatched within three minutes. I have to talk to him about anything I can to distract him from his misery – luckily, I am good at small talk! Officers arrive and I feel relief as I can hang up the phone. COVID-19 has really affected Essex this year. People are low and weary. You can hear it in their voices. The number of mental health incidents has gone through the roof, and even the force control room team is quieter. Because of\\xa0the onset of lockdown restrictions, more calls are coming in from the public reporting their neighbours for flouting the rules: 'We’re following the rules, why don’t they? What makes them think they are special?'\\xa0 Gone are the past calls about drunken people leaving the pub. Instead, we have members of the public who are tired of being tied to the house and resentful of those who ignore the restrictions. After lunch, we receive a flurry of calls. There’s a domestic, involving a woman who tells me that ‘he didn’t mean to hit me, he loves me’. I spend time with this caller. There are three horses in the road. A driver has hit a dog and is upset, so I reassure him it wasn’t his fault. It gets busier. Essex is up and running but I am not. I feel tired but this is my job, so I make sure that nobody will hear it in my voice. Finally, it’s time to go home and hang up the headset for another day. I tend not to reflect on my day too much, so I can have some time to myself. There is no typical day in the control room.\\n\\n###\\n\\nPolice officers should ask the care home or local authority for details of the child’s risk assessment so that it can be taken into account during the investigation.\\xa0Many forces are using the Philomena Protocol to guide their actions in relation to relevant cases involving children. A child, especially a teenager, is unlikely to share all information about their life with their parents or carers. Investigators should not overlook information from siblings, friends, associates, school teachers and others. The online activity of the child may also provide valuable additional information which parents and carers may not be aware of. For further information see: Children's views on being reported missing from care\\xa0 No Place at Home - Risks facing children and young people who go missing from out of area placements Child Rescue Alert Child Rescue Alert\\xa0(CRA) – (available to authorised users logged on to the restricted online\\xa0College Learn) is a partnership between the police and the media which seeks public assistance when it is feared that a child may be at risk of serious harm. Assistance is sought via TV, radio, text messaging, social and digital media (including the internet) so that relevant information relating to the child, offender or any specified vehicle is passed on to the police. CRA\\xa0focuses on the risk to the child, rather than whether or not an offence has taken place. The criteria for launching an alert is: The child is apparently under 18 years old There is a perception that the child is in imminent danger of serious harm or death There is sufficient information available to enable the public to assist police in locating the child Child Rescue Alert Activation Protocol\\xa0(available to authorised users logged on to the restricted online\\xa0College Learn) The\\xa0CRA\\xa0has been expanded to enable alerts to be disseminated by the charity Missing People. The system is managed by the National Crime\\xa0Agency (NCA) and specialist advice is available 24/7 by contacting 0800\\xa0234 6034.\\n\\n###\\n\\nThis is particularly true to the Greater Manchester area. As a result, this research seeks to utilise this appeal in providing a platform for young people who have received a Threats to Life Notice and children to parents who have received a Threat to Life Notice, in order to steer them away from potential criminal activity and receive mentoring through football coaching, education and employability skills. Operationally, this would seek to provide tactical options in the short and long-term in instances where a young person (under the age of 18) is particularly vulnerable as either a victim or perpetrator of criminal behaviour, and potentially reduce the threat, risk and harm pertaining to such individuals. The key aims are: to develop the individuals in the cohort with regards to qualifications, personal wellbeing, skill development, employability and social action (part of their local community and their perception of their community) – this in turn will potentially reduce the impact of trauma and increase confidence and self-esteem of the cohort to reduce the effects of adverse childhood experiences through re-sensitisation and raising awareness that adverse childhood experiences should not be considered the norm to understand and improve the delivery and impact of Threats to Life on young people to inform police forces on the best possible process and follow-on actions needed when delivering Threats to Life Notices  Research methodology A cohort of 16 young people between the ages of 13 to 18 will be identified through GMP’s systems who have either directly received a Threat to Life Notice in the past 12 months, or are children to a parent(s) who have received a Threat to Life Notice in the past 12 months. Their details will then be given to the mentors at the Manchester United Youth Foundation who then make contact through post in the first instance to invite the young person into the scheme. More individuals will be invited into the scheme as it progresses to ensure a minimum of 16 participants at any given stage (catering for any refusals to be involved or disengagement).\")\n\n\n\njason_risk_profile = \"\"\" Jason is a 15 year old adult male, who has gone missing from his care home in Southwark. His carer has contacted the school, which has said he was not in today.\nThey that this is not the first time, and that Jason has been seen hanging out with older boys, who may be involved in crime and drugs.\"\"\"\n\n\njason_answer = machine_risk_assessment(jason_risk_profile, df, debug=True)\njason_answer\n\nQuestion:\n Jason is a 15 year old adult male, who has gone missing from his care home in Southwark. His carer has contacted the school, which has said he was not in today.\nThey that this is not the first time, and that Jason has been seen hanging out with older boys, who may be involved in crime and drugs.\nContext:\n First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review – please see APP on international investigation for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child  Voice of the child practice briefing  Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children’s Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‘streetwise’ and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children may put themselves in danger because they may have been abused, neglected or rejected by their families or others and, as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to work with all the agencies and carers that have been in regular contact with the child as they may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\n\n###\n\nThese include children or young people who: go missing – especially on regular occasions from home or care live in a chaotic or dysfunctional family have a history of domestic abuse within the family environment have a history of abuse (including familial child sexual abuse, risk of forced marriage, risk of honour-based violence, physical and emotional abuse and neglect) have experienced or are experiencing problematic parenting have parents who misuse drugs or alcohol have parents with health problems are young carers within the family unit experience social exclusion as a result of poverty have experienced recent bereavement or loss have unsupervised use of social networking chat rooms/sites have mental ill health have social or learning difficulties have low self-esteem or self-confidence are unsure about their sexual orientation or are unable to confide in their family about their sexual orientation misuse alcohol and/or drugs have been or are excluded from mainstream education are involved in gang activity attend school with other young people who are sexually exploited are friends with individuals who are sexually exploited do not have friends in the same age group are being bullied live in care, foster care, hostels and/or bed and breakfast accommodation – particularly when living out of their home area are homeless have associations with gangs through relatives, peers or intimate relationships live in a gang neighbourhood This is not an exhaustive list, nor have the vulnerabilities been listed in order of importance. Children from loving and secure homes can also be victims of sexual exploitation. The characteristics common to all victims are not always their age, ethnicity, disability or sexual orientation, but their powerlessness and vulnerability. Warning signs Despite the increased profile of CSE and improvements in how the police work with partner agencies, CSE cases are still under-reported. The Office of the Children’s Commissioner conducted a two-year inquiry into child sexual exploitation in gangs and groups. Their 2013 report, If only someone had listened, highlights that sexually exploited children are not always identified even when they show signs of being victims. Numerous warning signs were identified in the Office of the Children’s Commissioner 2012 interim report, I thought I was the only one, the only one in the world, which can indicate that a young person is being forced or manipulated into sexual activity and is a victim of sexual exploitation. Practitioners need to be aware of these warning signs and recognise that a victim does not have to exhibit all of the warning signs to be a victim of sexual exploitation.\n\n###\n\nStaff should tell the detainee that she can ask to see the carer at any time. Forces must implement policies and procedures to ensure that all girls who are detained and in custody are under the care of a woman. For further information, see PACE Code C, section 3.20A. Transporting children and young people Children and young persons under the age of 18 are not allowed to associate with adult detainees while being detained, conveyed to and from court or waiting to be so conveyed. An exception to this is permitted in accordance with the Children and Young Persons Act 1933 section 31, where the young person is jointly charged with an adult or relative. Officers should make arrangements to prevent association when the child or young person is: detained in a police station being conveyed to or from any criminal court attending court Children and young people should not be carried in a vehicle with adult detainees unless the vehicle has been designed and built to carry them separately and simultaneously. Vehicles that are available for this specific purpose are authorised under the 2011 Prisoner Escort and Custody Services’ contract arrangements. Appropriate adults Forces should establish policies and protocols to provide access to appropriate adults for young persons in police custody. Local YOTs have a statutory responsibility to ensure that an appropriate adult service is provided for children and young people, whether they provide the service themselves or contract a voluntary or private sector agency to do so on their behalf. It is the responsibility of the appropriate adult provider to work with the local force to develop policies and protocols to ensure there is effective provision of appropriate adult services in line with Youth Justice Board (2014) case management guidance. This guidance makes it clear that the appropriate adult service should operate out of hours as well as during standard working hours. All appropriate adults, custody managers, custody officers and staff must be aware of their role as defined by PACE, and also of any agreed local policies, protocols or service level agreements for providing appropriate adults. For further information, see PACE and section 38 of the Crime and Disorder Act 1998. See also the Youth Justice Board’s (YJB) National Standards for Youth Justice Services. The YJB and National Appropriate Adult Network have also published joint guidance and advice on appropriate adult services. When should an appropriate adult be contacted? Detention can be very stressful so it is important that an appropriate adult attends as soon as is practicable to minimise the amount of time the child or young person spends in detention.\n\n\n\n\n\n('Graded as Medium risk, because of the below risk factors: \\n- Jason is a 15 year old adult male, who has gone missing from his care home in Southwark\\n- This is not the first time he has gone missing\\n- He has been seen hanging out with older boys, who may be involved in crime and drugs',\n ' First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review – please see\\xa0APP\\xa0on international investigation\\xa0for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see\\xa0Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child\\xa0 Voice of the child practice briefing\\xa0 Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children’s Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‘streetwise’ and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children\\xa0may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,\\xa0as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to\\xa0work with all the agencies and carers\\xa0that\\xa0have been\\xa0in regular contact with the child as they\\xa0may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\\n\\n###\\n\\nThese include children or young people who: go missing\\xa0– especially on regular occasions from home or care live in a chaotic or dysfunctional family have a history of domestic abuse within the family environment have a history of abuse (including familial\\xa0child sexual abuse, risk of\\xa0forced marriage, risk of\\xa0honour-based violence,\\xa0physical\\xa0and\\xa0emotional\\xa0abuse and\\xa0neglect) have experienced or are experiencing problematic parenting have parents who misuse drugs or alcohol have parents with health problems are young carers within the family unit experience social exclusion as a result of poverty have experienced recent bereavement or loss have unsupervised use of social networking chat rooms/sites have mental ill health have social or learning difficulties have low self-esteem or self-confidence are unsure about their sexual orientation or are unable to confide in their family about their sexual orientation misuse alcohol and/or drugs have been or are excluded from mainstream education are involved in gang activity attend school with other young people who are sexually exploited are friends with individuals who are sexually exploited do not have friends in the same age group are being bullied live in care, foster care, hostels and/or bed and breakfast accommodation – particularly when living out of their home area are homeless have associations with gangs through relatives, peers or intimate relationships live in a gang neighbourhood This is not an exhaustive list, nor have the vulnerabilities been listed in order of importance. Children from loving and secure homes can also be victims of sexual exploitation. The\\xa0characteristics common to all victims are not always their age, ethnicity, disability or sexual orientation, but their powerlessness and vulnerability. Warning signs Despite the increased profile of\\xa0CSE\\xa0and improvements in how the police work with partner agencies,\\xa0CSE\\xa0cases are still under-reported. The Office of the Children’s Commissioner conducted a two-year inquiry into child sexual exploitation in gangs and groups. Their 2013\\xa0report,\\xa0If only someone had listened, highlights that sexually exploited children are not always identified even when they show signs of being victims. Numerous warning signs were identified in the Office of the Children’s Commissioner 2012 interim report,\\xa0I thought I was the only one, the only one in the world, which can indicate that\\xa0a young person is being forced or manipulated into sexual activity and is a victim of sexual exploitation. Practitioners need to be aware of these warning signs and recognise that a victim does not\\xa0have to exhibit all of the warning signs to be a victim of sexual exploitation.\\n\\n###\\n\\nStaff should tell the detainee that she can ask to see the carer at any time. Forces must implement policies and procedures to ensure that all girls who are detained and in custody are under the care of a woman. For further information, see\\xa0PACE Code C, section 3.20A. Transporting children and young people Children and young persons under the age of 18 are not allowed to associate with adult detainees while being detained, conveyed to and from court or waiting to be so conveyed. An exception to this is permitted in accordance with the Children and Young Persons Act 1933 section 31, where the young person is jointly charged with an adult or relative. Officers should make arrangements to prevent association when the child or young person is: detained in a police station being conveyed to or from any criminal court attending court Children and young people should not be carried in a vehicle with adult detainees unless the vehicle has been designed and built to carry them separately and simultaneously. Vehicles that are available for this specific purpose are authorised under the 2011 Prisoner Escort and Custody Services’ contract arrangements. Appropriate adults Forces should establish policies and protocols to provide access to appropriate adults for young persons in police custody. Local YOTs have a statutory responsibility to ensure that an appropriate adult service is provided for children and young people, whether they provide the service themselves or contract a voluntary or private sector agency to do so on their behalf. It is the responsibility of the appropriate adult provider to work with the local force to develop policies and protocols to ensure there is effective provision of appropriate adult services in line with Youth Justice Board (2014) case management guidance. This guidance makes it clear that the appropriate adult service should operate out of hours as well as during standard working hours. All appropriate adults, custody managers, custody officers and staff must be aware of their role as defined by PACE, and also of any agreed local policies, protocols or service level agreements for providing appropriate adults. For further information, see PACE and section 38 of the Crime and Disorder Act 1998. See also the Youth Justice Board’s (YJB) National Standards for Youth Justice Services. The YJB and National Appropriate Adult Network have also published joint guidance and advice on appropriate adult services. When should an appropriate adult be contacted? Detention can be very stressful so it is important that an appropriate adult attends as soon as is practicable to minimise the amount of time the child or young person spends in detention.')\n\n\nIt’s honestly not too bad! while this certainly wouldn’t replace human decision making, it could certainly act as a safeguard against missing a key piece of information at three in the morning when you have a queue of 8 missing people to evaluate, each pages worth of history.\nThere are certainly some pretty major niggles you’d need to fix: for one I’m not sure how the model will perform given actual legislation, rather than “plain English” guidance, nor do I imagine it will cope particularly well with policing specific terminology. The risks it’s identified so far are all mostly obvious, rather than identifying one needle in a giant haystack of intelligence reports. It’s also all going through the OpenAI black-box servers, though I imagine that could be replaced with something open-ish like Llama without too much effort. But when I consider where NLP was even 18 months ago, and just how much computing cognitive power we’ve been able to deploy in only a few hours…who knows where we’ll be next year?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hi!",
    "section": "",
    "text": "I’m Andreas Varotsis, a data scientist and public safety researcher working to improve the use of data and evidence in government.\nI also help coordinate Police Rewired, a community of volunteer professionals with an interest in policing technology, community safety, and fighting crime.\nI try to use empirical approach and multi-disciplinary, quantitative tools to understand, diagnose, and propose solutions to crime and policing questions.\nMy particular areas of interest are:\n\ndata-science and machine learning\ncrime and police activity in physical space\neconometrics and economic modelling\nexperimental and quasi-experimental testing\nrapid technological solutions and innovation\n\nI use this site to showcase my recent work, and discuss new data-science and statistical tools and techniques on my blog.\nIf you’d like to chat, hear more, or absolutely anything else, on Matrix or by email."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Andreas Varotsis",
    "section": "",
    "text": "If you’d like to discuss anything at all, please get in touch! The form below will send me an email, and I’ll get back to you as soon as I can. \n\n\nYour name \n\n\nYour email \n\n\nYour message"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Teaching OpenAI to assess risk, with CopBot!\n\n\n\n\n\n\n\npolicing\n\n\nai\n\n\ndata-science\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWhat’s Happened to Burglary, and does Attending Help?\n\n\n\n\n\n\n\npolicing\n\n\ncrime\n\n\ndata-science\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\nI’ve migrated to Quarto!\n\n\n\n\n\n\n\nnews\n\n\n\n\nI’ve migrated to a fancy new technical writing platform, and I’m a little in love with it\n\n\n\n\n\n\nSep 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning GIS in Python - Robbery and Police Searches in Space\n\n\n\n\n\n\n\ncrime\n\n\ngeospatial\n\n\n\n\nUsing Python GIS libraries to explore the spatial distribution of robberies in London\n\n\n\n\n\n\nMar 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\nLearning R - Exploring the COVID Crime Effect in London\n\n\n\n\n\n\n\ndata-science\n\n\nforecasting\n\n\ncrime\n\n\ngeospatial\n\n\n\n\nI use public London crime data on robbery and burglary to examine where this COVID crime shift was strongest, and whether any specific drivers or correlates can be identified.\n\n\n\n\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLAPD Call Prediction for Fun (and Prophet)\n\n\n\n\n\n\n\ndata-science\n\n\nforecasting\n\n\ncrime\n\n\n\n\nPredicting LAPD police call demand using the Prophet time series forecasting library\n\n\n\n\n\n\nNov 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter Sentiment Analysis with FastAI\n\n\n\n\n\n\n\ndata-science\n\n\nnlp\n\n\n\n\nUsing open source twitter data to explore sentiment in public debate\n\n\n\n\n\n\nSep 10, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recent_work.html",
    "href": "recent_work.html",
    "title": "Recent Work and Publications",
    "section": "",
    "text": "Talks\n\nExamining Policing Use of Force and Disproportionality through Data (Global Evidence Based Policing Conference - October 2022)\nThe importance of a multi-disciplinary approach: How data science helps us understand policing & how policing helps us understand our own data (Society of Evidence Based Policing Conference - May 2021)\n\n\n\nPackages and Libraries\n\nPyDyTuesday (Python) - Automatically download #TidyTuesday datasets\n\n\n\nJournals\n\nIdentifying the Impact of Varying Officer Dosage in Crime Hotspots. Varotsis, A. and T. Davies (2021). In preparation\n\n\n\nArticles (See more at Medium)\n\nAutomating Knife Classification with Machine Learning (April 2021)\nPredicting Police Call Demand for Fun (and Prophet) (Towards Data Science - May 2020)\nLessons from Scraping a DarkNet Market (Towards Data Science - April 2020)\n\n\n\nWork - Professional\n\nMet police gather clues to best practice in fighting crime (Financial Times - October 2020) - A Financial Times article on the work of our team designing and testing an intervention to reduce child re-offending\nDomestic abuse in times of quarantine (LSE Centre for Economic Performance - July 2020) - Our team worked with partners at LSE to produce a data-driven dive into the changes in patterns of domestic crime under lockdown, and design an appropriate intervention. Also available as a blog.\nFighting crime with data and innovation (Behavioural Insights Team Blog - February 2020) - this blog by the SIU Head of Unit summarises some of our recent work and projects"
  }
]