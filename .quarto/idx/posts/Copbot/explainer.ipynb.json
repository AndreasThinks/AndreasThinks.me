{"title":"Teaching OpenAI to assess risk, with CopBot!","markdown":{"yaml":{"title":"Teaching OpenAI to assess risk, with CopBot!","format":{"html":{"code-fold":false,"code-overflow":"wrap"}},"date":"03/19/2023","categories":["policing","ai","data-science"]},"headingText":"Teaching policing to AI","containsRefs":false,"markdown":"\n\nIn case you've been living under a rock and missed all the recent excitement around [ChatGPT](https://chat.openai.com/chat), it works really, really well now... Like, \"oh god that's actual wizardry\" well. The fact it would probably breeze through a [Voight-Kampff test](https://www.youtube.com/watch?v=Umc9ezAyJv0) should probably worry me a bit, but more importantly, what cool stuff can we do with it?\n\nGiven using the tool in development is [actually quite affordable](https://openai.com/pricing), I thought I'd build a few prototypes for public safety use cases, and see how it performs... and after a few hours of work, \"CopBot\" is alive!\n\n![](images/cop_bot.png)\n\nReading, evaluating and prioritising risk is a core policing skill: from investigating crimes with piles of witness statements in dingy offices, to responding to life threatening incidents incidents at 2am on a rainy street, the key thread is you've got loads of risk, and you need to decide what to do first, taking into account decades worth of policing legislation and policy, and making sure your decision is justifiable when it inevitably goes wrong.\n\nSo can an AI learn to \"speak police\" and make vaguely convicing risk assessments? I scraped all the policing guidance I could find, fed it to an OpenAI powered model, and asked it to predict risk for missing people in an explainable way...and it kind of works! **You [can try out the prototype here](https://andreasthinks-police-risk-open-ai-main-y0l65v.streamlit.app/).** \n\nOf course, please don't put any real personal data through it, or rely on it for actual work...it's a weekend experiment, not an actual policing tool. For those who want to get into the detail, the [project code is available here](https://github.com/AndreasThinks/police-risk-open-ai) (built using [Jupyter Notebooks with nbdev](https://nbdev.fast.ai/), which makes it easy to read and deserves it's own blog post), and I thought I'd put together a post to explain the high level principles and document some thoughts. \n\n\n\nYou've probably already played with the [public version of ChatGPT](https://openai.com/blog/chatgpt), and hopefully understand the basic principles: the language model is trained on a huge corpus of publicly available text, aiming to answer questions in a helpful way...but not necessarily an operationally useful one, nor one that takes into account of legislation and best practice.\n \nIf you had all the relevant documentation to hand, and knew exactly which was most relevant, you could just feed it into your prompt - something like \"answer this operation question, but consider this legislative text\" - but how do you do that if you don't know what's relevant? If you want to teach it how to investigate missing people, where do you event start?\n\nThankfully, the [College of Policing](https://www.college.police.uk/) is wonderfully transparent, and shares all their [Authorised Professional Practice](https://www.college.police.uk/app/major-investigation-and-public-protection/missing-persons) in one place (though sadly not through an API). With a clever web crawler, you can quickly collect every page of guidance, as well as every other connected document.\n\nThe next stage is to convert all those documents into [embeddings](https://platform.openai.com/docs/guides/embeddings), turning them from text into numerical of their semantic meaning (according to the model). I've previously done those computations myself, using libraries like [Huggingface](https://huggingface.co/) or [Spacy](https://spacy.io/), but [OpenAI provides all of their embeddings through a quite affordable API you can query](https://platform.openai.com/docs/guides/embeddings).\n\nOnce you've converted all your all our documentation into embeddings, we can quickly calculate the distance between our question and each document, and that tells us which pieces from our corpus of of text is most closely associated to the question we're asking!\n\nUnlike ChatGPT though, we want to limit our model to *only answer questions from that corpus*: if our documents don't contain information about a certain topic, [don't just go and hallucinate a whole new answer](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)). Then it's just a matter of writing our question to extract the most meaningful documents linked to our question, feed them into our prompt, and ask OpenAI to complete the answer, *unless it doesn't know based off the documentation provided*.\n\nSo how does it work?  Well, let's start by asking it some generic questions about policy.\n\nYou can see that as I've enabled debug mode on the function, it will start by printing the relevant documentation it has found (the context), before then giving its answer...which is actually pretty convicing! Let's see what happens if I ask a question it can't know the answer to.\n\nSucess! While it does find a bunch of documentation relating to the current day in our corpus, it does identify that it doesn't know what the day is now.\n\nLet's try something a little bit more technical, and see if it can answer a few questions from the [Sergeants’ and inspectors’ NPPF legal exams](https://www.college.police.uk/career-learning/taking-exams-online/nppf-step-two). I couldn't find any official questions available publicly, [here are two questions from an online guidance service.](https://www.how2become.com/blog/police-sergeants-inspectors-exam/) \n\nThe answer both should have been C, so that's 50/50 for CopBot... not bad! You can see it's referring to to relevant guidance, but sadly that doesn't really help you pass a promotion exam (though I suspect it would do seriously well trained on a bank of questions instead).\n\n## So does it work?\n\nBefore we test our model on fictional missing scenarios, I made one last tweak: I've amended the prompt to explictly refer to identified risk factors, and return them in a given format - you can see how it works below.\n\nIt's honestly not too bad! while this certainly wouldn't replace human decision making, it could certainly act as a safeguard against missing a key piece of information at three in the morning when you have a queue of 8 missing people to evaluate, each pages worth of history.\n\nThere are certainly some pretty major niggles you'd need to fix: for one I'm not sure how the model will perform given actual legislation, rather than \"plain English\" guidance, nor do I imagine it will cope particularly well with policing specific terminology. The risks it's identified so far are all mostly obvious, rather than identifying one needle in a giant haystack of intelligence reports. It's also all going through the OpenAI black-box servers, though I imagine that could be replaced with something open-ish like [Llama](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) without too much effort. But when I consider where NLP was even 18 months ago, and just how much computing cognitive power we've been able to deploy in only a few hours...who knows where we'll be next year?\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"explainer.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.78","theme":"cyborg","title-block-banner":true,"title":"Teaching OpenAI to assess risk, with CopBot!","date":"03/19/2023","categories":["policing","ai","data-science"]},"extensions":{"book":{"multiFile":true}}}}}