{"title":"Learning R - Exploring the COVID Crime Effect in London","markdown":{"yaml":{"title":"Learning R - Exploring the COVID Crime Effect in London","description":" I use public London crime data on robbery and burglary to examine where this COVID crime shift was strongest, and whether any specific drivers or correlates can be identified.  ","date":"5/22/2021","categories":["data-science","forecasting","crime","geospatial"]},"headingText":"Resources I've used","containsRefs":false,"markdown":"\n\n\nThe lockdown and social distancing measures that were brought in throughout the world to tackle COVID in 2020 have had a significant, widespread effect on crime. In this notebook, I use public London crime data on robbery and burglary to examine where this \"COVID crime shift\" was strongest, and whether any specific drivers or correlates can be identified. I use three years of Metropolitan Police Service data from [data.police.uk.](https://data.police.uk/)\n\n\nThe findings suggest that the relative change in burglary and robbery in April and May 2020 was heavily affected by local characteristics: areas with a high residential population saw the sharpest decreases in burglary (likely due to a reduction in available targets) while the reduction in robberies instead seem to be driven by geographic features and  indicators of deprivation (potentially suggesting more available targets for robbery in communities least able to work for from home).\n\nThe primary purpose of this exercise was to learn R - I've previously worked entirely in Python, which is more than sufficient 99% of the time, but has at times proved a blocker when I want to tackle some more experimental geospatial and statistical methods.  With that in mind, this is likely to be a little messy, and I'll aim to condense my main lessons into a blog post in the future. The models are not heavily tuned (aiming to explore correlates rather than provide accurate predictions) and there are likely to be correlation between our various predictors - as such these should not be taken to suggest direct causation.\n\nThe full code and data for this exercise are available on my [Github repo](https://github.com/crimsoneer/Covid-Crime-Shift). I'm hoping to summarise my key lessons in the Python to R journey in [Medium post](https://medium.com/@andreas.varotsis) in the next few weeks.\n\n\n\n- Matt Ashby Crime Mapping course: https://github.com/mpjashby/crimemapping/\n- Spatial Modelling for Data Scientists: https://gdsl-ul.github.io/san/\n- R for Data Science: https://r4ds.had.co.nz/index.html\n- Geocomputation with R: https://geocompr.robinlovelace.net/\n\n\n\n\n### Tasks\n\n1. Ingest Data \n2. Predict trend by MSOA\n3. Quantify MSOA COVID Effect\n4. Model\n\n## Ingest Data\nFor this exercise, I'll be importing crime and robbery data by MSOA.[MSOAs are geographical units specifically designed for analysis, and to be comparable: they all have an average population of just over 8,000.](https://www.ons.gov.uk/methodology/geography/ukgeographies/censusgeography#output-area-oa) There is a compromise here between smaller geographical units (that create more variance that may help us identify predictors), but the necessity for enough crime per unit to identify meaningful trends - MSOAs should be suitable.\n\n```{r, results='hide', collapse=TRUE, warning=FALSE, cache=TRUE, echo=FALSE}\n# Data manipulation, transformation and visualisation\nlibrary(tidyverse)\n# Nice tables\nlibrary(kableExtra)\n# Simple features (a standardised way to encode vector data ie. points, lines, polygons)\nlibrary(sf) \n# Spatial objects conversion\nlibrary(sp) \n# Thematic maps\nlibrary(tmap) \n# Colour palettes\nlibrary(RColorBrewer) \n# More colour palettes\nlibrary(viridis)\n#ggplot organiastion\nlibrary(ggpubr)\nlibrary(raster)  # raster data\nlibrary(rgdal)  # input/output, projections\nlibrary(rgeos)  # geometry ops\nlibrary(spdep)  # spatial dependence\nlibrary(lubridate) #date and time\n#random forest and metrics\nlibrary(Metrics)\nlibrary(caret)\nlibrary(randomForest)\nrequire(caTools)\nlibrary(DALEX)\nlibrary(dplyr)\n#correlation matrix\nlibrary(corrr)\n\noptions(warn=-1)\n\n\n```\n\nTo build our process, we'll start by taking one month of crime data, exploring it, and writing all our steps for automation.\n\n```{r, warning=FALSE, cache=TRUE}\ntest_df <- read.csv(\"crimes/2018-01/2018-01-metropolitan-street.csv\")\n```\nOur crime data is categorised according to the Home Office major crime types, and like Python, we can list them all through the \"unique\" function. Here I'll be focusing on robbery and burglary: two crime types that are heavily reliant on encountering victim's in public spaces, and as such should be affected by the \"COVID effect\".\n\nTo avoid this getting particularly computationally intensive, let's write a function to pull out robberies and burglaries, and assign them a specific MSOA. Then we can iterate over all our months and get monthly counts for each offence type.\n\n```{r, warning=FALSE, cache=TRUE}\nsubset_df <- filter(test_df, Crime.type==\"Burglary\" | Crime.type==\"Robbery\")\nhead(subset_df)\n```\nOur single month of data contains 10,501 crimes.\n\nWe now need to link this to our spatial data. We use the MSOA borders provided by MOPAC, and use the UK National Grid coordinate system. Police.uk does not use that system, so we'll need to reproject our crime data.\n\n```{r, warning=FALSE, cache=TRUE}\nlsoa_borders <- st_read(\"msoa_borders/MSOA_2011_London_gen_MHW.tab\", crs=27700)\nplot(lsoa_borders)\n```\nBefore we can link our crimes to MSOA, we'll need to ensure identical coordinate systems, and remove any non-geolocated values we'll need to erase any missing values (while checking we retain enough data for analysis.)\n\n\n```{r , warning=FALSE, cache=TRUE}\n#count missing values in the longitude column\nprint(\"Missing values identified:\")\nsum(is.na(subset_df[\"Longitude\"]))\n```\n\nThankfully, we only identify 82 crimes which we need to remove, leaving plenty for analysis.\n\n```{r , warning=FALSE, cache=TRUE}\nclean_df <- subset_df[!rowSums(is.na(subset_df[\"Longitude\"])), ]\n```\nWe can now convert our crime data to spacial data, using our longitude and latitude coordinates - this allows us to quickly plot our data, and confirm it looks right.\n\n```{r , warning=FALSE, cache=TRUE, echo=FALSE}\n\nsubset_spatial <- st_as_sf(clean_df, coords = c(\"Longitude\", \"Latitude\"), \n                      crs = 4326, remove = FALSE)\n```\n\n\n```{r, cache=TRUE, echo=FALSE}\nplot(subset_spatial)\n```\nWith our data now mapped, we ensure everything is aligned to the appropriate coordinate system, and assign each crime to an MSOA from our data - the data is then aggregated into a monthly MSOA crime count, to which we assign our monthly date.\n\n\n```{r, cache=TRUE, echo=FALSE}\n\nlatlong = \"+init=epsg:4326\"\nukgrid = \"+init=epsg:27700\"\n\n#transform our spatial data to Easting and Northing coordinates\nsubset_osgb <- st_transform(subset_spatial, ukgrid)\n\n#use a spatial join to link them to MSOAs.\ncrime_with_msoa <- st_join(subset_osgb, lsoa_borders[\"MSOA11CD\"])\n\n#pivot the data to provide a count by crime and msoa.\nmsoa_list<- crime_with_msoa %>%\n  group_by(MSOA11CD, Crime.type) %>%\n  summarize(count_by_msoa = n())\n\nmsoa_pivot_tibble <- as_tibble(msoa_list)\nmsoa_pivot_tibble <- msoa_pivot_tibble[0:3]\n\n\n#creating a df with all msoa names, for robbery and burglary\nmsoa_zero_df_robbery <- unique(as_tibble(lsoa_borders)[\"MSOA11CD\"])\nmsoa_zero_df_burglary <- unique(as_tibble(lsoa_borders)[\"MSOA11CD\"])\n\n#adding our crime type column \nmsoa_zero_df_burglary[\"Crime.type\"] = \"Burglary\"\nmsoa_zero_df_robbery[\"Crime.type\"] = \"Robbery\"\n\n#Creating a \"count\" column identical to our pivot, and filling it with 0\nmsoa_zero_df_burglary[\"count_by_msoa\"] = as.numeric(0)\nmsoa_zero_df_robbery[\"count_by_msoa\"] = as.numeric(0)\n\n#combine both\nduplicate_concat <- rbind(msoa_zero_df_robbery, msoa_zero_df_burglary)\n\n#add our duplicates to our original table\ndf_with_dups <- rbind(msoa_pivot_tibble, duplicate_concat)\n\n\n#creating a filter for duplicates columns, which should ignore the first instance\ndup_filters <- duplicated(df_with_dups[0:2])\n\n\n#bring it all back together\nmonthly_df <- filter(df_with_dups, !dup_filters)\n\n#select the first unique value of months in the original dataframe\nmonth <- unique(test_df[\"Month\"])[1,1]\nmonthly_df[\"Month\"] <- month\nhead(monthly_df)\n\n```\nBringing together all the code so far into a function, we can create an pipeline to generate our crime count per MSOA time series for the entirety of our dataset.\n\n```{r , cache=TRUE, echo=FALSE}\n#quick initial function to generate our MSOA borde spatial frame, to avoid it sitting in the initial frame and gobbling loads of memory.\ngenerate_msoa_borders <- function(file){\n  msoa_borders <- st_read(file, crs=27700)\n  return(msoa_borders)\n}\n\nmake_month_pivot <- function(file){\n  #define our CRS\n  latlong = \"+init=epsg:4326\"\n  ukgrid = \"+init=epsg:27700\"\n  #read our crime from the file\n  test_df <- read.csv(file)\n  #select only our target crime types\n  subset_df <- filter(test_df, Crime.type==\"Burglary\" | Crime.type==\"Robbery\")\n  #remove any rows with a long/lat coordinate\n  clean_df <- subset_df[!rowSums(is.na(subset_df[\"Longitude\"])), ]\n  #generate a spatial df\n  subset_spatial <- st_as_sf(clean_df, coords = c(\"Longitude\", \"Latitude\"), \n                      crs = 4326, remove = FALSE)\n  #reproject to uk grid coords\n  subset_osgb <- st_transform(subset_spatial, ukgrid)\n  #spatially join to assign to an MSOA\n  crime_with_msoa <- st_join(subset_osgb, msoa_borders[\"MSOA11CD\"])\n  #summarise by count of MSOA\n  msoa_list<- crime_with_msoa %>%\n    group_by(MSOA11CD, Crime.type) %>%\n    summarize(count_by_msoa = n())\n  #return to a non-geographic msoa\n  msoa_pivot_tibble <- as_tibble(msoa_list)\n  msoa_pivot_tibble <- msoa_pivot_tibble[0:3]\n  #creating a df with all msoa names, for robbery and burglary\n  msoa_zero_df_robbery <- unique(as_tibble(msoa_borders)[\"MSOA11CD\"])\n  msoa_zero_df_burglary <- unique(as_tibble(msoa_borders)[\"MSOA11CD\"])\n  #adding our crime type column \n  msoa_zero_df_burglary[\"Crime.type\"] = \"Burglary\"\n  msoa_zero_df_robbery[\"Crime.type\"] = \"Robbery\"\n  #Creating a \"count\" column identical to our pivot, and filling it with 0\n  msoa_zero_df_burglary[\"count_by_msoa\"] = as.numeric(0)\n  msoa_zero_df_robbery[\"count_by_msoa\"] = as.numeric(0)\n  duplicate_concat <- rbind(msoa_zero_df_robbery, msoa_zero_df_burglary)\n  df_with_dups <- rbind(msoa_pivot_tibble, duplicate_concat)\n  #creating a filter for duplicates columns, which should ignore the first instance\n  dup_filters <- duplicated(df_with_dups[0:2])\n  monthly_df <- filter(df_with_dups, !dup_filters)\n  #re-add our month column\n  month <- unique(test_df[\"Month\"])[1,1]\n  monthly_df[\"Month\"] <- month\n  return(monthly_df)\n}\n```\n\nFor this project, I haven't used the Police.uk API (which would have enabled me to automate  the downloads and query the data directly) - as such, we have to iterate over our subfolders, ingesting our CSV data and running through our process.\n\n```{r, cache=TRUE, warning=FALSE, echo=FALSE, results=\"hide\"}\n#create empty dataframe to bring together our data\nempty_df <- tibble(\nMSOA11CD = \"\", \nCrime.type= \"\",\ncount_by_msoa= \"\",\nMonth= \"\"\n)\n\n#re-ingest our MSOA data\nmsoa_borders <- generate_msoa_borders(\"msoa_borders/MSOA_2011_London_gen_MHW.tab\")\n\n#recursive argument ensures we also explore subfolders.\nsubfiles <- list.files(path = \"crimes\", recursive=T)\n\nfor (file in subfiles){\n  folder_subdir <- \"crimes/\"\n  #concatenate to get our total subfolder directory - hacky but will work here.\n  sub_path <- paste(folder_subdir, file, sep=\"\")\n  monthly_df <- make_month_pivot(sub_path)\n  empty_df <- rbind(empty_df, monthly_df)\n}\n\n```\n\n\n```{r, cache=TRUE,  ,warning=FALSE, echo=FALSE}\nhead(empty_df)\n```\nWe now have a combined dataframe of 71,848 rows, from January 2018 through December 2020.\n\n\n```{r, results='hide', collapse=TRUE, warning=FALSE, cache=TRUE}\n#saving file to CSV\n#write.csv(empty_df,\"msoa_crime_matrix.csv\")\n\n```\n\n## 2. Predict trend by MSOA\n#### Visualisation and Exploration\nWith our data now cleaned and aggregated, we can focus on the more interesting part - forecasting our \"expected\" pandemic crime, and examining how much it diverges from our \"actual\" crime.\n\n```{r , cache=TRUE}\nempty_df <- read.csv(\"msoa_crime_matrix.csv\")\nempty_df <- empty_df[2:70848,2:5]\nhead(empty_df)\n```\n\n\nBefore going any further, let's use this to explore and visualise the distribution of robbery and burglary across time and space during our \"pre-pandemic\" period, in March 2020 - based on London mobility indicators, this is when movement accross London began to be heavily affected, and the disruption was most notable in April\n\n![London mobility data](london_mobility.png)\n\n```{r , cache=TRUE}\nburglary_df<-empty_df\n\n#add a \"1\" so our month can be converted to a full date\nburglary_df$DateString <- paste(burglary_df$Month, \"-01\", sep=\"\")\n\n#convert to date format\nburglary_df$DateClean <- ymd(burglary_df$DateString)\n\n#filter out only burglary prior to the pandemic\nburglaryExplore <- filter(burglary_df,  DateClean < \"2020-03-01\" & Crime.type==\"Burglary\")\n\nhead(burglaryExplore)\n```\n\n\nLooking at the aggregate counts of burglary across London, a visual observation suggests yearly trends (which we'll have to consider in our forecast), which sharp peaks during the Winter months and the lowest numbers in summer (when the days are longest).\n```{r , cache=TRUE}\n#group burglary count by months and plot\nburglary_by_month <- burglaryExplore %>%\n  group_by(DateClean) %>%\n  summarize(total_burglaries = sum(count_by_msoa))\n\nggplot(burglary_by_month, aes(x=DateClean, y=total_burglaries)) +\n  geom_line()\n```\n\nTo observe how crime counts are distributed in space, let's map both counts by MSOA. As  previously mentioned, MSOAs are designed to be comparable units, at least from a population perspective - we don't need to produce per population rates. \n\n\n```{r, fig.width = 13 , cache=TRUE}\nburglary_by_msoa <- burglaryExplore %>%\n  group_by(MSOA11CD) %>%\n  summarize(total_burglaries = sum(count_by_msoa))\n\n#we join our burglary counts to their geographic msoa\nburglary_map <- left_join(lsoa_borders, burglary_by_msoa, by = \"MSOA11CD\")\n\n#user brewer colour palette https://colorbrewer2.org\npal <- brewer.pal(5,\"BuGn\")\n\n#create our map, and add the layout options\nburglary_map <-tm_shape(burglary_map) +\n  tm_fill(col = \"total_burglaries\", title = \"Total Burglary Count by MSOA\", style=\"quantile\", palette=\"BuGn\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\nrobbery_df<-empty_df\n\nrobbery_df$DateString <- paste(robbery_df$Month, \"-01\", sep=\"\")\nrobbery_df$DateClean <- ymd(robbery_df$DateString)\nrobberyExplore <- filter(robbery_df,  DateClean < \"2020-03-01\" & Crime.type==\"Robbery\")\n\nrobbery_by_msoa <- robberyExplore %>%\n  group_by(MSOA11CD) %>%\n  summarize(total_robberies = sum(count_by_msoa))\n\nrobbery_map <- left_join(lsoa_borders, robbery_by_msoa, by = \"MSOA11CD\")\n\npal <- brewer.pal(5,\"BuGn\")\n\n\nrobbery_map <-tm_shape(robbery_map) +\n  tm_fill(col = \"total_robberies\", title = \"Total Robbery Count by MSOA\", style=\"quantile\", palette=\"BuGn\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\n#arrange the maps together\ntmap_arrange(burglary_map, robbery_map, nrow = 2)\n```\n\nWe notice that robbery is noticeably more concentrated in central London, with burglary remaining quite common across the city. That said, there are also obvious spatial patterns here - these crimes are clustered in certain geographies. \n\n#### Modelling \nWe can now begin the forecasting process. To design our process, we'll start by focusing on a single MSOA - the first in our dataset, [E02000001, or the City of London.](https://findthatpostcode.uk/areas/E02000001.html)\n\n```{r , cache=TRUE}\nsingle_msoa_df <- filter(empty_df, MSOA11CD == \"E02000001\" & Crime.type==\"Burglary\")\n\n#we add a 01 to our date to ensure R recognises the date format\nsingle_msoa_df$DateString <- paste(single_msoa_df$Month, \"-01\")\n\n\nsingle_msoa_df$DateClean <- ymd(single_msoa_df$DateString)\nsingle_msoa_df\n```\nFrom a forecasting/time-series perspective, this is a very small dataset - 36 monthly observations. We will be shrinking this further to only 26 by focusing on data prior to March 2020, when the COVID crime impact is felt. This significantly limits our forecasting options, and will impact accuracy, if we treat each MSOA in isolation - we could explore some sort of Vector Autoregressive Model to limit this, but given that we're then going to be exploring the error of all our models in aggregation, this isn't crucial. Our focus is on models that we can accurately deploy without needing to tune each of them individually, and that can capture the seasonal trend, and generate reliable predictions on our limited dataset. \n\nGiven these limitations, I've opted for [the Prophet algorith.](https://facebook.github.io/prophet/) While it's more opaque than a auto-arima or VAR model, it works well with monthly data, and extracting seasonal trends. It also requires very little tuning.\n\nAs such, we'll extract our \"training set\" prior to March, and start forecasting.\n\n```{r , cache=TRUE}\n\ntraining_set <- filter(single_msoa_df, DateClean < \"2020-03-01\")\n\ntraining_df <- tibble(\n  ds=training_set$DateClean,\n  y=training_set$count_by_msoa\n)\nhead(training_df)\n```\n\n```{r, results='hide' , cache=TRUE}\nlibrary(prophet)\nm <- prophet(training_df)\n\n```\nFor now, we'll forecast on a 6 month horizon - we obviously wouldn't expect it to be accurate that far into the future.\n\n```{r , cache=TRUE}\n#prophet generates a future dataframe using our data, for 6 mperiods\nfuture <- make_future_dataframe(m, periods = 6, freq = 'month')\n\n\nforecast <- predict(m, future)\n\nplot(m, forecast)\n\n```\nAs we can see, the model seems consistent on a short horizon, and gets very wide as it goes further into the future. More importantly however, it has extracted a yearly seasonal compontent - the summer decrease we identified previously - as well as a long term trend.  \n\n```{r, cache=TRUE,  ,warning=FALSE}\n\nprophet_plot_components(m, forecast)\n```\nThese predictions seem far-fetched, but remember we will be observing a London wide error rate. As such, we must now isolate our \"pandemic period\" - which we define as April and May 2020 - and compare the predicted crime counts to the actual crime counts to obtain a metric of our \"COVID crime shift\", or our error rate.\n\n```{r, cache=TRUE, warning=FALSE}\n\n\nforecast$Month <- month(forecast$ds)\nforecast$Year <- year(forecast$ds)\n\n\nthis_year <- filter(forecast, Year > 2019)\npeak_pandemic <- filter(this_year, Month== 4 | Month== 5 )\n\npredictionPivot <- peak_pandemic %>%\n  group_by(Month) %>%\n  summarize(predicted_burglary = mean(yhat))\n\n\nsingle_msoa_df$MonthNum <- month(single_msoa_df$DateClean)\nsingle_msoa_df$YearNum <- year(single_msoa_df$DateClean)\n\nthis_year_actual <- filter(single_msoa_df, YearNum > 2019)\npeak_pandemic_actual <- filter(this_year_actual, MonthNum== 4 | MonthNum== 5 )\n\nactual_burglary <- sum(peak_pandemic_actual$count_by_msoa)\npred_burglary <- sum(predictionPivot$predicted_burglary)\n\nerror <- actual_burglary - pred_burglary\npercentage_error <- error / pred_burglary \n\nprint(\"Burglary Count\")\nprint(actual_burglary)\nprint(\"Predicted\")\nprint(pred_burglary)\n\nprint(\"Actual Error\")\nprint(error)\nprint(\"Percentage Error\")\nprint(percentage_error)\n```\nIn this MSOA, our model predicted nearly 8 burglaries would occur in these two months, based on pre-pandemic trends. In reality, 1 took place - a large error rate, suggesting a strong \"COVID effect\".\n\nThis process can now be replicated for every MSOA in London, to obtain this metric for each MSOA.\n\n```{r}\nlength(unique(empty_df$MSOA11CD))\n\n```\n\n\n\n```{r, collapse=TRUE, warning=FALSE, cache=TRUE, echo=FALSE,message=FALSE}\n\n\nmsoa_error_tibble <- tibble(\nMSOA11CD = \"\", \nburglaryActual= \"\",\nburglaryPredicted= \"\",\nburglaryError= \"\",\nburglaryPercentError=\"\",\nrobberyActual= \"\",\nrobberyPredicted= \"\",\nrobberyError= \"\",\nrobberyPercentError=\"\"\n)\n\ncalculate_error <- function(msoaName){\n  #select only burglary and our msoa\n  single_msoa_df <- filter(empty_df, MSOA11CD == msoaName & Crime.type==\"Burglary\")\n  #clean date date\n  single_msoa_df$DateString <- paste(single_msoa_df$Month, \"-01\")\n  single_msoa_df$DateClean <- ymd(single_msoa_df$DateString)\n  #generate training set up until March\n  training_set <- filter(single_msoa_df, DateClean < \"2020-03-01\")\n  #prepare for Prophet\n  training_df <- tibble(\n    ds=training_set$DateClean,\n    y=training_set$count_by_msoa)\n  #start and predict prophet for 6 months\n  m <- prophet(training_df)\n  future <- make_future_dataframe(m, periods = 6, freq = 'month')\n  forecast <- predict(m, future)\n  forecast$Month <- month(forecast$ds)\n  forecast$Year <- year(forecast$ds)\n  #aggregate forecasts and actual crime\n  this_year <- filter(forecast, Year > 2019)\n  peak_pandemic <- filter(this_year, Month== 4 | Month== 5 )\n  predictionPivot <- peak_pandemic %>%\n    group_by(Month) %>%\n    summarize(predicted_burglary = mean(yhat))\n\n  single_msoa_df$MonthNum <- month(single_msoa_df$DateClean)\n  single_msoa_df$YearNum <- year(single_msoa_df$DateClean)\n  #generate error rates\n  this_year_actual <- filter(single_msoa_df, YearNum > 2019)\n  peak_pandemic_actual <- filter(this_year_actual, MonthNum== 4 | MonthNum== 5 )\n  actual_burglary <- sum(peak_pandemic_actual$count_by_msoa)\n  pred_burglary <- sum(predictionPivot$predicted_burglary)\n  error_burg <- actual_burglary - pred_burglary\n  percentage_error_burg <- error_burg / pred_burglary \n  \n  #now repeat for robbery\n  \n  single_msoa_df <- filter(empty_df, MSOA11CD == msoaName & Crime.type==\"Robbery\")\n  single_msoa_df$DateString <- paste(single_msoa_df$Month, \"-01\")\n  single_msoa_df$DateClean <- ymd(single_msoa_df$DateString)\n  training_set <- filter(single_msoa_df, DateClean < \"2020-03-01\")\n  training_df <- tibble(\n    ds=training_set$DateClean,\n    y=training_set$count_by_msoa)\n  m <- prophet(training_df)\n  future <- make_future_dataframe(m, periods = 6, freq = 'month')\n  forecast <- predict(m, future)\n  forecast$Month <- month(forecast$ds)\n  forecast$Year <- year(forecast$ds)\n  this_year <- filter(forecast, Year > 2019)\n  peak_pandemic <- filter(this_year, Month== 4 | Month== 5 )\n  predictionPivot <- peak_pandemic %>%\n    group_by(Month) %>%\n    summarize(predicted_burglary = mean(yhat))\n\n  single_msoa_df$MonthNum <- month(single_msoa_df$DateClean)\n  single_msoa_df$YearNum <- year(single_msoa_df$DateClean)\n\n  this_year_actual <- filter(single_msoa_df, YearNum > 2019)\n  peak_pandemic_actual <- filter(this_year_actual, MonthNum== 4 | MonthNum== 5 )\n  actual_robbery <- sum(peak_pandemic_actual$count_by_msoa)\n  pred_robbery <- sum(predictionPivot$predicted_burglary)\n  error_rob <- actual_robbery - pred_robbery\n  percentage_error_rob <- error_rob / pred_robbery \n  \n  #create our output dataframe and return it\n  \n  msoa_error_tibble <- tibble(\n    MSOA11CD = msoaName, \n    burglaryActual= actual_burglary,\n    burglaryPredicted= pred_burglary,\n    burglaryError= error_burg,\n    burglaryPercentError = percentage_error_burg,\n    robberyActual= actual_robbery,\n    robberyPredicted= pred_robbery,\n    robberyError= error_rob,\n    robberyPercentError=percentage_error_rob\n    )\n\n  return(msoa_error_tibble)\n}\n\n\ntryCatch(\n    expr = {\n      for (msoa in unique(empty_df$MSOA11CD)){\n        iterated_msoa_df <- calculate_error(msoa)\n        msoa_error_tibble <- rbind(msoa_error_tibble, iterated_msoa_df)\n  \n}\n    },\n    error = function(e){ \n        # (Optional)\n        # Do this if an error is caught...\n    },\n    warning = function(w){\n        # (Optional)\n        # Do this if an warning is caught...\n    },\n    finally = {\n        # (Optional)\n        # Do this at the end before quitting the tryCatch structure...\n    }\n)\n\n\n\n\n```\n\n\n\n\n```{r , cache=TRUE, warning=FALSE}\nhead(msoa_error_tibble)\n```\n\nOur process has completed: we have a \"COVID shift\" measure for all of London.\n\n## 3. Measuring Local COVID Crime Shifts\n\nWe now need to use our forecasts to measure the \"error\" - this should provide an indication of the \"COVID Crime Shift\", or how much the actual crime diverted from the previous forecasts.\n\nI explored various avenues for this: the ideal solution would be a relative rate of the error, as MSOAs with large crime numbers will likely generate large errors, and so a rate would be ideal, though this is complicated by our erratic prediction and mix of positive and negative numbers.\n\nOur final solution has explored two options:\n- the absolute error number\n- the relative error once the crime and predictions have been transformed (by adding 50)\n\n$$\nactual_{k} = actual + 50\n$$\n\n$$\npredicted_{k} = predicted + 50\n$$\n\n$$\nRPD = \\frac{(actual_{k} - predicted_{k})}  {(actual_{k} + predicted_{k})/2}\n$$\n\nWe visualise and describe these statistics first to ensure they appear sensible.\n\n\n```{r ,  results='hide', collapse=TRUE, warning=FALSE, cache=TRUE, echo=FALSE}\n#write_csv(msoa_error_tibble, \"msoa_error_table2.csv\")\n```\n\n```{r , cache=TRUE, warning=FALSE}\nmsoa_error_tibble <- read_csv(\"msoa_error_table2.csv\")\n\nmsoa_error_tibble[,2:9] <- lapply(msoa_error_tibble[,2:9], as.numeric)\n\nmsoa_error_tibble <- msoa_error_tibble[2:980, ]\n\nmsoa_error_tibble <- left_join(msoa_error_tibble, robbery_by_msoa, by = \"MSOA11CD\")\nmsoa_error_tibble <- left_join(msoa_error_tibble, burglary_by_msoa, by = \"MSOA11CD\")\n\n\n```\n\n\n```{r, cache=TRUE, warning=FALSE}\n\nmsoa_error_tibble$RPDBurglary <- (msoa_error_tibble$burglaryActual - msoa_error_tibble$burglaryPredicted)/((msoa_error_tibble$burglaryPredicted + msoa_error_tibble$burglaryActual)/2)\n\nmsoa_error_tibble$RPDRobbery <- (msoa_error_tibble$robberyActual - msoa_error_tibble$robberyPredicted)/((msoa_error_tibble$robberyPredicted + msoa_error_tibble$robberyActual)/2)\n\nmsoa_error_tibble$robberyActualShifted <- msoa_error_tibble$robberyActual + 50\nmsoa_error_tibble$robberyPredictedShifted <- msoa_error_tibble$robberyPredicted + 50\n\n\nmsoa_error_tibble$RPDRobberyShifted <- (msoa_error_tibble$robberyActualShifted - msoa_error_tibble$robberyPredictedShifted)/((msoa_error_tibble$robberyPredictedShifted + msoa_error_tibble$robberyActualShifted)/2)\n\nmsoa_error_tibble$burglaryActualShifted <- msoa_error_tibble$burglaryActual + 50\nmsoa_error_tibble$burglaryPredictedShifted <- msoa_error_tibble$burglaryPredicted + 50\n\n\nmsoa_error_tibble$RPDburglaryShifted <- (msoa_error_tibble$burglaryActualShifted - msoa_error_tibble$burglaryPredictedShifted)/((msoa_error_tibble$burglaryPredictedShifted + msoa_error_tibble$burglaryActualShifted)/2)\n\n\n```\n\n```{r, cache=TRUE, warning=FALSE}\nprint(\"Burglary Error\")\nsummary(msoa_error_tibble$burglaryError)\nprint(\"Burglary Relative Error\")\nsummary(msoa_error_tibble$RPDburglaryShifted)\nprint(\"Robbery Error\")\nsummary(msoa_error_tibble$robberyError)\nprint(\"Robbery Relative Error\")\nsummary(msoa_error_tibble$RPDRobberyShifted)\n\n```\nAs we can see, the average London MSOA experienced a negative COVID crime shift for both burglary and robbery, but this is far from equally distributed - at the extremes, some areas actually see large increases on our predicted values. \n\n```{r , fig.width = 13 , cache=TRUE, warning=FALSE}\n\nburg_hist <- ggplot(msoa_error_tibble, aes(x=burglaryError)) + geom_histogram()\nrob_hist <-ggplot(msoa_error_tibble, aes(x=robberyError)) + geom_histogram()\nburg_r_hist <- ggplot(msoa_error_tibble, aes(x=RPDburglaryShifted)) + geom_histogram()\nrob_r_hist <- ggplot(msoa_error_tibble, aes(x=RPDRobberyShifted)) + geom_histogram()\nscatter <- ggplot(msoa_error_tibble, aes(x = RPDRobberyShifted, y = RPDburglaryShifted)) +\n  geom_point()\n\nr_scatter <- ggplot(msoa_error_tibble, aes(x = robberyError, y = burglaryError)) +\n  geom_point()\n\nggarrange(rob_hist, burg_hist, rob_r_hist, burg_r_hist,scatter, r_scatter, ncol=2, nrow=3 )\n\n\n```\nOur shifted relative error rate seems to function as intended: while there are still outliers, they are more concentrated than they are for the pure error term, and the overall distribution is more focused, while still indicating the direction and relative strength of our COVID effect.\n\nLet's map this effect visually, and see if any particular areas stand out.\n\n```{r , fig.width = 13, cache=TRUE, warning=FALSE}\n#re-ingest our geographic MSOA borders\nmsoa_borders <- st_read(\"msoa_borders/MSOA_2011_London_gen_MHW.tab\", crs=27700)\n\ngeographic_error_map <- left_join(msoa_borders, msoa_error_tibble, by = \"MSOA11CD\")\n\nburg_map <- tm_shape(geographic_error_map) +\n  tm_fill(col = \"robberyError\", title = \"Robbery Error\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\nrob_map <-tm_shape(geographic_error_map) +\n  tm_fill(col = \"burglaryError\", title = \"Burglary  Error\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\nburg_map_rate <- tm_shape(geographic_error_map) +\n  tm_fill(col = \"RPDRobberyShifted\", title = \"Robbery Error Relative\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\nrob_map_rate <-tm_shape(geographic_error_map) +\n  tm_fill(col = \"RPDburglaryShifted\", title = \"Burglary  Error Relative\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\ntmap_arrange(burg_map, rob_map, burg_map_rate, rob_map_rate , nrow = 2, ncol=2)\n\n```\nIt's hard to identify any obvious effect visually, but we do notice that while central London sees some very strong reductions, it also sees some increases.  Conversely, the outskirts of London (notably to the south and West) are a near continuous area of large decreases. The effect does vary by offence type, but the pattern seen in South and West London appears broadly consistent.\n\n## Identifying Correlates and Modelling\n\nWe've identified that the COVID crime effect was felt unequally accross London, and varies by offence type. To finalise our project, we will be linking our data to [demographic data provided by MOPAC](https://data.london.gov.uk/dataset/msoa-atlas), and aiming to use it to identify correlates to our \"covid shift\", and hopefully build models disentangling the effect.\n\n```{r , cache=TRUE, warning=FALSE}\nlibrary(readxl)\n#ingest ATLAS\nmsoa_atlas <- read_excel(\"msoa_atlas/msoa-data.xls\")\n\n#join by MSOA\ngeographic_msoa_matrix <- left_join(geographic_error_map, msoa_atlas, by = \"MSOA11CD\")\n\n#convert to tibble\nmsoa_matrix_tbl <- as_tibble(geographic_msoa_matrix)\nwrite_csv(msoa_matrix_tbl, \"msoa_matrix.csv\")\n\n\n#select only numeric data\nmsoa_matrix_numeric <-dplyr::select_if(msoa_matrix_tbl, is.numeric)\nhead(msoa_matrix_numeric)\n```\nHaving now ingested and linked our data, we begin by exploring the factors most highly correlated with our relative error rates.\n\n```{r, results='hide', cache=TRUE, warning=FALSE}\n\ncorr_df <- correlate(dplyr::select_if(msoa_matrix_tbl, is.numeric), quiet = TRUE)\n\noptions(scipen=999)\n```\nStarting by our relative robbery error, a few interesting correlates stand out:\n- road traffic casualties\n- burglary numbers\n- the number of dwellings with no usual residents, and the number of commercial residents\n- households with no cars\n- the age composition of the area\n- general deprivation indicators (such as the proportion of households with central heating)\n\n```{r, cache=TRUE, warning=FALSE}\n#show only correlates with an absolute value higher than 0.2\n#filter(dplyr::select(corr_df[order(corr_df$RPDRobberyShifted),] , term, RPDRobberyShifted), RPDRobberyShifted < -0.2 | RPDRobberyShifted > 0.2)\n#filter(dplyr::select(corr_df[order(corr_df$RPDRobberyShifted),] , term, RPDRobberyShifted), RPDRobberyShifted < -0.2 | RPDRobberyShifted > 0.2)\nhigh_corr_rob <- filter(dplyr::select(corr_df, term, RPDRobberyShifted), RPDRobberyShifted < -0.15 | RPDRobberyShifted > 0.15)\n\nhigh_corr_rob[order(high_corr_rob$RPDRobberyShifted),]\n\n```\n\nThe correlations for burglary are weaker - only a few have an absolute value higher than 0.2 - but a few stand out:\n- households with no residents\n- high robbery numbers\n- house prices \n\n```{r, cache=TRUE, warning=FALSE}\n\nhigh_corr_burg <- filter(dplyr::select(corr_df, term, RPDburglaryShifted), RPDburglaryShifted < -0.15 | RPDburglaryShifted > 0.15)\n\nhigh_corr_burg[order(high_corr_burg$RPDburglaryShifted),]\n\n```\n\nThese correlates suggest we can model this shift - this is likely to prove more reliable for robbery (where the correlations are stronger), and seem linked to usual resident population(as measured by household composition), deprivation (through various proxy indicators such as central heating presence or housing type), and general crime patterns (through total burglary and robbery numbers)\n\nWe will take two approaches for modelling: a simple regression (to identify strong links) and random forest regressors (to identify non-linear associations)\n\n### Simple Regression\n\nWe begin through the use of simple OLS regression. This is a linear model that has large limitations for modelling complex relationships, but can be an effective first step, effectively with a few transformations.\n\nR does not cope well with blank spaces in terms, so we'll extract and rename our key correlates.\n\n```{r, cache=TRUE, warning=FALSE}\n\n#make copy of df and rename \n\nmsoa_copy <- msoa_matrix_numeric\n\nnames(msoa_copy)[names(msoa_copy) == \"Dwelling type (2011) Household spaces with no usual residents\"] <- \"DwellingNoResidents\"\nnames(msoa_copy)[names(msoa_copy) == \"House Prices Median House Price (£) 2010\"] <- \"MedianHousePrice\"\nnames(msoa_copy)[names(msoa_copy) == \"Dwelling type (2011) Flat, maisonette or apartment\"] <- \"FlatAprt\"\nnames(msoa_copy)[names(msoa_copy) == \"Qualifications (2011 Census) Schoolchildren and full-time students: Age 18 and over\"] <- \"fullTimeStudents\"\nnames(msoa_copy)[names(msoa_copy) == \"Car or van availability (2011 Census) No cars or vans in household\"] <- \"NoCars\"\nnames(msoa_copy)[names(msoa_copy) == \"Ethnic Group (2011 Census) Other ethnic group\"] <- \"OtherEthnicGroup\"\nnames(msoa_copy)[names(msoa_copy) == \"Central Heating (2011 Census) Households with central heating (%)\"] <- \"CentralHealingPercent\"\n\n\n\n```\nLet's now extract these to a separate dataframe, remove any missing values, and provide some quick summary statistics to identify any obvious concerns.\n\nWe also generate a matrix of scatter-plots, so as to identify any obvious relationships between our key values.\n\n```{r, cache=TRUE, warning=FALSE}\nfeature_df <- dplyr::select(msoa_copy, RPDburglaryShifted, RPDRobberyShifted, total_burglaries, total_robberies, DwellingNoResidents, MedianHousePrice, FlatAprt, fullTimeStudents, NoCars, OtherEthnicGroup, CentralHealingPercent, AvHholdSz, ComEstRes)\n\nfeature_df <- drop_na(feature_df, RPDburglaryShifted, RPDRobberyShifted)\nhead(feature_df)\n```\n\n\n\n\n```{r, cache=TRUE, warning=FALSE}\nsummary(feature_df)\n```\n\n\n```{r, cache=TRUE, warning=FALSE, fig.width = 13}\n\ncolSums(is.na(feature_df))\n\npairs(feature_df)\n\n```\nAs we hoped, some obvious relationships stand out: for instance, the presence of apartments, and households with no cars, or central heating and average household size.\n\nOur robbery and burglary data and change rates are densely clustered - they're unlikely to cleanly associate with anything. With that in mind, we'll perform a log transformation. This cannot be undertaken with negative values, so once again we'll perform a shift (of 2) for both of our relative error numbers, as well as a commercial resident column, before log transforming our features.\n\n\n```{r, cache=TRUE, warning=FALSE}\nfeature_df$BurglaryRPDTranform <- feature_df$RPDburglaryShifted    + 2\nfeature_df$RobberyRPDTranform <- feature_df$RPDRobberyShifted   + 2\n\nfeature_df$ComEstResTranform <- feature_df$ComEstRes   + 2\n\n\nfor (col in colnames(feature_df)){\n  new_name <- paste(\"log_\", col, sep = \"\")\n  feature_df[new_name] <- log(feature_df[col])\n}\n\ndrop<- c( \"log_ComEstRes\", \"log_RPDburglaryShifted\", \"log_RPDRobberyShifted\")\nfeature_df<- feature_df[,!(names(feature_df) %in% drop)]\n\n\nfeat_transform_df <- feature_df[,17:29]\norig_feat_df <- feature_df[,0:17]\n\n```\n\nWe've now separated a separate dataframe where each value has been log transformed - while this isn't hugely rigorous (and would benefit from inspecting the relationships in more detail) it serves our immediate purpose.\n\n```{r, fig.width = 13 , cache=TRUE}\npairs(feat_transform_df)\n\n```\n\nWhile we've introduced a bit of noise, we've also \"forced\" some of our variables into relationships that look semi linear. \n\nTo dig into this deeper, let's create a correlation matrix for our entire transformed dataframe.\n\n```{r , cache=TRUE}\ncorrelate(feat_transform_df)\n\n\n```\n\nTo provide a visual aid, I've extracted the column for our robbery relative change rate, and sorted the table accordingly.\n\n\n```{r , cache=TRUE}\ndplyr::select(correlate(feat_transform_df)[order(correlate(feat_transform_df)$log_BurglaryRPDTranform),], term, log_BurglaryRPDTranform)\n```\n### Regression\nNow that we've transformed our data, cleaned it up, and identified potential correlates, let's build our linear model. \n\nThere are various automated tools for this process that seek to provide the highest fit and significance, but given the high degree of correlation between my chosen features, I've taken a more manual approach and tested a variety of models until I identified one with a suitable fit.  The final model is below.\n\n```{r , cache=TRUE}\nmod_burglary <- lm(log_BurglaryRPDTranform ~ log_total_burglaries + log_FlatAprt + log_MedianHousePrice  + log_ComEstResTranform, data = feat_transform_df)\nsummary(mod_burglary)\n```\n\nOur model suggests the largest burglary decrease linked to lockdown was in MSOAs which a high level of historic burglary. The composition of housing/accomodation and area type also seems to play a role, with those areas with higher median house prices, and a larger number of commercial residents, seeing stronger decreases, while conversely areas with large number of apartments temper the effect.\n\nWhile all our variables are significant, the model is not a particularly good fit - the adjusted R2 is around 0.08, suggesting that less than 10% of the variance is accounted for by our model. I suspect more geographic features - such as distance from central London, more accurate footfall, or spatial lags - would probably be useful, but that's outside the scope of this project.\n\nLet's perform a similar exercise for robbery.\n\n```{r , cache=TRUE}\ndplyr::select(correlate(feat_transform_df)[order(correlate(feat_transform_df)$log_RobberyRPDTranform),], term, log_RobberyRPDTranform)\n```\n\n\n```{r, cache=TRUE}\nmod_burglary <- lm(log_RobberyRPDTranform ~ log_total_robberies  + log_total_burglaries + log_FlatAprt + log_CentralHealingPercent + log_fullTimeStudents, data = feat_transform_df)\nsummary(mod_burglary)\n```\n\nThis is a notably better fit than our burglary model, with our R2 suggesting we now account for over 20% of the variance. The nature of our predictors is also quite different: while we still see a negative relationship with historic crime (with areas of high historic crime experiencing larger relative decreases), there is a positive relationship with both the presence of apartments and central heating.\n\nI suspect some of these features are correlates of deprivation, so I want to create a quick scatter of three - for now we'll do it against median house price, which is definitely deprivation correlated.\n\n```{r, cache=TRUE}\n\nheating <- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_CentralHealingPercent)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\napartments <- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_FlatAprt)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\ncomest <- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_ComEstResTranform)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\n\n\nggarrange(heating, apartments, comest, ncol=3, nrow=1 )\n\n\n\n```\n\nWhile there does appear to be a relationship with some of these, it isn't strong - this suggests the factor's we have identified are significant not because of their association with deprivation and poverty, but because of what they mean about the specific characteristics of the area.\n\n### Random Forests\n\nUnlike regression, random forest doesn't really on any specific type of association - instead, we rely on computing power, repetition and iteration to capture the very best predictor for our variable, in any combination. \n\nThere are risks to this method: our sample size is smaller than I'd like, and this may lead to over-fit of outlier MSOAs.\n\nIt does mean we don't need to worry about transformations or correlations: we can return to our original dataset, and let the model identify the strongest predictors.\n\n```{r , cache=TRUE}\n# we remove rows where our main error is na or missing\nrf_msoa_matrix <- drop_na(msoa_matrix_numeric, RPDRobberyShifted)\n\n#then we repeat for any columns where value asre missing\nclean_rf_matrix <- rf_msoa_matrix[ , colSums(is.na(rf_msoa_matrix)) == 0]\n\n\n#then we drop out any values that directly predict our error.\n\ndrop<- c(\"burglaryActual\",\"burglaryError\",\"burglaryPercentError\",\"burglaryPredicted\",\"robberyActual\",\"robberyPredicted\",\"robberyError\",\"robberyPercentError\", \"RPDBurglaryShifted\", \"RPDRobberyShifted\")\n\n#remove out selected columns\ndata<- clean_rf_matrix[,!(names(clean_rf_matrix) %in% drop)]\n\n\n#automatically remove white space and make names r compatbiel\nnames(clean_rf_matrix)<-make.names(names(clean_rf_matrix),unique = TRUE)\n\ndrop<- c(\"burglaryActual\",\"burglaryError\",\"robberyActual\",\"robberyError\",\"RPDBurglary\",\"RPDRobbery\",\"robberyActualShifted\",\"robberyPredictedShifted\",\"burglaryActualShifted\",\"burglaryPredictedShifted\", \"burglaryPredicted\", \"burglaryPercentError\", \"robberyPredicted\", \"robberyPercentError\")\n#drop<- c(\"burglaryPercentError\",\"burglaryPredicted\",\"robberyPredicted\",\"robberyPercentError\")\n\ndata<- clean_rf_matrix[,!(names(clean_rf_matrix) %in% drop)]\n\n\nnames(data)<- make.names(names(data),unique = TRUE)\nhead(data)\n```\nWe start with modelling our relative rate of burglary shift. We divide our sample into a training set which we'll use to train the model, and a test set we'll use to verify accuracy and validity.\n\n```{r , cache=TRUE}\nset.seed(123)\n\nsample = sample.split(data$RPDburglaryShifted, SplitRatio = 0.7)\ntrain = subset(data, sample == TRUE)\ntest  = subset(data, sample == FALSE)\n\n\nrf_burglary <- randomForest(\n  RPDburglaryShifted ~ .,\n  data=train, \n  importance=TRUE\n)\n\nsummary(rf_burglary)\n```\n```{r , cache=TRUE}\n#calculate our predictions and a  rmse\nprediction <-predict(rf_burglary, test)\nMetrics::rmse(test$RPDburglaryShifted, prediction)\n```\n\nWhile machine learning models were once considered opaque and difficult to interpret, several libraries now offer functionality to explain predictions. Here I use [DALEX](https://cran.r-project.org/web/packages/DALEX/index.html) to do just this. \n\n```{r , cache=TRUE}\n\nrf_explainer_burglary <- DALEX::explain(rf_burglary, data=train, y= train$RPDburglaryShifted)\n\nrf_perf_burg <- model_performance(rf_explainer_burglary)\nrf_perf_burg\n```\nWe can see that our model significantly outperforms our best linear models: the R2 suggests that almost 85% of the variance is correctly interpreted, and our rmse (root mean squared error) is around 0.2 on our test set.\n\nThis model would be ill-suited to prediction or operationalising -it is a default forecast with no hyper-parameter tuning, and no consideration of error rates - but using tools like DALEX, we can identify which predictors the model identifies as most important. \n\n```{r, fig.width = 13 , cache=TRUE}\nmodel_parts_burg <-model_parts(rf_explainer_burglary)\n\n```\n\n```{r, fig.width = 13 , cache=TRUE}\nplot(model_parts_burg, max_vars=25)\n\n```\nThe three most important features our model highlights are:\n- the age composition of the population\n- the number of residents which are in commercial property\n- the historic number of burglaries\n\nThis seems to corroborate our previous regression model. To further unpick  these trends, we can use Partial Dependence Plots to identify how the model prediction shifts with these values. \n\n```{r, cache=TRUE, warning=FALSE, cache.lazy=FALSE}\npdp_b <- model_profile(rf_explainer_burglary)\n\n```\n\n```{r, cache=TRUE}\nplot(pdp_b, variables=\"total_burglaries\")\n\n```\nWe still see a strong association between a high number of historic, and a large reduction during the lockdown period\n\n```{r, cache=TRUE}\nplot(pdp_b, variables= \"Religion..2011..Christian\")\n\n```\n```{r, cache=TRUE}\nhist(data$Religion..2011..Christian)\n```\n\n```{r, cache=TRUE}\nplot(pdp_b, variables=\"ComEstRes\")\n```\n```{r, cache=TRUE}\nhist(msoa_matrix_numeric$ComEstRes)\n```\nBy combining the distribution of commercial residents by MSOA with our PDP, we can see that those MSOAs that are most densely populated by commercial residents see the smallest \"covid decrease\"(suggesting that those MSOAs that are very heavily residential saw the sharpest drops).\n\n```{r, cache=TRUE}\nplot(pdp_b, variables=\"House.Prices.Sales.2008\")\n\n```\n\nFinally, we can see that those areas that experienced the highest volume of house sales experienced the lowest relative decrease in burglary.\n\nThis highlights the importance of identifying correlates in RF models - especially in a dataset where features are highly interlinked, association does not imply causation, and we should be wary of over-interpreting.\n\nWe can now repeat our process for our robbery shift.\n\n\n```{r, cache=TRUE}\nset.seed(123)\n\n\nsample = sample.split(data$RPDRobberyShifted, SplitRatio = 0.75)\ntrain = subset(data, sample == TRUE)\ntest  = subset(data, sample == FALSE)\n\n\nrf_robbery <- randomForest(\n  RPDRobberyShifted ~ .,\n  data=train, \n  importance=TRUE\n)\n\nsummary(rf_robbery)\n\n```\nWe've now trained a model.  Let's now use the DALEX library to understand it, and see how it performs.\n\n```{r, cache=TRUE}\n\nrf_explainer_robbery <- DALEX::explain(rf_robbery, data=train, y= train$RPDRobberyShifted)\n\nrf_perf_rob <- model_performance(rf_explainer_robbery)\nrf_perf_rob\n```\n\n\n\n\n```{r, fig.width = 13,  cache=TRUE, warning=FALSE}\nmodel_parts_rob <-model_parts(rf_explainer_robbery)\n```\n\n```{r, fig.width = 13, cache=TRUE}\nplot(model_parts_rob, max_vars=25)\n```\n\n```{r, cache=TRUE, cache.lazy=FALSE}\npdp_rob <- model_profile(rf_explainer_robbery)\n```\n\n```{r, cache=TRUE}\nplot(pdp_rob, variables=\"total_robberies\")\n\n```\n\n```{r, cache=TRUE}\nplot(pdp_rob, variables=\"total_burglaries\")\n\n```\n\nThe effect of historic crime effects again appears like a reliable predictor of a robbery covid shift: those MSOAs with the highest number of burglaries and robberies see strong decreases in robbery (though the association with burglary is not clear cut, suggesting other interaction effects may be driving this)\n\n```{r, cache=TRUE}\nplot(pdp_rob, variables=\"Road.Casualties.2011.2011.Total\")\n\n```\nRoad casualties is another strong relationship. This could be a proxy for deprivation, but I suggest this is more down to geographic features - road casualties are probably rarer in denser urban environments, most likely to be affected by lockdown.\n\n```{r, cache=TRUE}\nplot(pdp_rob, variables=\"Ethnic.Group..2011.Census..Other.ethnic.group....\")\n```\n```{r, cache=TRUE}\nhist(data$Ethnic.Group..2011.Census..Other.ethnic.group....)\n\n```\n\n\nFinally, we see a demographic predictor linked to \"other ethnic group\". I'm not clear how to interpret this, but it suggests that those MSOAs that are most diverse (and have the largest representation by these ethnic groups) experienced the strongest decreases in robbery during lockdown. \n\nTogether, these analyses suggest that the crime drop for robbery and burglary during national lockdown was significantly affected by distinct local factors. For both offences, historical crime trends play a role, with high crime areas experiencing a relatively larger drop in both burglary and robbery.\n\nBeyond that, the drivers vary for each offence  type: burglary was driven by the composition of the residential population, with heavily residential areas, and areas with a relatively \"stable\" population as measured by low housing sales also saw larger decrease - this is likely due to the increased number of empty residential properties.\n\nRobbery decreases conversely, are well associated with high numbers of road casualties, as well as the ethnic makeup of the local population - this is likely due to an association with denser, more urban areas, with lower street speeds, and a possible link to deprivation, whereby minority population were least able to work from home, and were likely to still present as available targets for robbery.\n","srcMarkdownNoYaml":"\n\n\nThe lockdown and social distancing measures that were brought in throughout the world to tackle COVID in 2020 have had a significant, widespread effect on crime. In this notebook, I use public London crime data on robbery and burglary to examine where this \"COVID crime shift\" was strongest, and whether any specific drivers or correlates can be identified. I use three years of Metropolitan Police Service data from [data.police.uk.](https://data.police.uk/)\n\n\nThe findings suggest that the relative change in burglary and robbery in April and May 2020 was heavily affected by local characteristics: areas with a high residential population saw the sharpest decreases in burglary (likely due to a reduction in available targets) while the reduction in robberies instead seem to be driven by geographic features and  indicators of deprivation (potentially suggesting more available targets for robbery in communities least able to work for from home).\n\nThe primary purpose of this exercise was to learn R - I've previously worked entirely in Python, which is more than sufficient 99% of the time, but has at times proved a blocker when I want to tackle some more experimental geospatial and statistical methods.  With that in mind, this is likely to be a little messy, and I'll aim to condense my main lessons into a blog post in the future. The models are not heavily tuned (aiming to explore correlates rather than provide accurate predictions) and there are likely to be correlation between our various predictors - as such these should not be taken to suggest direct causation.\n\nThe full code and data for this exercise are available on my [Github repo](https://github.com/crimsoneer/Covid-Crime-Shift). I'm hoping to summarise my key lessons in the Python to R journey in [Medium post](https://medium.com/@andreas.varotsis) in the next few weeks.\n\n\n\n#### Resources I've used\n- Matt Ashby Crime Mapping course: https://github.com/mpjashby/crimemapping/\n- Spatial Modelling for Data Scientists: https://gdsl-ul.github.io/san/\n- R for Data Science: https://r4ds.had.co.nz/index.html\n- Geocomputation with R: https://geocompr.robinlovelace.net/\n\n\n\n\n### Tasks\n\n1. Ingest Data \n2. Predict trend by MSOA\n3. Quantify MSOA COVID Effect\n4. Model\n\n## Ingest Data\nFor this exercise, I'll be importing crime and robbery data by MSOA.[MSOAs are geographical units specifically designed for analysis, and to be comparable: they all have an average population of just over 8,000.](https://www.ons.gov.uk/methodology/geography/ukgeographies/censusgeography#output-area-oa) There is a compromise here between smaller geographical units (that create more variance that may help us identify predictors), but the necessity for enough crime per unit to identify meaningful trends - MSOAs should be suitable.\n\n```{r, results='hide', collapse=TRUE, warning=FALSE, cache=TRUE, echo=FALSE}\n# Data manipulation, transformation and visualisation\nlibrary(tidyverse)\n# Nice tables\nlibrary(kableExtra)\n# Simple features (a standardised way to encode vector data ie. points, lines, polygons)\nlibrary(sf) \n# Spatial objects conversion\nlibrary(sp) \n# Thematic maps\nlibrary(tmap) \n# Colour palettes\nlibrary(RColorBrewer) \n# More colour palettes\nlibrary(viridis)\n#ggplot organiastion\nlibrary(ggpubr)\nlibrary(raster)  # raster data\nlibrary(rgdal)  # input/output, projections\nlibrary(rgeos)  # geometry ops\nlibrary(spdep)  # spatial dependence\nlibrary(lubridate) #date and time\n#random forest and metrics\nlibrary(Metrics)\nlibrary(caret)\nlibrary(randomForest)\nrequire(caTools)\nlibrary(DALEX)\nlibrary(dplyr)\n#correlation matrix\nlibrary(corrr)\n\noptions(warn=-1)\n\n\n```\n\nTo build our process, we'll start by taking one month of crime data, exploring it, and writing all our steps for automation.\n\n```{r, warning=FALSE, cache=TRUE}\ntest_df <- read.csv(\"crimes/2018-01/2018-01-metropolitan-street.csv\")\n```\nOur crime data is categorised according to the Home Office major crime types, and like Python, we can list them all through the \"unique\" function. Here I'll be focusing on robbery and burglary: two crime types that are heavily reliant on encountering victim's in public spaces, and as such should be affected by the \"COVID effect\".\n\nTo avoid this getting particularly computationally intensive, let's write a function to pull out robberies and burglaries, and assign them a specific MSOA. Then we can iterate over all our months and get monthly counts for each offence type.\n\n```{r, warning=FALSE, cache=TRUE}\nsubset_df <- filter(test_df, Crime.type==\"Burglary\" | Crime.type==\"Robbery\")\nhead(subset_df)\n```\nOur single month of data contains 10,501 crimes.\n\nWe now need to link this to our spatial data. We use the MSOA borders provided by MOPAC, and use the UK National Grid coordinate system. Police.uk does not use that system, so we'll need to reproject our crime data.\n\n```{r, warning=FALSE, cache=TRUE}\nlsoa_borders <- st_read(\"msoa_borders/MSOA_2011_London_gen_MHW.tab\", crs=27700)\nplot(lsoa_borders)\n```\nBefore we can link our crimes to MSOA, we'll need to ensure identical coordinate systems, and remove any non-geolocated values we'll need to erase any missing values (while checking we retain enough data for analysis.)\n\n\n```{r , warning=FALSE, cache=TRUE}\n#count missing values in the longitude column\nprint(\"Missing values identified:\")\nsum(is.na(subset_df[\"Longitude\"]))\n```\n\nThankfully, we only identify 82 crimes which we need to remove, leaving plenty for analysis.\n\n```{r , warning=FALSE, cache=TRUE}\nclean_df <- subset_df[!rowSums(is.na(subset_df[\"Longitude\"])), ]\n```\nWe can now convert our crime data to spacial data, using our longitude and latitude coordinates - this allows us to quickly plot our data, and confirm it looks right.\n\n```{r , warning=FALSE, cache=TRUE, echo=FALSE}\n\nsubset_spatial <- st_as_sf(clean_df, coords = c(\"Longitude\", \"Latitude\"), \n                      crs = 4326, remove = FALSE)\n```\n\n\n```{r, cache=TRUE, echo=FALSE}\nplot(subset_spatial)\n```\nWith our data now mapped, we ensure everything is aligned to the appropriate coordinate system, and assign each crime to an MSOA from our data - the data is then aggregated into a monthly MSOA crime count, to which we assign our monthly date.\n\n\n```{r, cache=TRUE, echo=FALSE}\n\nlatlong = \"+init=epsg:4326\"\nukgrid = \"+init=epsg:27700\"\n\n#transform our spatial data to Easting and Northing coordinates\nsubset_osgb <- st_transform(subset_spatial, ukgrid)\n\n#use a spatial join to link them to MSOAs.\ncrime_with_msoa <- st_join(subset_osgb, lsoa_borders[\"MSOA11CD\"])\n\n#pivot the data to provide a count by crime and msoa.\nmsoa_list<- crime_with_msoa %>%\n  group_by(MSOA11CD, Crime.type) %>%\n  summarize(count_by_msoa = n())\n\nmsoa_pivot_tibble <- as_tibble(msoa_list)\nmsoa_pivot_tibble <- msoa_pivot_tibble[0:3]\n\n\n#creating a df with all msoa names, for robbery and burglary\nmsoa_zero_df_robbery <- unique(as_tibble(lsoa_borders)[\"MSOA11CD\"])\nmsoa_zero_df_burglary <- unique(as_tibble(lsoa_borders)[\"MSOA11CD\"])\n\n#adding our crime type column \nmsoa_zero_df_burglary[\"Crime.type\"] = \"Burglary\"\nmsoa_zero_df_robbery[\"Crime.type\"] = \"Robbery\"\n\n#Creating a \"count\" column identical to our pivot, and filling it with 0\nmsoa_zero_df_burglary[\"count_by_msoa\"] = as.numeric(0)\nmsoa_zero_df_robbery[\"count_by_msoa\"] = as.numeric(0)\n\n#combine both\nduplicate_concat <- rbind(msoa_zero_df_robbery, msoa_zero_df_burglary)\n\n#add our duplicates to our original table\ndf_with_dups <- rbind(msoa_pivot_tibble, duplicate_concat)\n\n\n#creating a filter for duplicates columns, which should ignore the first instance\ndup_filters <- duplicated(df_with_dups[0:2])\n\n\n#bring it all back together\nmonthly_df <- filter(df_with_dups, !dup_filters)\n\n#select the first unique value of months in the original dataframe\nmonth <- unique(test_df[\"Month\"])[1,1]\nmonthly_df[\"Month\"] <- month\nhead(monthly_df)\n\n```\nBringing together all the code so far into a function, we can create an pipeline to generate our crime count per MSOA time series for the entirety of our dataset.\n\n```{r , cache=TRUE, echo=FALSE}\n#quick initial function to generate our MSOA borde spatial frame, to avoid it sitting in the initial frame and gobbling loads of memory.\ngenerate_msoa_borders <- function(file){\n  msoa_borders <- st_read(file, crs=27700)\n  return(msoa_borders)\n}\n\nmake_month_pivot <- function(file){\n  #define our CRS\n  latlong = \"+init=epsg:4326\"\n  ukgrid = \"+init=epsg:27700\"\n  #read our crime from the file\n  test_df <- read.csv(file)\n  #select only our target crime types\n  subset_df <- filter(test_df, Crime.type==\"Burglary\" | Crime.type==\"Robbery\")\n  #remove any rows with a long/lat coordinate\n  clean_df <- subset_df[!rowSums(is.na(subset_df[\"Longitude\"])), ]\n  #generate a spatial df\n  subset_spatial <- st_as_sf(clean_df, coords = c(\"Longitude\", \"Latitude\"), \n                      crs = 4326, remove = FALSE)\n  #reproject to uk grid coords\n  subset_osgb <- st_transform(subset_spatial, ukgrid)\n  #spatially join to assign to an MSOA\n  crime_with_msoa <- st_join(subset_osgb, msoa_borders[\"MSOA11CD\"])\n  #summarise by count of MSOA\n  msoa_list<- crime_with_msoa %>%\n    group_by(MSOA11CD, Crime.type) %>%\n    summarize(count_by_msoa = n())\n  #return to a non-geographic msoa\n  msoa_pivot_tibble <- as_tibble(msoa_list)\n  msoa_pivot_tibble <- msoa_pivot_tibble[0:3]\n  #creating a df with all msoa names, for robbery and burglary\n  msoa_zero_df_robbery <- unique(as_tibble(msoa_borders)[\"MSOA11CD\"])\n  msoa_zero_df_burglary <- unique(as_tibble(msoa_borders)[\"MSOA11CD\"])\n  #adding our crime type column \n  msoa_zero_df_burglary[\"Crime.type\"] = \"Burglary\"\n  msoa_zero_df_robbery[\"Crime.type\"] = \"Robbery\"\n  #Creating a \"count\" column identical to our pivot, and filling it with 0\n  msoa_zero_df_burglary[\"count_by_msoa\"] = as.numeric(0)\n  msoa_zero_df_robbery[\"count_by_msoa\"] = as.numeric(0)\n  duplicate_concat <- rbind(msoa_zero_df_robbery, msoa_zero_df_burglary)\n  df_with_dups <- rbind(msoa_pivot_tibble, duplicate_concat)\n  #creating a filter for duplicates columns, which should ignore the first instance\n  dup_filters <- duplicated(df_with_dups[0:2])\n  monthly_df <- filter(df_with_dups, !dup_filters)\n  #re-add our month column\n  month <- unique(test_df[\"Month\"])[1,1]\n  monthly_df[\"Month\"] <- month\n  return(monthly_df)\n}\n```\n\nFor this project, I haven't used the Police.uk API (which would have enabled me to automate  the downloads and query the data directly) - as such, we have to iterate over our subfolders, ingesting our CSV data and running through our process.\n\n```{r, cache=TRUE, warning=FALSE, echo=FALSE, results=\"hide\"}\n#create empty dataframe to bring together our data\nempty_df <- tibble(\nMSOA11CD = \"\", \nCrime.type= \"\",\ncount_by_msoa= \"\",\nMonth= \"\"\n)\n\n#re-ingest our MSOA data\nmsoa_borders <- generate_msoa_borders(\"msoa_borders/MSOA_2011_London_gen_MHW.tab\")\n\n#recursive argument ensures we also explore subfolders.\nsubfiles <- list.files(path = \"crimes\", recursive=T)\n\nfor (file in subfiles){\n  folder_subdir <- \"crimes/\"\n  #concatenate to get our total subfolder directory - hacky but will work here.\n  sub_path <- paste(folder_subdir, file, sep=\"\")\n  monthly_df <- make_month_pivot(sub_path)\n  empty_df <- rbind(empty_df, monthly_df)\n}\n\n```\n\n\n```{r, cache=TRUE,  ,warning=FALSE, echo=FALSE}\nhead(empty_df)\n```\nWe now have a combined dataframe of 71,848 rows, from January 2018 through December 2020.\n\n\n```{r, results='hide', collapse=TRUE, warning=FALSE, cache=TRUE}\n#saving file to CSV\n#write.csv(empty_df,\"msoa_crime_matrix.csv\")\n\n```\n\n## 2. Predict trend by MSOA\n#### Visualisation and Exploration\nWith our data now cleaned and aggregated, we can focus on the more interesting part - forecasting our \"expected\" pandemic crime, and examining how much it diverges from our \"actual\" crime.\n\n```{r , cache=TRUE}\nempty_df <- read.csv(\"msoa_crime_matrix.csv\")\nempty_df <- empty_df[2:70848,2:5]\nhead(empty_df)\n```\n\n\nBefore going any further, let's use this to explore and visualise the distribution of robbery and burglary across time and space during our \"pre-pandemic\" period, in March 2020 - based on London mobility indicators, this is when movement accross London began to be heavily affected, and the disruption was most notable in April\n\n![London mobility data](london_mobility.png)\n\n```{r , cache=TRUE}\nburglary_df<-empty_df\n\n#add a \"1\" so our month can be converted to a full date\nburglary_df$DateString <- paste(burglary_df$Month, \"-01\", sep=\"\")\n\n#convert to date format\nburglary_df$DateClean <- ymd(burglary_df$DateString)\n\n#filter out only burglary prior to the pandemic\nburglaryExplore <- filter(burglary_df,  DateClean < \"2020-03-01\" & Crime.type==\"Burglary\")\n\nhead(burglaryExplore)\n```\n\n\nLooking at the aggregate counts of burglary across London, a visual observation suggests yearly trends (which we'll have to consider in our forecast), which sharp peaks during the Winter months and the lowest numbers in summer (when the days are longest).\n```{r , cache=TRUE}\n#group burglary count by months and plot\nburglary_by_month <- burglaryExplore %>%\n  group_by(DateClean) %>%\n  summarize(total_burglaries = sum(count_by_msoa))\n\nggplot(burglary_by_month, aes(x=DateClean, y=total_burglaries)) +\n  geom_line()\n```\n\nTo observe how crime counts are distributed in space, let's map both counts by MSOA. As  previously mentioned, MSOAs are designed to be comparable units, at least from a population perspective - we don't need to produce per population rates. \n\n\n```{r, fig.width = 13 , cache=TRUE}\nburglary_by_msoa <- burglaryExplore %>%\n  group_by(MSOA11CD) %>%\n  summarize(total_burglaries = sum(count_by_msoa))\n\n#we join our burglary counts to their geographic msoa\nburglary_map <- left_join(lsoa_borders, burglary_by_msoa, by = \"MSOA11CD\")\n\n#user brewer colour palette https://colorbrewer2.org\npal <- brewer.pal(5,\"BuGn\")\n\n#create our map, and add the layout options\nburglary_map <-tm_shape(burglary_map) +\n  tm_fill(col = \"total_burglaries\", title = \"Total Burglary Count by MSOA\", style=\"quantile\", palette=\"BuGn\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\nrobbery_df<-empty_df\n\nrobbery_df$DateString <- paste(robbery_df$Month, \"-01\", sep=\"\")\nrobbery_df$DateClean <- ymd(robbery_df$DateString)\nrobberyExplore <- filter(robbery_df,  DateClean < \"2020-03-01\" & Crime.type==\"Robbery\")\n\nrobbery_by_msoa <- robberyExplore %>%\n  group_by(MSOA11CD) %>%\n  summarize(total_robberies = sum(count_by_msoa))\n\nrobbery_map <- left_join(lsoa_borders, robbery_by_msoa, by = \"MSOA11CD\")\n\npal <- brewer.pal(5,\"BuGn\")\n\n\nrobbery_map <-tm_shape(robbery_map) +\n  tm_fill(col = \"total_robberies\", title = \"Total Robbery Count by MSOA\", style=\"quantile\", palette=\"BuGn\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\n#arrange the maps together\ntmap_arrange(burglary_map, robbery_map, nrow = 2)\n```\n\nWe notice that robbery is noticeably more concentrated in central London, with burglary remaining quite common across the city. That said, there are also obvious spatial patterns here - these crimes are clustered in certain geographies. \n\n#### Modelling \nWe can now begin the forecasting process. To design our process, we'll start by focusing on a single MSOA - the first in our dataset, [E02000001, or the City of London.](https://findthatpostcode.uk/areas/E02000001.html)\n\n```{r , cache=TRUE}\nsingle_msoa_df <- filter(empty_df, MSOA11CD == \"E02000001\" & Crime.type==\"Burglary\")\n\n#we add a 01 to our date to ensure R recognises the date format\nsingle_msoa_df$DateString <- paste(single_msoa_df$Month, \"-01\")\n\n\nsingle_msoa_df$DateClean <- ymd(single_msoa_df$DateString)\nsingle_msoa_df\n```\nFrom a forecasting/time-series perspective, this is a very small dataset - 36 monthly observations. We will be shrinking this further to only 26 by focusing on data prior to March 2020, when the COVID crime impact is felt. This significantly limits our forecasting options, and will impact accuracy, if we treat each MSOA in isolation - we could explore some sort of Vector Autoregressive Model to limit this, but given that we're then going to be exploring the error of all our models in aggregation, this isn't crucial. Our focus is on models that we can accurately deploy without needing to tune each of them individually, and that can capture the seasonal trend, and generate reliable predictions on our limited dataset. \n\nGiven these limitations, I've opted for [the Prophet algorith.](https://facebook.github.io/prophet/) While it's more opaque than a auto-arima or VAR model, it works well with monthly data, and extracting seasonal trends. It also requires very little tuning.\n\nAs such, we'll extract our \"training set\" prior to March, and start forecasting.\n\n```{r , cache=TRUE}\n\ntraining_set <- filter(single_msoa_df, DateClean < \"2020-03-01\")\n\ntraining_df <- tibble(\n  ds=training_set$DateClean,\n  y=training_set$count_by_msoa\n)\nhead(training_df)\n```\n\n```{r, results='hide' , cache=TRUE}\nlibrary(prophet)\nm <- prophet(training_df)\n\n```\nFor now, we'll forecast on a 6 month horizon - we obviously wouldn't expect it to be accurate that far into the future.\n\n```{r , cache=TRUE}\n#prophet generates a future dataframe using our data, for 6 mperiods\nfuture <- make_future_dataframe(m, periods = 6, freq = 'month')\n\n\nforecast <- predict(m, future)\n\nplot(m, forecast)\n\n```\nAs we can see, the model seems consistent on a short horizon, and gets very wide as it goes further into the future. More importantly however, it has extracted a yearly seasonal compontent - the summer decrease we identified previously - as well as a long term trend.  \n\n```{r, cache=TRUE,  ,warning=FALSE}\n\nprophet_plot_components(m, forecast)\n```\nThese predictions seem far-fetched, but remember we will be observing a London wide error rate. As such, we must now isolate our \"pandemic period\" - which we define as April and May 2020 - and compare the predicted crime counts to the actual crime counts to obtain a metric of our \"COVID crime shift\", or our error rate.\n\n```{r, cache=TRUE, warning=FALSE}\n\n\nforecast$Month <- month(forecast$ds)\nforecast$Year <- year(forecast$ds)\n\n\nthis_year <- filter(forecast, Year > 2019)\npeak_pandemic <- filter(this_year, Month== 4 | Month== 5 )\n\npredictionPivot <- peak_pandemic %>%\n  group_by(Month) %>%\n  summarize(predicted_burglary = mean(yhat))\n\n\nsingle_msoa_df$MonthNum <- month(single_msoa_df$DateClean)\nsingle_msoa_df$YearNum <- year(single_msoa_df$DateClean)\n\nthis_year_actual <- filter(single_msoa_df, YearNum > 2019)\npeak_pandemic_actual <- filter(this_year_actual, MonthNum== 4 | MonthNum== 5 )\n\nactual_burglary <- sum(peak_pandemic_actual$count_by_msoa)\npred_burglary <- sum(predictionPivot$predicted_burglary)\n\nerror <- actual_burglary - pred_burglary\npercentage_error <- error / pred_burglary \n\nprint(\"Burglary Count\")\nprint(actual_burglary)\nprint(\"Predicted\")\nprint(pred_burglary)\n\nprint(\"Actual Error\")\nprint(error)\nprint(\"Percentage Error\")\nprint(percentage_error)\n```\nIn this MSOA, our model predicted nearly 8 burglaries would occur in these two months, based on pre-pandemic trends. In reality, 1 took place - a large error rate, suggesting a strong \"COVID effect\".\n\nThis process can now be replicated for every MSOA in London, to obtain this metric for each MSOA.\n\n```{r}\nlength(unique(empty_df$MSOA11CD))\n\n```\n\n\n\n```{r, collapse=TRUE, warning=FALSE, cache=TRUE, echo=FALSE,message=FALSE}\n\n\nmsoa_error_tibble <- tibble(\nMSOA11CD = \"\", \nburglaryActual= \"\",\nburglaryPredicted= \"\",\nburglaryError= \"\",\nburglaryPercentError=\"\",\nrobberyActual= \"\",\nrobberyPredicted= \"\",\nrobberyError= \"\",\nrobberyPercentError=\"\"\n)\n\ncalculate_error <- function(msoaName){\n  #select only burglary and our msoa\n  single_msoa_df <- filter(empty_df, MSOA11CD == msoaName & Crime.type==\"Burglary\")\n  #clean date date\n  single_msoa_df$DateString <- paste(single_msoa_df$Month, \"-01\")\n  single_msoa_df$DateClean <- ymd(single_msoa_df$DateString)\n  #generate training set up until March\n  training_set <- filter(single_msoa_df, DateClean < \"2020-03-01\")\n  #prepare for Prophet\n  training_df <- tibble(\n    ds=training_set$DateClean,\n    y=training_set$count_by_msoa)\n  #start and predict prophet for 6 months\n  m <- prophet(training_df)\n  future <- make_future_dataframe(m, periods = 6, freq = 'month')\n  forecast <- predict(m, future)\n  forecast$Month <- month(forecast$ds)\n  forecast$Year <- year(forecast$ds)\n  #aggregate forecasts and actual crime\n  this_year <- filter(forecast, Year > 2019)\n  peak_pandemic <- filter(this_year, Month== 4 | Month== 5 )\n  predictionPivot <- peak_pandemic %>%\n    group_by(Month) %>%\n    summarize(predicted_burglary = mean(yhat))\n\n  single_msoa_df$MonthNum <- month(single_msoa_df$DateClean)\n  single_msoa_df$YearNum <- year(single_msoa_df$DateClean)\n  #generate error rates\n  this_year_actual <- filter(single_msoa_df, YearNum > 2019)\n  peak_pandemic_actual <- filter(this_year_actual, MonthNum== 4 | MonthNum== 5 )\n  actual_burglary <- sum(peak_pandemic_actual$count_by_msoa)\n  pred_burglary <- sum(predictionPivot$predicted_burglary)\n  error_burg <- actual_burglary - pred_burglary\n  percentage_error_burg <- error_burg / pred_burglary \n  \n  #now repeat for robbery\n  \n  single_msoa_df <- filter(empty_df, MSOA11CD == msoaName & Crime.type==\"Robbery\")\n  single_msoa_df$DateString <- paste(single_msoa_df$Month, \"-01\")\n  single_msoa_df$DateClean <- ymd(single_msoa_df$DateString)\n  training_set <- filter(single_msoa_df, DateClean < \"2020-03-01\")\n  training_df <- tibble(\n    ds=training_set$DateClean,\n    y=training_set$count_by_msoa)\n  m <- prophet(training_df)\n  future <- make_future_dataframe(m, periods = 6, freq = 'month')\n  forecast <- predict(m, future)\n  forecast$Month <- month(forecast$ds)\n  forecast$Year <- year(forecast$ds)\n  this_year <- filter(forecast, Year > 2019)\n  peak_pandemic <- filter(this_year, Month== 4 | Month== 5 )\n  predictionPivot <- peak_pandemic %>%\n    group_by(Month) %>%\n    summarize(predicted_burglary = mean(yhat))\n\n  single_msoa_df$MonthNum <- month(single_msoa_df$DateClean)\n  single_msoa_df$YearNum <- year(single_msoa_df$DateClean)\n\n  this_year_actual <- filter(single_msoa_df, YearNum > 2019)\n  peak_pandemic_actual <- filter(this_year_actual, MonthNum== 4 | MonthNum== 5 )\n  actual_robbery <- sum(peak_pandemic_actual$count_by_msoa)\n  pred_robbery <- sum(predictionPivot$predicted_burglary)\n  error_rob <- actual_robbery - pred_robbery\n  percentage_error_rob <- error_rob / pred_robbery \n  \n  #create our output dataframe and return it\n  \n  msoa_error_tibble <- tibble(\n    MSOA11CD = msoaName, \n    burglaryActual= actual_burglary,\n    burglaryPredicted= pred_burglary,\n    burglaryError= error_burg,\n    burglaryPercentError = percentage_error_burg,\n    robberyActual= actual_robbery,\n    robberyPredicted= pred_robbery,\n    robberyError= error_rob,\n    robberyPercentError=percentage_error_rob\n    )\n\n  return(msoa_error_tibble)\n}\n\n\ntryCatch(\n    expr = {\n      for (msoa in unique(empty_df$MSOA11CD)){\n        iterated_msoa_df <- calculate_error(msoa)\n        msoa_error_tibble <- rbind(msoa_error_tibble, iterated_msoa_df)\n  \n}\n    },\n    error = function(e){ \n        # (Optional)\n        # Do this if an error is caught...\n    },\n    warning = function(w){\n        # (Optional)\n        # Do this if an warning is caught...\n    },\n    finally = {\n        # (Optional)\n        # Do this at the end before quitting the tryCatch structure...\n    }\n)\n\n\n\n\n```\n\n\n\n\n```{r , cache=TRUE, warning=FALSE}\nhead(msoa_error_tibble)\n```\n\nOur process has completed: we have a \"COVID shift\" measure for all of London.\n\n## 3. Measuring Local COVID Crime Shifts\n\nWe now need to use our forecasts to measure the \"error\" - this should provide an indication of the \"COVID Crime Shift\", or how much the actual crime diverted from the previous forecasts.\n\nI explored various avenues for this: the ideal solution would be a relative rate of the error, as MSOAs with large crime numbers will likely generate large errors, and so a rate would be ideal, though this is complicated by our erratic prediction and mix of positive and negative numbers.\n\nOur final solution has explored two options:\n- the absolute error number\n- the relative error once the crime and predictions have been transformed (by adding 50)\n\n$$\nactual_{k} = actual + 50\n$$\n\n$$\npredicted_{k} = predicted + 50\n$$\n\n$$\nRPD = \\frac{(actual_{k} - predicted_{k})}  {(actual_{k} + predicted_{k})/2}\n$$\n\nWe visualise and describe these statistics first to ensure they appear sensible.\n\n\n```{r ,  results='hide', collapse=TRUE, warning=FALSE, cache=TRUE, echo=FALSE}\n#write_csv(msoa_error_tibble, \"msoa_error_table2.csv\")\n```\n\n```{r , cache=TRUE, warning=FALSE}\nmsoa_error_tibble <- read_csv(\"msoa_error_table2.csv\")\n\nmsoa_error_tibble[,2:9] <- lapply(msoa_error_tibble[,2:9], as.numeric)\n\nmsoa_error_tibble <- msoa_error_tibble[2:980, ]\n\nmsoa_error_tibble <- left_join(msoa_error_tibble, robbery_by_msoa, by = \"MSOA11CD\")\nmsoa_error_tibble <- left_join(msoa_error_tibble, burglary_by_msoa, by = \"MSOA11CD\")\n\n\n```\n\n\n```{r, cache=TRUE, warning=FALSE}\n\nmsoa_error_tibble$RPDBurglary <- (msoa_error_tibble$burglaryActual - msoa_error_tibble$burglaryPredicted)/((msoa_error_tibble$burglaryPredicted + msoa_error_tibble$burglaryActual)/2)\n\nmsoa_error_tibble$RPDRobbery <- (msoa_error_tibble$robberyActual - msoa_error_tibble$robberyPredicted)/((msoa_error_tibble$robberyPredicted + msoa_error_tibble$robberyActual)/2)\n\nmsoa_error_tibble$robberyActualShifted <- msoa_error_tibble$robberyActual + 50\nmsoa_error_tibble$robberyPredictedShifted <- msoa_error_tibble$robberyPredicted + 50\n\n\nmsoa_error_tibble$RPDRobberyShifted <- (msoa_error_tibble$robberyActualShifted - msoa_error_tibble$robberyPredictedShifted)/((msoa_error_tibble$robberyPredictedShifted + msoa_error_tibble$robberyActualShifted)/2)\n\nmsoa_error_tibble$burglaryActualShifted <- msoa_error_tibble$burglaryActual + 50\nmsoa_error_tibble$burglaryPredictedShifted <- msoa_error_tibble$burglaryPredicted + 50\n\n\nmsoa_error_tibble$RPDburglaryShifted <- (msoa_error_tibble$burglaryActualShifted - msoa_error_tibble$burglaryPredictedShifted)/((msoa_error_tibble$burglaryPredictedShifted + msoa_error_tibble$burglaryActualShifted)/2)\n\n\n```\n\n```{r, cache=TRUE, warning=FALSE}\nprint(\"Burglary Error\")\nsummary(msoa_error_tibble$burglaryError)\nprint(\"Burglary Relative Error\")\nsummary(msoa_error_tibble$RPDburglaryShifted)\nprint(\"Robbery Error\")\nsummary(msoa_error_tibble$robberyError)\nprint(\"Robbery Relative Error\")\nsummary(msoa_error_tibble$RPDRobberyShifted)\n\n```\nAs we can see, the average London MSOA experienced a negative COVID crime shift for both burglary and robbery, but this is far from equally distributed - at the extremes, some areas actually see large increases on our predicted values. \n\n```{r , fig.width = 13 , cache=TRUE, warning=FALSE}\n\nburg_hist <- ggplot(msoa_error_tibble, aes(x=burglaryError)) + geom_histogram()\nrob_hist <-ggplot(msoa_error_tibble, aes(x=robberyError)) + geom_histogram()\nburg_r_hist <- ggplot(msoa_error_tibble, aes(x=RPDburglaryShifted)) + geom_histogram()\nrob_r_hist <- ggplot(msoa_error_tibble, aes(x=RPDRobberyShifted)) + geom_histogram()\nscatter <- ggplot(msoa_error_tibble, aes(x = RPDRobberyShifted, y = RPDburglaryShifted)) +\n  geom_point()\n\nr_scatter <- ggplot(msoa_error_tibble, aes(x = robberyError, y = burglaryError)) +\n  geom_point()\n\nggarrange(rob_hist, burg_hist, rob_r_hist, burg_r_hist,scatter, r_scatter, ncol=2, nrow=3 )\n\n\n```\nOur shifted relative error rate seems to function as intended: while there are still outliers, they are more concentrated than they are for the pure error term, and the overall distribution is more focused, while still indicating the direction and relative strength of our COVID effect.\n\nLet's map this effect visually, and see if any particular areas stand out.\n\n```{r , fig.width = 13, cache=TRUE, warning=FALSE}\n#re-ingest our geographic MSOA borders\nmsoa_borders <- st_read(\"msoa_borders/MSOA_2011_London_gen_MHW.tab\", crs=27700)\n\ngeographic_error_map <- left_join(msoa_borders, msoa_error_tibble, by = \"MSOA11CD\")\n\nburg_map <- tm_shape(geographic_error_map) +\n  tm_fill(col = \"robberyError\", title = \"Robbery Error\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\nrob_map <-tm_shape(geographic_error_map) +\n  tm_fill(col = \"burglaryError\", title = \"Burglary  Error\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\nburg_map_rate <- tm_shape(geographic_error_map) +\n  tm_fill(col = \"RPDRobberyShifted\", title = \"Robbery Error Relative\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\nrob_map_rate <-tm_shape(geographic_error_map) +\n  tm_fill(col = \"RPDburglaryShifted\", title = \"Burglary  Error Relative\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\ntmap_arrange(burg_map, rob_map, burg_map_rate, rob_map_rate , nrow = 2, ncol=2)\n\n```\nIt's hard to identify any obvious effect visually, but we do notice that while central London sees some very strong reductions, it also sees some increases.  Conversely, the outskirts of London (notably to the south and West) are a near continuous area of large decreases. The effect does vary by offence type, but the pattern seen in South and West London appears broadly consistent.\n\n## Identifying Correlates and Modelling\n\nWe've identified that the COVID crime effect was felt unequally accross London, and varies by offence type. To finalise our project, we will be linking our data to [demographic data provided by MOPAC](https://data.london.gov.uk/dataset/msoa-atlas), and aiming to use it to identify correlates to our \"covid shift\", and hopefully build models disentangling the effect.\n\n```{r , cache=TRUE, warning=FALSE}\nlibrary(readxl)\n#ingest ATLAS\nmsoa_atlas <- read_excel(\"msoa_atlas/msoa-data.xls\")\n\n#join by MSOA\ngeographic_msoa_matrix <- left_join(geographic_error_map, msoa_atlas, by = \"MSOA11CD\")\n\n#convert to tibble\nmsoa_matrix_tbl <- as_tibble(geographic_msoa_matrix)\nwrite_csv(msoa_matrix_tbl, \"msoa_matrix.csv\")\n\n\n#select only numeric data\nmsoa_matrix_numeric <-dplyr::select_if(msoa_matrix_tbl, is.numeric)\nhead(msoa_matrix_numeric)\n```\nHaving now ingested and linked our data, we begin by exploring the factors most highly correlated with our relative error rates.\n\n```{r, results='hide', cache=TRUE, warning=FALSE}\n\ncorr_df <- correlate(dplyr::select_if(msoa_matrix_tbl, is.numeric), quiet = TRUE)\n\noptions(scipen=999)\n```\nStarting by our relative robbery error, a few interesting correlates stand out:\n- road traffic casualties\n- burglary numbers\n- the number of dwellings with no usual residents, and the number of commercial residents\n- households with no cars\n- the age composition of the area\n- general deprivation indicators (such as the proportion of households with central heating)\n\n```{r, cache=TRUE, warning=FALSE}\n#show only correlates with an absolute value higher than 0.2\n#filter(dplyr::select(corr_df[order(corr_df$RPDRobberyShifted),] , term, RPDRobberyShifted), RPDRobberyShifted < -0.2 | RPDRobberyShifted > 0.2)\n#filter(dplyr::select(corr_df[order(corr_df$RPDRobberyShifted),] , term, RPDRobberyShifted), RPDRobberyShifted < -0.2 | RPDRobberyShifted > 0.2)\nhigh_corr_rob <- filter(dplyr::select(corr_df, term, RPDRobberyShifted), RPDRobberyShifted < -0.15 | RPDRobberyShifted > 0.15)\n\nhigh_corr_rob[order(high_corr_rob$RPDRobberyShifted),]\n\n```\n\nThe correlations for burglary are weaker - only a few have an absolute value higher than 0.2 - but a few stand out:\n- households with no residents\n- high robbery numbers\n- house prices \n\n```{r, cache=TRUE, warning=FALSE}\n\nhigh_corr_burg <- filter(dplyr::select(corr_df, term, RPDburglaryShifted), RPDburglaryShifted < -0.15 | RPDburglaryShifted > 0.15)\n\nhigh_corr_burg[order(high_corr_burg$RPDburglaryShifted),]\n\n```\n\nThese correlates suggest we can model this shift - this is likely to prove more reliable for robbery (where the correlations are stronger), and seem linked to usual resident population(as measured by household composition), deprivation (through various proxy indicators such as central heating presence or housing type), and general crime patterns (through total burglary and robbery numbers)\n\nWe will take two approaches for modelling: a simple regression (to identify strong links) and random forest regressors (to identify non-linear associations)\n\n### Simple Regression\n\nWe begin through the use of simple OLS regression. This is a linear model that has large limitations for modelling complex relationships, but can be an effective first step, effectively with a few transformations.\n\nR does not cope well with blank spaces in terms, so we'll extract and rename our key correlates.\n\n```{r, cache=TRUE, warning=FALSE}\n\n#make copy of df and rename \n\nmsoa_copy <- msoa_matrix_numeric\n\nnames(msoa_copy)[names(msoa_copy) == \"Dwelling type (2011) Household spaces with no usual residents\"] <- \"DwellingNoResidents\"\nnames(msoa_copy)[names(msoa_copy) == \"House Prices Median House Price (£) 2010\"] <- \"MedianHousePrice\"\nnames(msoa_copy)[names(msoa_copy) == \"Dwelling type (2011) Flat, maisonette or apartment\"] <- \"FlatAprt\"\nnames(msoa_copy)[names(msoa_copy) == \"Qualifications (2011 Census) Schoolchildren and full-time students: Age 18 and over\"] <- \"fullTimeStudents\"\nnames(msoa_copy)[names(msoa_copy) == \"Car or van availability (2011 Census) No cars or vans in household\"] <- \"NoCars\"\nnames(msoa_copy)[names(msoa_copy) == \"Ethnic Group (2011 Census) Other ethnic group\"] <- \"OtherEthnicGroup\"\nnames(msoa_copy)[names(msoa_copy) == \"Central Heating (2011 Census) Households with central heating (%)\"] <- \"CentralHealingPercent\"\n\n\n\n```\nLet's now extract these to a separate dataframe, remove any missing values, and provide some quick summary statistics to identify any obvious concerns.\n\nWe also generate a matrix of scatter-plots, so as to identify any obvious relationships between our key values.\n\n```{r, cache=TRUE, warning=FALSE}\nfeature_df <- dplyr::select(msoa_copy, RPDburglaryShifted, RPDRobberyShifted, total_burglaries, total_robberies, DwellingNoResidents, MedianHousePrice, FlatAprt, fullTimeStudents, NoCars, OtherEthnicGroup, CentralHealingPercent, AvHholdSz, ComEstRes)\n\nfeature_df <- drop_na(feature_df, RPDburglaryShifted, RPDRobberyShifted)\nhead(feature_df)\n```\n\n\n\n\n```{r, cache=TRUE, warning=FALSE}\nsummary(feature_df)\n```\n\n\n```{r, cache=TRUE, warning=FALSE, fig.width = 13}\n\ncolSums(is.na(feature_df))\n\npairs(feature_df)\n\n```\nAs we hoped, some obvious relationships stand out: for instance, the presence of apartments, and households with no cars, or central heating and average household size.\n\nOur robbery and burglary data and change rates are densely clustered - they're unlikely to cleanly associate with anything. With that in mind, we'll perform a log transformation. This cannot be undertaken with negative values, so once again we'll perform a shift (of 2) for both of our relative error numbers, as well as a commercial resident column, before log transforming our features.\n\n\n```{r, cache=TRUE, warning=FALSE}\nfeature_df$BurglaryRPDTranform <- feature_df$RPDburglaryShifted    + 2\nfeature_df$RobberyRPDTranform <- feature_df$RPDRobberyShifted   + 2\n\nfeature_df$ComEstResTranform <- feature_df$ComEstRes   + 2\n\n\nfor (col in colnames(feature_df)){\n  new_name <- paste(\"log_\", col, sep = \"\")\n  feature_df[new_name] <- log(feature_df[col])\n}\n\ndrop<- c( \"log_ComEstRes\", \"log_RPDburglaryShifted\", \"log_RPDRobberyShifted\")\nfeature_df<- feature_df[,!(names(feature_df) %in% drop)]\n\n\nfeat_transform_df <- feature_df[,17:29]\norig_feat_df <- feature_df[,0:17]\n\n```\n\nWe've now separated a separate dataframe where each value has been log transformed - while this isn't hugely rigorous (and would benefit from inspecting the relationships in more detail) it serves our immediate purpose.\n\n```{r, fig.width = 13 , cache=TRUE}\npairs(feat_transform_df)\n\n```\n\nWhile we've introduced a bit of noise, we've also \"forced\" some of our variables into relationships that look semi linear. \n\nTo dig into this deeper, let's create a correlation matrix for our entire transformed dataframe.\n\n```{r , cache=TRUE}\ncorrelate(feat_transform_df)\n\n\n```\n\nTo provide a visual aid, I've extracted the column for our robbery relative change rate, and sorted the table accordingly.\n\n\n```{r , cache=TRUE}\ndplyr::select(correlate(feat_transform_df)[order(correlate(feat_transform_df)$log_BurglaryRPDTranform),], term, log_BurglaryRPDTranform)\n```\n### Regression\nNow that we've transformed our data, cleaned it up, and identified potential correlates, let's build our linear model. \n\nThere are various automated tools for this process that seek to provide the highest fit and significance, but given the high degree of correlation between my chosen features, I've taken a more manual approach and tested a variety of models until I identified one with a suitable fit.  The final model is below.\n\n```{r , cache=TRUE}\nmod_burglary <- lm(log_BurglaryRPDTranform ~ log_total_burglaries + log_FlatAprt + log_MedianHousePrice  + log_ComEstResTranform, data = feat_transform_df)\nsummary(mod_burglary)\n```\n\nOur model suggests the largest burglary decrease linked to lockdown was in MSOAs which a high level of historic burglary. The composition of housing/accomodation and area type also seems to play a role, with those areas with higher median house prices, and a larger number of commercial residents, seeing stronger decreases, while conversely areas with large number of apartments temper the effect.\n\nWhile all our variables are significant, the model is not a particularly good fit - the adjusted R2 is around 0.08, suggesting that less than 10% of the variance is accounted for by our model. I suspect more geographic features - such as distance from central London, more accurate footfall, or spatial lags - would probably be useful, but that's outside the scope of this project.\n\nLet's perform a similar exercise for robbery.\n\n```{r , cache=TRUE}\ndplyr::select(correlate(feat_transform_df)[order(correlate(feat_transform_df)$log_RobberyRPDTranform),], term, log_RobberyRPDTranform)\n```\n\n\n```{r, cache=TRUE}\nmod_burglary <- lm(log_RobberyRPDTranform ~ log_total_robberies  + log_total_burglaries + log_FlatAprt + log_CentralHealingPercent + log_fullTimeStudents, data = feat_transform_df)\nsummary(mod_burglary)\n```\n\nThis is a notably better fit than our burglary model, with our R2 suggesting we now account for over 20% of the variance. The nature of our predictors is also quite different: while we still see a negative relationship with historic crime (with areas of high historic crime experiencing larger relative decreases), there is a positive relationship with both the presence of apartments and central heating.\n\nI suspect some of these features are correlates of deprivation, so I want to create a quick scatter of three - for now we'll do it against median house price, which is definitely deprivation correlated.\n\n```{r, cache=TRUE}\n\nheating <- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_CentralHealingPercent)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\napartments <- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_FlatAprt)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\ncomest <- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_ComEstResTranform)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\n\n\nggarrange(heating, apartments, comest, ncol=3, nrow=1 )\n\n\n\n```\n\nWhile there does appear to be a relationship with some of these, it isn't strong - this suggests the factor's we have identified are significant not because of their association with deprivation and poverty, but because of what they mean about the specific characteristics of the area.\n\n### Random Forests\n\nUnlike regression, random forest doesn't really on any specific type of association - instead, we rely on computing power, repetition and iteration to capture the very best predictor for our variable, in any combination. \n\nThere are risks to this method: our sample size is smaller than I'd like, and this may lead to over-fit of outlier MSOAs.\n\nIt does mean we don't need to worry about transformations or correlations: we can return to our original dataset, and let the model identify the strongest predictors.\n\n```{r , cache=TRUE}\n# we remove rows where our main error is na or missing\nrf_msoa_matrix <- drop_na(msoa_matrix_numeric, RPDRobberyShifted)\n\n#then we repeat for any columns where value asre missing\nclean_rf_matrix <- rf_msoa_matrix[ , colSums(is.na(rf_msoa_matrix)) == 0]\n\n\n#then we drop out any values that directly predict our error.\n\ndrop<- c(\"burglaryActual\",\"burglaryError\",\"burglaryPercentError\",\"burglaryPredicted\",\"robberyActual\",\"robberyPredicted\",\"robberyError\",\"robberyPercentError\", \"RPDBurglaryShifted\", \"RPDRobberyShifted\")\n\n#remove out selected columns\ndata<- clean_rf_matrix[,!(names(clean_rf_matrix) %in% drop)]\n\n\n#automatically remove white space and make names r compatbiel\nnames(clean_rf_matrix)<-make.names(names(clean_rf_matrix),unique = TRUE)\n\ndrop<- c(\"burglaryActual\",\"burglaryError\",\"robberyActual\",\"robberyError\",\"RPDBurglary\",\"RPDRobbery\",\"robberyActualShifted\",\"robberyPredictedShifted\",\"burglaryActualShifted\",\"burglaryPredictedShifted\", \"burglaryPredicted\", \"burglaryPercentError\", \"robberyPredicted\", \"robberyPercentError\")\n#drop<- c(\"burglaryPercentError\",\"burglaryPredicted\",\"robberyPredicted\",\"robberyPercentError\")\n\ndata<- clean_rf_matrix[,!(names(clean_rf_matrix) %in% drop)]\n\n\nnames(data)<- make.names(names(data),unique = TRUE)\nhead(data)\n```\nWe start with modelling our relative rate of burglary shift. We divide our sample into a training set which we'll use to train the model, and a test set we'll use to verify accuracy and validity.\n\n```{r , cache=TRUE}\nset.seed(123)\n\nsample = sample.split(data$RPDburglaryShifted, SplitRatio = 0.7)\ntrain = subset(data, sample == TRUE)\ntest  = subset(data, sample == FALSE)\n\n\nrf_burglary <- randomForest(\n  RPDburglaryShifted ~ .,\n  data=train, \n  importance=TRUE\n)\n\nsummary(rf_burglary)\n```\n```{r , cache=TRUE}\n#calculate our predictions and a  rmse\nprediction <-predict(rf_burglary, test)\nMetrics::rmse(test$RPDburglaryShifted, prediction)\n```\n\nWhile machine learning models were once considered opaque and difficult to interpret, several libraries now offer functionality to explain predictions. Here I use [DALEX](https://cran.r-project.org/web/packages/DALEX/index.html) to do just this. \n\n```{r , cache=TRUE}\n\nrf_explainer_burglary <- DALEX::explain(rf_burglary, data=train, y= train$RPDburglaryShifted)\n\nrf_perf_burg <- model_performance(rf_explainer_burglary)\nrf_perf_burg\n```\nWe can see that our model significantly outperforms our best linear models: the R2 suggests that almost 85% of the variance is correctly interpreted, and our rmse (root mean squared error) is around 0.2 on our test set.\n\nThis model would be ill-suited to prediction or operationalising -it is a default forecast with no hyper-parameter tuning, and no consideration of error rates - but using tools like DALEX, we can identify which predictors the model identifies as most important. \n\n```{r, fig.width = 13 , cache=TRUE}\nmodel_parts_burg <-model_parts(rf_explainer_burglary)\n\n```\n\n```{r, fig.width = 13 , cache=TRUE}\nplot(model_parts_burg, max_vars=25)\n\n```\nThe three most important features our model highlights are:\n- the age composition of the population\n- the number of residents which are in commercial property\n- the historic number of burglaries\n\nThis seems to corroborate our previous regression model. To further unpick  these trends, we can use Partial Dependence Plots to identify how the model prediction shifts with these values. \n\n```{r, cache=TRUE, warning=FALSE, cache.lazy=FALSE}\npdp_b <- model_profile(rf_explainer_burglary)\n\n```\n\n```{r, cache=TRUE}\nplot(pdp_b, variables=\"total_burglaries\")\n\n```\nWe still see a strong association between a high number of historic, and a large reduction during the lockdown period\n\n```{r, cache=TRUE}\nplot(pdp_b, variables= \"Religion..2011..Christian\")\n\n```\n```{r, cache=TRUE}\nhist(data$Religion..2011..Christian)\n```\n\n```{r, cache=TRUE}\nplot(pdp_b, variables=\"ComEstRes\")\n```\n```{r, cache=TRUE}\nhist(msoa_matrix_numeric$ComEstRes)\n```\nBy combining the distribution of commercial residents by MSOA with our PDP, we can see that those MSOAs that are most densely populated by commercial residents see the smallest \"covid decrease\"(suggesting that those MSOAs that are very heavily residential saw the sharpest drops).\n\n```{r, cache=TRUE}\nplot(pdp_b, variables=\"House.Prices.Sales.2008\")\n\n```\n\nFinally, we can see that those areas that experienced the highest volume of house sales experienced the lowest relative decrease in burglary.\n\nThis highlights the importance of identifying correlates in RF models - especially in a dataset where features are highly interlinked, association does not imply causation, and we should be wary of over-interpreting.\n\nWe can now repeat our process for our robbery shift.\n\n\n```{r, cache=TRUE}\nset.seed(123)\n\n\nsample = sample.split(data$RPDRobberyShifted, SplitRatio = 0.75)\ntrain = subset(data, sample == TRUE)\ntest  = subset(data, sample == FALSE)\n\n\nrf_robbery <- randomForest(\n  RPDRobberyShifted ~ .,\n  data=train, \n  importance=TRUE\n)\n\nsummary(rf_robbery)\n\n```\nWe've now trained a model.  Let's now use the DALEX library to understand it, and see how it performs.\n\n```{r, cache=TRUE}\n\nrf_explainer_robbery <- DALEX::explain(rf_robbery, data=train, y= train$RPDRobberyShifted)\n\nrf_perf_rob <- model_performance(rf_explainer_robbery)\nrf_perf_rob\n```\n\n\n\n\n```{r, fig.width = 13,  cache=TRUE, warning=FALSE}\nmodel_parts_rob <-model_parts(rf_explainer_robbery)\n```\n\n```{r, fig.width = 13, cache=TRUE}\nplot(model_parts_rob, max_vars=25)\n```\n\n```{r, cache=TRUE, cache.lazy=FALSE}\npdp_rob <- model_profile(rf_explainer_robbery)\n```\n\n```{r, cache=TRUE}\nplot(pdp_rob, variables=\"total_robberies\")\n\n```\n\n```{r, cache=TRUE}\nplot(pdp_rob, variables=\"total_burglaries\")\n\n```\n\nThe effect of historic crime effects again appears like a reliable predictor of a robbery covid shift: those MSOAs with the highest number of burglaries and robberies see strong decreases in robbery (though the association with burglary is not clear cut, suggesting other interaction effects may be driving this)\n\n```{r, cache=TRUE}\nplot(pdp_rob, variables=\"Road.Casualties.2011.2011.Total\")\n\n```\nRoad casualties is another strong relationship. This could be a proxy for deprivation, but I suggest this is more down to geographic features - road casualties are probably rarer in denser urban environments, most likely to be affected by lockdown.\n\n```{r, cache=TRUE}\nplot(pdp_rob, variables=\"Ethnic.Group..2011.Census..Other.ethnic.group....\")\n```\n```{r, cache=TRUE}\nhist(data$Ethnic.Group..2011.Census..Other.ethnic.group....)\n\n```\n\n\nFinally, we see a demographic predictor linked to \"other ethnic group\". I'm not clear how to interpret this, but it suggests that those MSOAs that are most diverse (and have the largest representation by these ethnic groups) experienced the strongest decreases in robbery during lockdown. \n\nTogether, these analyses suggest that the crime drop for robbery and burglary during national lockdown was significantly affected by distinct local factors. For both offences, historical crime trends play a role, with high crime areas experiencing a relatively larger drop in both burglary and robbery.\n\nBeyond that, the drivers vary for each offence  type: burglary was driven by the composition of the residential population, with heavily residential areas, and areas with a relatively \"stable\" population as measured by low housing sales also saw larger decrease - this is likely due to the increased number of empty residential properties.\n\nRobbery decreases conversely, are well associated with high numbers of road casualties, as well as the ethnic makeup of the local population - this is likely due to an association with denser, more urban areas, with lower street speeds, and a possible link to deprivation, whereby minority population were least able to work from home, and were likely to still present as available targets for robbery.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.39","theme":"cyborg","title-block-banner":true,"title":"Learning R - Exploring the COVID Crime Effect in London","description":" I use public London crime data on robbery and burglary to examine where this COVID crime shift was strongest, and whether any specific drivers or correlates can be identified.  ","date":"5/22/2021","categories":["data-science","forecasting","crime","geospatial"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}