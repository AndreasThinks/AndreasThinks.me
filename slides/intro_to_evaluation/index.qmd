---
title: "Evaluation & Impact in Civic Technology"
subtitle: "Newspeak House 2025"
author: "Andreas Varotsis"
title-slide-attributes:
  data-background-image: ../images/newspeak_house_logo.png
  data-background-size: 10%
  data-background-position: bottom
format:
  revealjs:
    incremental: true
    chalkboard: true
    theme: [default, ../newspeak-theme.scss]
    logo: ../images/newspeak_house_logo.png
    footer: |
      ![](../images/newspeak_house_logo.png){height=40px}
---

# Evaluation & Impact in Civic Technology

> *â€œIf we canâ€™t measure what matters, weâ€™ll keep funding whatâ€™s loudest.â€*

---

## Why Evaluate?

- Builds **credibility and trust** â€” essential in civic tech and policy.
- Turns **stories into strategy** â€” bridges activism and evidence.
- Ensures **resources go where impact is real**, not assumed.
- Translates innovation into **policy adoption**.

::: notes
Open by asking: â€œWho here has had to prove impact to a funder or policymaker?â€
Emphasize that evaluation is about *learning* as much as *accountability*.
:::

---

## What Counts as Evaluation?

**Evaluation â‰  Reporting**

| Reporting | Evaluation |
|------------|-------------|
| What we did | What difference it made |

**Levels:**
- **Outputs** â†’ activities or reach  
- **Outcomes** â†’ behavioural or system change  
- **Impact** â†’ long-term societal effects

> *Evaluation is the bridge from prototype â†’ policy.*

---

## Causality 101

**Goal:** Understand if outcomes happened *because of* your intervention.

- Correlation â‰  causation  
- Counterfactuals matter â€” *what if the program didnâ€™t exist?*

**Example:**  
A civic app increases volunteer sign-ups â€” but so did a local festival that month. Which caused it?

---

## Evaluation Approaches

| Type | Description | Pros | Cons |
|------|--------------|------|------|
| **RCTs** | Randomly assign treatment/control | Strong causal inference | Costly, sometimes unethical |
| **Quasi-experimental** | Natural variation or rollout timing | Often feasible | Harder to prove causality |
| **Observational** | Compare existing data patterns | Easy, fast | Prone to bias |
| **Qualitative / Participatory** | Interviews, ethnography, co-design | Context-rich | Hard to generalize |

::: notes
Ask: â€œWhich methods do you think most civic tech orgs *actually* use â€” and why?â€
:::

---

## Case Study: Sure Start (UK)

- Early evaluations: little measurable impact.  
- Long-term follow-ups: strong benefits in education and health.  
- Demonstrates **value of persistence and longitudinal data.**

**Lesson:** Good programs take time to show impact.

[Sure Start Impact Study â†’](https://ifs.org.uk/publications/sure-start-long-term-benefits)

---

## Case Study: FixMyStreet (mySociety)

- Evaluated via **behavioural data** (reports before/after launch).  
- Found sustained increases in citizen engagement.  
- **Method:** Quasi-experimental (variation by council rollout).

**Lesson:** Behavioural data can be powerful when context is understood.

[mySociety Research â†’](https://research.mysociety.org)

---

## Case Study: CitizenLab

- Combines **platform analytics (quant)** + **partner interviews (qual)**.  
- Found: engagement increases when participants see visible outcomes.  
- Illustrates **value of mixed methods**.

[CitizenLab Impact Report â†’](https://www.citizenlab.co)

---

## Case Study: D.A.R.E. (US)

- Popular anti-drug education program.  
- Evaluations showed **no measurable effect** on drug use.  
- Yet widely funded for decades.

**Lesson:** Popular â‰  effective â€” evidence must challenge assumptions.

[D.A.R.E. Evaluation â†’](https://www.drugabuse.gov/publications/preventing-drug-use-among-children-adolescents/chapter-3-prevention-intervention-programs/)

---

## Quantitative vs Qualitative

| Quantitative | Qualitative |
|---------------|-------------|
| How much, how often | Why and how |
| Numbers, patterns | Stories, meaning |
| Scale | Context |

> **Best practice:** Combine both â€”  
> Quant shows *pattern*, Qual explains *mechanism.*

---

## Designing Meaningful Metrics

**SMART** â†’ Specific, Measurable, Achievable, Relevant, Time-bound.

Avoid **vanity metrics**:
- App downloads â‰  engagement  
- Workshops held â‰  behaviour change  

âœ… **Better metrics:**
- % of residents using civic data tools to contact councils  
- % of participants reporting improved civic confidence  

::: notes
Emphasize â€œoutcomes not outputsâ€ â€” behaviour change, policy shifts, etc.
:::

---

## Linking Evaluation to Policy Impact

- Policymakers respond to **robust, communicable evidence**.
- Civic tech influences policy when:
  - Evaluation aligns with **timing and priorities**.
  - Thereâ€™s a clear **theory of change**.
  - Results are **framed in actionable terms**.

**Example:**  
â€œDigital participation improves local accountabilityâ€ > â€œUsers liked our app.â€

---

## Common Pitfalls

- Evaluating too early or too narrowly.  
- Over-attributing success to tech layer.  
- Ignoring social/economic context.  
- Treating evaluation as funder compliance, not learning.

---

## Discussion: Designing Your Evaluation

ğŸ—£ **Group activity**

1. Pick a civic-tech or policy project you know.  
2. How could it *prove* or *improve* its impact?  
3. What would be a good **metric** or **method**?

> Share 2â€“3 examples with the group.

---

## Key Takeaways

- Evaluation helps civic tech move from **experiment â†’ evidence â†’ policy**.  
- Focus on **causality**: show not just what changed, but *why*.  
- Combine **quant + qual**, rigor + storytelling.  
- **Start small, but start measuring.**

> *â€œWhat gets measured gets improved â€” and what gets understood gets funded.â€*

---

## Further Reading

- **Andrew Leigh**, *Randomistas: How Radical Researchers Changed the World*  
- **Nesta**, *Standards of Evidence*  
- **mySociety Research Reports*  
- **Institute for Fiscal Studies**, *Sure Start Evaluations*  
- **What Works Digital**, *Evaluation Guidance for Digital Services (2024)*

---

## Q&A

- How could you apply these approaches in your projects?  
- Whatâ€™s one evaluation challenge youâ€™ve faced?

---
