[
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Andreas Varotsis",
    "section": "",
    "text": "If you‚Äôd like to discuss anything at all, please get in touch! The form below will send me an email, and I‚Äôll get back to you as soon as I can. \n\n\nYour name \n\n\nYour email \n\n\nYour message"
  },
  {
    "objectID": "slides/intro_to_evaluation/summary.html",
    "href": "slides/intro_to_evaluation/summary.html",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "",
    "text": "NoteüìÖ Session Information\n\n\n\nDate: October 14, 2025\nModule: Public Sector Innovation - Evaluation Track\nSlides: View the presentation ‚Üí"
  },
  {
    "objectID": "slides/intro_to_evaluation/summary.html#session-summary",
    "href": "slides/intro_to_evaluation/summary.html#session-summary",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Session Summary",
    "text": "Session Summary\n\nüß© What Evaluation Is\nWe defined evaluation as more than reporting ‚Äî it‚Äôs not just what we did, but what difference it made.\n\n\n\n\n\n\nImportantKey Distinction\n\n\n\nReporting tells you what happened.\nEvaluation tells you what caused it to happen.\n\n\nEvaluation bridges experimentation and policy by showing causal impact, not just correlation.\n\n\n\n\n\n\nTipüçΩÔ∏è Case Study: Ration Club\n\n\n\nThroughout the session, we used Ration Club as a practical example to explore evaluation concepts:\nThe Challenge: How could we improve Ration Club? What does ‚Äúimproving‚Äù even look like, and how can we test it?\nThe Idea: If we have an interesting idea to encourage people to pair up at Ration Club, how do we test whether it works?\nThis real-world example helped us explore: - Defining meaningful metrics (number of interactions? number of people? quality of connections?) - Finding sources of randomness (randomize by people? by time? by pairs?) - Understanding what we‚Äôre actually trying to measure\n\n\n\n\n\nüéØ The Importance of Good Metrics\nWe explored what makes a metric meaningful:\n\nIt must be measurable, valid, and actionable\nWe distinguished between outputs (activities) and outcomes (behavioural or systemic change)\nWe discussed types of validity and how to test whether a metric actually captures what matters\n\n\n\n\n\n\n\nNoteApplying to Ration Club\n\n\n\nIn our Ration Club context, we asked: Are we looking at number of interactions? Number of people? Duration of conversations? What are we actually trying to get right?\n\n\n\n\n\nüé≤ Randomness and Causality\nWe looked at how randomness or natural variation allows us to identify true impact:\n\nIn RCTs, randomness is designed in (treatment vs.¬†control)\nIn observational studies, we find randomness retrospectively (timing, geography, thresholds)\nWithout variation, we can describe patterns ‚Äî but not prove cause and effect\n\n\n\n\n\n\n\nWarningThe Randomization Challenge\n\n\n\nAt Ration Club, we discussed randomizing by people, but identified how that could go wrong. Could we randomize by time, or pairs? How can we find the randomness we need to evaluate effectively?\n\n\n\n\n\nüî¢ Quantitative and Qualitative Balance\nGood evaluations combine numbers and stories:\n\nQuantitative data reveals patterns and scale\nQualitative data explains mechanisms and meaning\n\nTogether, they form a fuller picture of impact.\n\n\n\n\n\n\nNoteExample\n\n\n\nIf we find an approach that increases minutes of conversation at Ration Club, how do we know if it‚Äôs because of the intervention or something else (like better food that night)? Qualitative data helps us understand the why behind the numbers.\n\n\n\n\n\nüìä Communicating Findings\nWe emphasized that communication is part of the science:\n\nFindings only create change if they‚Äôre shared clearly and credibly\nDifferent audiences need different formats ‚Äî policy briefs, dashboards, or public summaries\nTransparent storytelling builds trust and invites collaboration\n\n\n\n\n\n\n\nTipCommunication Best Practices\n\n\n\n\nKnow your audience (policymakers vs.¬†practitioners vs.¬†public)\nLead with the outcome, not the method\nBe transparent about limitations\nMake data and methods accessible"
  },
  {
    "objectID": "slides/intro_to_evaluation/summary.html#next-steps",
    "href": "slides/intro_to_evaluation/summary.html#next-steps",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "üß™ Next Steps",
    "text": "üß™ Next Steps\nIn the coming weeks, participants will:\n\nDesign their own evaluation project ‚Äî as a group first, then individually\n\nDefine a theory of change\nChoose one good metric\nIdentify potential sources of randomness or variation\nPlan how they‚Äôll communicate findings\n\nJoin the follow-up module: ‚ÄúCore Statistics 101 for Evaluation‚Äù, covering key statistical tools for understanding uncertainty, significance, and effect size"
  },
  {
    "objectID": "slides/intro_to_evaluation/summary.html#further-reading",
    "href": "slides/intro_to_evaluation/summary.html#further-reading",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "üìö Further Reading",
    "text": "üìö Further Reading\n\nEssential Books\n\nRandomistas: How Radical Researchers Changed Our World by Andrew Leigh\nAn accessible introduction to randomized controlled trials and their impact on policy\nFreakonomics: A Rogue Economist Explores the Hidden Side of Everything by Steven D. Levitt and Stephen J. Dubner\nExplores how economic thinking and data can reveal surprising causal relationships\n\n\n\nOnline Resources\n\nCausal Inference for the Brave and True by Matheus Facure\nA comprehensive Python-based guide to causal inference methods (for those ready to dive deep into the technical details)\nNesta‚Äôs Standards of Evidence\nPractical framework for assessing the strength of evidence in social innovation\nmySociety Research\nReal-world civic tech evaluation examples and case studies\n\n\nPart of the Newspeak House 2025-26 series on Evidence, Impact & Innovation."
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#introduction",
    "href": "slides/evaluation_masterclass/index.html#introduction",
    "title": "Evaluation Masterclass",
    "section": "Introduction",
    "text": "Introduction\n\nImportance of Evaluation:\n\nDemonstrates impact, accountability, and guides improvements.\n\nTreatment Effect:\n\nMeasures causal impact of interventions."
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#why-evaluation-matters",
    "href": "slides/evaluation_masterclass/index.html#why-evaluation-matters",
    "title": "Evaluation Masterclass",
    "section": "Why Evaluation Matters",
    "text": "Why Evaluation Matters\n\nCrucial for funding and credibility.\nEssential for learning and continuous improvement.\nHelps organizations focus resources effectively."
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#treatment-effects-why-they-matter",
    "href": "slides/evaluation_masterclass/index.html#treatment-effects-why-they-matter",
    "title": "Evaluation Masterclass",
    "section": "Treatment Effects: Why They Matter",
    "text": "Treatment Effects: Why They Matter\n\nShows causal impact, not just correlation.\nEssential for effective decision-making and resource allocation."
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#challenges-correlation-vs-causation",
    "href": "slides/evaluation_masterclass/index.html#challenges-correlation-vs-causation",
    "title": "Evaluation Masterclass",
    "section": "Challenges: Correlation vs Causation",
    "text": "Challenges: Correlation vs Causation\n\nCorrelation ‚â† causation.\nImportance of a counterfactual (control group).\n\nExample: - Youth employment improvements might coincide with economic growth, not just program impact."
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#rigorous-methods-randomized-controlled-trials-rcts",
    "href": "slides/evaluation_masterclass/index.html#rigorous-methods-randomized-controlled-trials-rcts",
    "title": "Evaluation Masterclass",
    "section": "Rigorous Methods: Randomized Controlled Trials (RCTs)",
    "text": "Rigorous Methods: Randomized Controlled Trials (RCTs)\n\nGold standard for impact evaluation.\nParticipants randomly assigned (treatment/control).\n\nUK Example: - Family Nurse Partnership Scotland Evaluation"
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#rigorous-alternatives-quasi-experimental",
    "href": "slides/evaluation_masterclass/index.html#rigorous-alternatives-quasi-experimental",
    "title": "Evaluation Masterclass",
    "section": "Rigorous Alternatives: Quasi-Experimental",
    "text": "Rigorous Alternatives: Quasi-Experimental\n\nWhen RCTs aren‚Äôt feasible.\nPropensity score matching, difference-in-differences, regression discontinuity.\n\nCase Study: Family Nurse Partnership (FNP), Scotland, using natural timing differences as controls."
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#case-study-1-read-easy-uk",
    "href": "slides/evaluation_masterclass/index.html#case-study-1-read-easy-uk",
    "title": "Evaluation Masterclass",
    "section": "Case Study 1: Read Easy UK",
    "text": "Case Study 1: Read Easy UK\n\nVolunteer-led adult literacy improvement.\nPersonal narratives (e.g., Jay Blades).\n\n\n‚ÄúOver 200 adults gained literacy skills through personalized coaching, enabling life-changing improvements.‚Äù\n\nRead Easy UK"
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#case-study-code-club",
    "href": "slides/evaluation_masterclass/index.html#case-study-code-club",
    "title": "Evaluation Masterclass",
    "section": "Case Study: Code Club",
    "text": "Case Study: Code Club\n\nFree coding clubs for children aged 9-13.\nOver 90% improved in confidence and problem-solving.\n\nNarrative: ‚ÄúCode Club transforms children‚Äôs lives by boosting digital skills and creativity, reaching over 200,000 young people globally.‚Äù\nCode Club Impact"
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#case-study-skills-for-life-strategy",
    "href": "slides/evaluation_masterclass/index.html#case-study-skills-for-life-strategy",
    "title": "Evaluation Masterclass",
    "section": "Case Study: Skills for Life Strategy",
    "text": "Case Study: Skills for Life Strategy\n\nMassive UK adult literacy and numeracy initiative.\nDespite extensive investment, numeracy levels declined.\n\nLesson: Outputs don‚Äôt guarantee outcomes. Importance of targeting and meaningful metrics.\nSkills for Life Evaluation"
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#case-study-sure-start",
    "href": "slides/evaluation_masterclass/index.html#case-study-sure-start",
    "title": "Evaluation Masterclass",
    "section": "Case Study: Sure Start",
    "text": "Case Study: Sure Start\n\nIntegrated child services for disadvantaged families.\nInitial evaluations found minimal impact; long-term studies showed significant positive outcomes.\n\nNarrative: Early struggles can lead to later successes; patience and continued evaluation essential.\nSure Start Impact Study"
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#famous-failure-d.a.r.e.",
    "href": "slides/evaluation_masterclass/index.html#famous-failure-d.a.r.e.",
    "title": "Evaluation Masterclass",
    "section": "Famous Failure: D.A.R.E.",
    "text": "Famous Failure: D.A.R.E.\n\nPopular anti-drug education program.\nRigorous evaluations showed no effect on drug use.\n\nLesson: Popular ‚â† effective. Importance of evidence-based design.\nD.A.R.E. Evaluation"
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#qualitative-vs-quantitative",
    "href": "slides/evaluation_masterclass/index.html#qualitative-vs-quantitative",
    "title": "Evaluation Masterclass",
    "section": "Qualitative vs Quantitative",
    "text": "Qualitative vs Quantitative\n\nQuantitative: Numbers, measurable outcomes.\nQualitative: Stories, feedback, context.\n\nBest Practice: Use both to tell a comprehensive story."
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#selecting-metrics",
    "href": "slides/evaluation_masterclass/index.html#selecting-metrics",
    "title": "Evaluation Masterclass",
    "section": "Selecting Metrics",
    "text": "Selecting Metrics\n\nOutcomes vs outputs.\nSMART criteria (Specific, Measurable, Achievable, Relevant, Time-bound).\n\nExample of Good Metrics: - Percentage increase in civic participation. - Long-term housing retention for homeless interventions."
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#summary-key-takeaways",
    "href": "slides/evaluation_masterclass/index.html#summary-key-takeaways",
    "title": "Evaluation Masterclass",
    "section": "Summary & Key Takeaways",
    "text": "Summary & Key Takeaways\n\nEvaluation demonstrates impact and drives improvement.\nTreatment effects require rigorous methods.\nEffective communication combines data and storytelling.\nAvoid common pitfalls with clear planning and meaningful metrics."
  },
  {
    "objectID": "slides/evaluation_masterclass/index.html#qa-and-discussion",
    "href": "slides/evaluation_masterclass/index.html#qa-and-discussion",
    "title": "Evaluation Masterclass",
    "section": "Q&A and Discussion",
    "text": "Q&A and Discussion\n\nYour experiences?\nHow could these concepts apply to your projects?"
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Evidence and Impact - Module Guide",
    "section": "",
    "text": "This page contains the slide content for the Newspeak House Evidence and Impact module (2025-26)."
  },
  {
    "objectID": "slides/index.html#module-slides-2025-26",
    "href": "slides/index.html#module-slides-2025-26",
    "title": "Evidence and Impact - Module Guide",
    "section": "Module Slides (2025-26)",
    "text": "Module Slides (2025-26)\n\nEvaluation & Impact in Civic Technology | Session Summary (Oct 14, 2025)\nIntroduction to evaluation approaches in civic tech, covering causality, mixed methods, and designing meaningful metrics. Includes case studies from mySociety, CitizenLab, and Sure Start.\nStatistics 101 for Evaluation | Session: Nov 2, 2025\nFoundational statistics for rigorous evaluation: understanding uncertainty, probability, sampling, significance testing, confidence intervals, and statistical power. Includes interactive demonstrations and practical examples from civic tech contexts."
  },
  {
    "objectID": "slides/index.html#previous-years-module-public-sector-innovation-2024-25",
    "href": "slides/index.html#previous-years-module-public-sector-innovation-2024-25",
    "title": "Evidence and Impact - Module Guide",
    "section": "Previous Year‚Äôs Module: Public Sector Innovation (2024-25)",
    "text": "Previous Year‚Äôs Module: Public Sector Innovation (2024-25)\n\nDelivering Innovation in Public Institutions\nIntroduction to the module covering implementation challenges, delivery obstacles, and how to navigate pushback when implementing innovation in public sector organizations.\nEvaluation Masterclass\nDeep dive into rigorous evaluation methods including RCTs, quasi-experimental designs, and treatment effects. Features case studies from Read Easy UK, Code Club, Skills for Life, and D.A.R.E."
  },
  {
    "objectID": "technitos.html",
    "href": "technitos.html",
    "title": "Consulting",
    "section": "",
    "text": "Consulting\nIf you‚Äôd like to hire me to consult or contract on your projects, I‚Äôm always happy to chat: I specialise in applying advanced analytics and data science to operational problems. Get in touch and we‚Äôll go from there!"
  },
  {
    "objectID": "posts/share-crappy-fiddles.html",
    "href": "posts/share-crappy-fiddles.html",
    "title": "My 2025 resolution: share more crappy fiddles (and unfinished research)",
    "section": "",
    "text": "I‚Äôm a big believer of ‚Äúworking in the open‚Äù - open-source, open science, open communication, that sort of thing. It‚Äôs not just that I think this stuff is ethnical or right (though it mostly is), I think it helps build things that really matter.\nBut it‚Äôs also really bloody hard: exposing yourself to the scrutiny of the world (and more specifically, the internet), while doing the extra real, chunky, hard work to put yourself under that withering gaze, really isn‚Äôt something I do naturally‚Ä¶. but a few things in the last few months have reminded me it‚Äôs probably worth it. So here‚Äôs my commitment for 2025: I‚Äôm sharing more unfinished stuff. And maybe, just maybe, that will help me actually finish some bits along the way. \n\n\n\n\n\n\nTipSo what am i sharing?\n\n\n\nSo, I‚Äôm doing two things:\n\nMore pre-prints! I‚Äôm taking all the ‚Äònot quite finished‚Äô research I‚Äôve got, and putting it out there. You can see the first two pieces here\nMore early-work! I‚Äôve decided to start putting my random pieces of research up earlier. You‚Äôll be able to see them all in the ‚Äúwork in progress‚Äù tag here\n\n\n\nSo, why all this faff? I‚Äôm hoping working in the open will help me build more stuff, be better at communicating, and do better science. If you want to see nearly all my arguments articulated far better than I probably will, you can start by watching this excellent video by Todepond, who kicked off a lot of this thinking.\n\n\nBuild more stuff\nTodepond (who also works on Tldraw, which is also very awesome) thinks we should all be sharing more crappy fiddles: that we all spend so long sitting on early and half-finished work, worried about the harsh reaction of public scrutiny, that we miss out on all the incredible benefits of the internets gaze: lots of (sometimes helpful) feedback, and an army of nerds often willing to help build stuff.\nI should probably recognise that I‚Äôve got some pretty chonky privilege here: as a good looking, straight white man, the internet is a lot less harsh and angry at me than it is too others. Plenty of people don‚Äôt have the luxury of putting stuff onto the internet and it not being a horrible, abusive place‚Ä¶ But I‚Äôm going to take Lu‚Äôs inspirational call to arms to heart, and share more stuff.\n\n\nMore, better writing\nTechnical writers I admire share a few things in common: they write often, they write in public, but most importantly they write for themselves. I‚Äôm endlessly inspired by people like Simon or Terence, who are terrifyingly prolific just by regularly, consistently putting stuff into the wild, but also people like Jeremy Howard of fast.ai, who puts a huge emphasis on writing up what you build to help you and others learn.\nWriting in public is hard, and doing it consistently is harder still, but it helps you learn - just like taking notes, putting words into the wild forces you to put them in order in your head‚Ä¶and in the long run, I‚Äôm hoping it will help me write better. So here is to consistent writing in 2025.\n\n\nBetter science\nNow, this might be the most self-agrandising writing I‚Äôve ever done, but bear with me: writing in public is how good science happens.\nI‚Äôve done a fair bit of thinking about the scientific process in the last few years. I started a PhD, which led to most of my work being stuck inside endless drafts while I fretted about while it was ready for publication. I‚Äôve been inspired by machine learning researchers who put everything in pre-prints (though I do wonder who reads them). And meanwhile, Elon and Yann LeCun are duking it out about technical papers on twitter. And more importantly, I‚Äôve seen people building exciting new networks to think about what science could be.\nThe very best science is built on collaborations you never saw coming. It‚Äôs built on connections, networks and communities, on the insights of brilliant humans standing on the shoulders of other excellent humans. Very often, those humans didn‚Äôt meet in the corridors of prestigious universities or in lecture halls, but in cafes and bars, in letters, or even on social media. Communities and networks build innovation.\nLearning (and after all, that‚Äôs what science is) happens to us all together. So share your thoughts with the world. I‚Äôm going to try and do more of it!"
  },
  {
    "objectID": "posts/week-notes/Weeknote - 2024-W34 (2024-08-19).html",
    "href": "posts/week-notes/Weeknote - 2024-W34 (2024-08-19).html",
    "title": "Weeknote - 2024-W34 (2024-08-19)",
    "section": "",
    "text": "Inspired by Simon Willison, and because I‚Äôm starting as Newspeak House faculty this year, I‚Äôm starting to write weekly notes‚Ä¶ let‚Äôs see how they get on!"
  },
  {
    "objectID": "posts/week-notes/Weeknote - 2024-W34 (2024-08-19).html#what-ive-done",
    "href": "posts/week-notes/Weeknote - 2024-W34 (2024-08-19).html#what-ive-done",
    "title": "Weeknote - 2024-W34 (2024-08-19)",
    "section": "What I‚Äôve done",
    "text": "What I‚Äôve done\n\nI‚Äôve written my first post about Copbot Online, an online tool I built to compare human and AI risk perception, with some interesting notes about bias in LLM models\nSpeaking of CopBot, I wrote it using FastHTML, which I‚Äôm really enjoying building with. I promise I‚Äôm not just a Jeremy Howard shill, though his other projects like FastAI and nbdev are all bloody excellent.\nI also made an Obsidian plugin! I write my notes using Obsidian, but my site is managed with Quarto, and swapping one to the other was a pain. Now it‚Äôs magical and automated! Thank god for LLMs.\n\nIt is a teeeeeny bit janky, so apologies for formatting weirdness on this page. Thanks for beta testing?"
  },
  {
    "objectID": "posts/week-notes/Weeknote - 2024-W34 (2024-08-19).html#what-ive-found",
    "href": "posts/week-notes/Weeknote - 2024-W34 (2024-08-19).html#what-ive-found",
    "title": "Weeknote - 2024-W34 (2024-08-19)",
    "section": "What I‚Äôve found",
    "text": "What I‚Äôve found\n\nUV, the rust-based Python package manager and all around pip replacement, has a new version out. I was previously bundling it with Rye, and found it didn‚Äôt quite work in every scenario‚Ä¶ but this looks like it might just be perfect?\n\nZed, the AI coding IDE. Programming was probably the first industry to be meaningfully disrupted by AI, but I do think co-pilot is probably the very fist pass at a bespoke tool‚Ä¶ Like most revolutions, building the infrastructure into our daily practice will take longer than the ‚Äúcore‚Äù discovery."
  },
  {
    "objectID": "posts/week-notes/Weeknote - 2024-W34 (2024-08-19).html#what-ive-read",
    "href": "posts/week-notes/Weeknote - 2024-W34 (2024-08-19).html#what-ive-read",
    "title": "Weeknote - 2024-W34 (2024-08-19)",
    "section": "What I‚Äôve Read",
    "text": "What I‚Äôve Read\n\nPapers\n\nThe Llama 3 Herd of Models\nThis thing is disgustingly long, but worth a browse. Some headline thoughts:\n\nTraining LLMs isn‚Äôt an exploratory science anymore, it‚Äôs now an industry. The amount of thought that goes into scaling and optimising the learning process is faintly terrifying\nArchitecture wise, we‚Äôre still surprisingly‚Ä¶ old school? No mixture of experts, no fancy new rethink, it‚Äôs still a big transformer.\n\n\n\n\nArticles\n\nThe Death of ‚ÄúDeliverism‚Äù\nIt turns out that government just getting stuff done might not be enough to convince people it‚Äôs great. This feels linked to a bunch of articles I‚Äôve been seeing about how the ‚ÄúWest‚Äù needs to rediscover it‚Äôs passion and ability to tell stories and really inspire people again. I‚Äôm yet to be convinced - it feels like pushing back against the enlightenment, and people who feel inspired can do very stupid things.\n\n\n\nBooks\n\nHow to Take Smart Notes\nGod, I wish I could take good note. I can‚Äôt, and maybe this will inspire me. I‚Äôve at least bought a tiny notebook, so that‚Äôs something.\n\nMetadata\n\nAuthor: [[S√∂nke Ahrens]]\nFull Title: How to Take Smart Notes\n\n\n\n\nQuit\nI‚Äôve been pondering when and how you should dump projects recently‚Ä¶if anybody has figured it out, let me know!\n\nMetadata\n\nAuthor: [[Annie Duke]]\nFull Title: Quit"
  },
  {
    "objectID": "posts/national-data-library.html",
    "href": "posts/national-data-library.html",
    "title": "What I want from a National Data Library",
    "section": "",
    "text": "NoteTimed Post\n\n\n\nThis was a timed post, a new approach I‚Äôm testing to post more often: I give myself 60 minutes to write a post, and if I don‚Äôt finish it in time, it gets deleted. You can read all these posts here.\nOnce upon a time, I was a young neighbourhood police officer in London, and I loved it - I think it laid the groundwork for a lot of who I am today. It wasn‚Äôt perfect though, and one part made me specifically grump: ward panel meetings. Every month or so, your local cop sits in a community hall, supposedly to meet with the local community and discuss crime prevention issues. The sad reality is that it tends to be the same 12 people it‚Äôs been forever, and nobody else hears about or engages with it. It‚Äôs a depressing failure of civic engagement.\nSo two weeks ago, when I had to contact my local neighbourhood team, I wondered when their next meeting was. I went to the website, looked up my postcode, etc‚Ä¶. and decided this process should be better. So in an hour of vibe-coding, sitting in a caf√© in Montreal, I built https://yourpolice.events/, which automatically adds upcoming meetings to your calendar feed.\nAt the risk of sounding grandiose, I kind of think that‚Äôs wonderful: I saw a problem with how I could engage with government, and I took an hour to build a tool - just for me - and I ‚Äúfixed‚Äù it. But that was only possible because of an old, slightly fading API of public policing data, built while I was still stomping around South London in a silly top hat.\nI think vibe coding is going to help millions of people like me build millions of things, but well maintained public data APIs will make those things collectively meaningful and useful.\nRather conveniently, the government has committed to launching a National Data Library, and I‚Äôve got a sneaking suspicion they‚Äôre still trying to figure out what that practically is. I don‚Äôt know what it should be, but I think if you‚Äôre going to launch a National Data Library, you could start by rejuvenating the public API landscape."
  },
  {
    "objectID": "posts/national-data-library.html#the-open-data-dream",
    "href": "posts/national-data-library.html#the-open-data-dream",
    "title": "What I want from a National Data Library",
    "section": "The Open Data Dream",
    "text": "The Open Data Dream\nThe Coalition years feel kind of weird to look back on now. In some ways, they kickstarted some of the trends that defined our world: Brexit. Austerity. Seemingly endless angry protests.\nAnd yet, looking back, part of it also feels slightly hopeful? From a tech perspective, it felt like that‚Äôs the last time we really believed the bustling tech scene might transform government. The Big Society may have been na√Øve, but the open data movement that came with it was genuinely ambitious.\nIt‚Äôs hard to think back to now, but the vestiges of those golden years still litter the civic tech landscape, like so many crumbling ruins in a dystopian wasteland. Some I think are pretty much dead, their skeletons poking out of the digital sand. Some limp along, vestiges of their grand ambitions. Others have managed to adapt to this new world.\nAs I discovered in my moment of impromptu building, plenty of public APIs are definitely in the ‚Äúlimping along‚Äù bucket, and data.police.uk is a perfect example. Some forces reliably populate event data, others have mostly stopped bothering. Manchester Police just stopped centrally reporting crime for awhile while it figured out its new IT system.\nI suspect if I checked a few more sources on data.gov.uk, I‚Äôd find a mostly similar story: open data is great, but when austerity bit and it fell out of the political spotlight, departments focused on core services over clean data. And you know what, I can hardly blame them: not that many people knew those datasets even existed, let alone were building on them."
  },
  {
    "objectID": "posts/national-data-library.html#restarting-the-civic-tech-open-data-dream",
    "href": "posts/national-data-library.html#restarting-the-civic-tech-open-data-dream",
    "title": "What I want from a National Data Library",
    "section": "Restarting the Civic Tech Open-Data Dream",
    "text": "Restarting the Civic Tech Open-Data Dream\nMaybe I‚Äôm drinking the Kool-Aid, but I think LLMs and vibe-coding really are the enabler to kick-start the civic tech open-data revolution that never really happened. Building on APIs was actually quite hard - most normal humans never want to figure out what CURL is, or what the difference is between POST, GET, and PUT.\nVibe coding really does shift part of that equation. Of course, you‚Äôre not going to be writing production ready code, but if there was a central, well documented repository of public API data that LLMs knew how to query and use? The barrier for prototyping on it essentially becomes 0, and that could be glorious.\nSo what would it take? I‚Äôd point to a few things off the top of my head.\nFirst, a funding model that makes departments build and maintain open-datasets - I‚Äôm not sure what the answer is, but we should recognise that public data is a public good, and no department should have to pick between investing in public data and front-line services (and of course, it shouldn‚Äôt be acceptable to just stop populating it without good reason).\nSecondly, and this is my hottest take, recognise that your main customer is LLM enabled, and may very well be LLM dependent - that means making it as easy as possible for models/agents to discover and use datasets. LLMs.txt on everything. MCP servers coming out of all the endpoints. Those fancy new Claude skill packages. When I ask ChatGPT to help with anything around building on public technology and data, I want that LLM to just know what data is out there, and how to use it‚Ä¶ But more critically, just good, clean, maintained machine readable documentation (the good news is building that is a fantastic investment, even if all LLMs were to disappear tomorrow).\nFinally, let‚Äôs build a community: you want people to build on that data, and we should make it exciting, collective. Government data is ours, so make it feel that way! Run a million events and hackathons. Hire some goddamn librarians, and task them with building a thriving ecosystem! The UK Data Service has shown you can actually do this, with data conferences and such. Let‚Äôs make playing with government data exciting.\nThe NDL shouldn‚Äôt just be a library‚Ä¶ it should be our library. A public good, which the public uses, and treasures. Not some dusty archive that government maintains out of obligation, but a living resource that sparks curiosity and enables action. When someone wonders about their neighbourhood, their council, their police - the NDL should be where they turn, and where they can actually do something with what they find.\nAnd honestly? I think we‚Äôre at a moment where this becomes genuinely transformative. As digital ID starts becoming a practical thing rather than a concept, we might move to a world where we all can authenticate and build on our own data. Imagine being able to prototype tools for checking your council tax, monitoring your kids‚Äô school performance data, or comparing local healthcare outcomes - not because you‚Äôre a developer, but because you had a question and an LLM could help you answer it using secure, authenticated access to public services. That‚Äôs when civic engagement stops being something 12 people do in a community hall, and becomes something woven into how we all interact with government.\nSo my hot take on the NDL? Turns out we had one all along. We just need to show it a bit of love, give it the funding it deserves, and maybe teach it to speak fluent LLM. The infrastructure is there, rusting in the digital rain. Let‚Äôs bring it back to life."
  },
  {
    "objectID": "posts/quarto_comments/index.html",
    "href": "posts/quarto_comments/index.html",
    "title": "Quarto Comments, Powered by the Fediverse",
    "section": "",
    "text": "I‚Äôve become a big fan of the ‚Äúfediverse‚Äù in the last few years - social media let me find my own delightful community of crime/ai/evidence nerds, and I don‚Äôt want those systems to be any more centralised and out of my control than they are already. I also really like the scientific publishing platform Quarto (that‚Äôs what the site is built on!).\nSo last night, I got wondering how hard it would be to get Mastodon powered comments on this blog. Quarto does support a few native comment options, but predictably, nothing Fediverse compatible‚Ä¶ so that‚Äôs why I built one! It‚Äôs bundled up as a neat little Quarto extension, and the whole thing was actually surprisingly painless.\n\n\n\n\n\nMost of that is thanks to this excellent web-component, which does 90% of the technical heavy lifting - you can read about their implementation in more detail here. To cut a long story short, thanks to the open nature of the Fediverse, it‚Äôs relatively straightforward for some javascript code to get updates on a toot. Thanks to the surprisingly understandable Quarto extensions framework, all you then need is some Lua code to inject the component, along with your chosen variables into your Quarto theme, and after a bit of javascript tweaking and some help from ChatGPT, bob‚Äôs your uncle‚Ä¶ in theory, you should be seeing comments below this post.\nIf you found this useful, leave a comment! I‚Äôd also very much welcome any PRs or contributions on my horrendous code (please help me fix it)."
  },
  {
    "objectID": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html",
    "href": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html",
    "title": "Twitter Sentiment Analysis with FastAI",
    "section": "",
    "text": "The twitter API infuriatingly does not let you pull tweets more than a week old. As such, I‚Äôve had to use a combination of the GetOldTweets library and then Tweepy to extract the biography data. As such, this does need access to the Twitter API and a developer account (though I found that quite easy to get). \n\npip install GetOldTweets3\n\nCollecting GetOldTweets3\n  Downloading https://files.pythonhosted.org/packages/ed/f4/a00c2a7c90801abc875325bb5416ce9090ac86d06a00cc887131bd73ba45/GetOldTweets3-0.0.11-py3-none-any.whl\nRequirement already satisfied: lxml&gt;=3.5.0 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (4.2.6)\nCollecting pyquery&gt;=1.2.10\n  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\nCollecting cssselect&gt;0.7.9\n  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\nInstalling collected packages: cssselect, pyquery, GetOldTweets3\nSuccessfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.1\n\n\n\nimport tweepy\nimport logging\nimport GetOldTweets3 as got\nimport pandas as pd\n\n\nAPI_KEY = \"xx\"\nAPI_SECRET = \"xx\"\nACCESS_TOKEN = \"xx\"\nACCESS_TOKEN_SECRET =\"xx\"\n\nauth = tweepy.OAuthHandler(API_KEY, API_SECRET)\nauth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\napi = tweepy.API(auth)\n\nfrom tweepy import API\n\n\nuser_name = \"@metpoliceuk\"\nq='to:{}'.format(user_name)\n\n\nreplies = api.search(q=q)\n\nIn this case, I‚Äôve put together a query that pulls all tweets directed to the MPS username between set dates. I‚Äôd warn that even using GetOldTweets, pull too many tweets at once and Twitter will kick you out - I found that especially during especially active social media days, I had to add a delay to my code.\n\ntweetCriteria = got.manager.TweetCriteria().setQuerySearch(q)\\\n                                           .setSince(\"2020-08-24\")\\\n                                           .setUntil(\"2020-08-25\")\ntweets = got.manager.TweetManager.getTweets(tweetCriteria)\n\ndf = pd.DataFrame(tweets)\n\ndef get_text(tweet):\n  return tweet.text\n\ndef get_username(tweet):\n  return tweet.username\n\ndf[\"username\"] = df[0].apply(get_username)\ndf[\"text\"] = df[0].apply(get_text)\n\nbios_df = df.copy()\nbios = bios_df.dropna(axis=0, subset=[\"username\"])\n\nlist_of_bios = bios[\"username\"].tolist()\n\ndef get_bio(user):\n  list_of_bios = pd.DataFrame(columns=[\"Users\",\"Bios\"])\n  i = 100\n  while i &lt; len(user):\n    users = user[i-100:i]\n    test = api.lookup_users(screen_names =users)\n    for person in test:\n      list_of_bios = list_of_bios.append({'Users' : person.screen_name , 'Bios' : person.description}, ignore_index=True)\n    i = i + 100\n  users = user[i-100:len(user)]\n  test = api.lookup_users(screen_names =users)\n  for person in test:\n      list_of_bios = list_of_bios.append({'Users' : person.screen_name , 'Bios' : person.description}, ignore_index=True)\n  return list_of_bios\n \nlist_of_new_bios = get_bio(df[\"username\"].tolist())\n\nlist_of_new_bios[\"username\"] = list_of_new_bios[\"Users\"]\ndf_with_bio = df.merge(list_of_new_bios, on=\"username\")\ndf_with_bio = df_with_bio.rename(columns={\"Bios\":\"bio\"})\ndf_with_bio = df_with_bio.drop(\"Users\",axis=1)\n\nWith that all done, you can pull out a list of all your tweets that day, as well a the userbiography.\n\ndf_with_bio\n\n\n\n\n\n\n\n\n0\nusername\ntext\nbio\n\n\n\n\n0\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nClintClease\nHe‚Äôs got smug look on his face. A creature lik...\nAuthor, Designer, Composer, Photographer, Anti...\n\n\n1\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nListhebest2020\nLondon is safe huh?! Judging by the majority o...\nNobody you know. #NLF #TheKLF and #KBF. RTs ar...\n\n\n2\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nrepentandtrust\n@metpoliceuk @UKParliament why are you still a...\n‚úùÔ∏è Christian\\n‚úùÔ∏è Defending Christianity agains...\n\n\n3\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nhoxtonist\nIf you killed someone driving a car, you reall...\n\n\n\n4\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\njoepublic99\n@metpoliceuk @UKSupremeCourt @TheFCA @LibDems ...\nTrue Social Justice ¬¶ Thriving Fair Economy ¬¶ ...\n\n\n...\n...\n...\n...\n...\n\n\n175\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nMornnigGlory\n@metpoliceuk #metpoliceuk\nSelf-Made man - confident reliable honest cari...\n\n\n176\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nMatthewFahey15\nUm, not to sure what to say other than that's ...\nDisability rights advocate, Supporter of the a...\n\n\n177\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nAlfredWintle\nLondon isn‚Äôt safe so stop virtue signalling, s...\nWestern virtues such as democracy, free speech...\n\n\n178\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nPatriciaRickey2\nBut BLM riots allowed to happen\n\n\n\n179\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nMe1Chri\n\nJournalist forever (I care, write, edit, desig...\n\n\n\n\n180 rows √ó 4 columns"
  },
  {
    "objectID": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html#scraping-and-cleaning-tweets",
    "href": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html#scraping-and-cleaning-tweets",
    "title": "Twitter Sentiment Analysis with FastAI",
    "section": "",
    "text": "The twitter API infuriatingly does not let you pull tweets more than a week old. As such, I‚Äôve had to use a combination of the GetOldTweets library and then Tweepy to extract the biography data. As such, this does need access to the Twitter API and a developer account (though I found that quite easy to get). \n\npip install GetOldTweets3\n\nCollecting GetOldTweets3\n  Downloading https://files.pythonhosted.org/packages/ed/f4/a00c2a7c90801abc875325bb5416ce9090ac86d06a00cc887131bd73ba45/GetOldTweets3-0.0.11-py3-none-any.whl\nRequirement already satisfied: lxml&gt;=3.5.0 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (4.2.6)\nCollecting pyquery&gt;=1.2.10\n  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\nCollecting cssselect&gt;0.7.9\n  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\nInstalling collected packages: cssselect, pyquery, GetOldTweets3\nSuccessfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.1\n\n\n\nimport tweepy\nimport logging\nimport GetOldTweets3 as got\nimport pandas as pd\n\n\nAPI_KEY = \"xx\"\nAPI_SECRET = \"xx\"\nACCESS_TOKEN = \"xx\"\nACCESS_TOKEN_SECRET =\"xx\"\n\nauth = tweepy.OAuthHandler(API_KEY, API_SECRET)\nauth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\napi = tweepy.API(auth)\n\nfrom tweepy import API\n\n\nuser_name = \"@metpoliceuk\"\nq='to:{}'.format(user_name)\n\n\nreplies = api.search(q=q)\n\nIn this case, I‚Äôve put together a query that pulls all tweets directed to the MPS username between set dates. I‚Äôd warn that even using GetOldTweets, pull too many tweets at once and Twitter will kick you out - I found that especially during especially active social media days, I had to add a delay to my code.\n\ntweetCriteria = got.manager.TweetCriteria().setQuerySearch(q)\\\n                                           .setSince(\"2020-08-24\")\\\n                                           .setUntil(\"2020-08-25\")\ntweets = got.manager.TweetManager.getTweets(tweetCriteria)\n\ndf = pd.DataFrame(tweets)\n\ndef get_text(tweet):\n  return tweet.text\n\ndef get_username(tweet):\n  return tweet.username\n\ndf[\"username\"] = df[0].apply(get_username)\ndf[\"text\"] = df[0].apply(get_text)\n\nbios_df = df.copy()\nbios = bios_df.dropna(axis=0, subset=[\"username\"])\n\nlist_of_bios = bios[\"username\"].tolist()\n\ndef get_bio(user):\n  list_of_bios = pd.DataFrame(columns=[\"Users\",\"Bios\"])\n  i = 100\n  while i &lt; len(user):\n    users = user[i-100:i]\n    test = api.lookup_users(screen_names =users)\n    for person in test:\n      list_of_bios = list_of_bios.append({'Users' : person.screen_name , 'Bios' : person.description}, ignore_index=True)\n    i = i + 100\n  users = user[i-100:len(user)]\n  test = api.lookup_users(screen_names =users)\n  for person in test:\n      list_of_bios = list_of_bios.append({'Users' : person.screen_name , 'Bios' : person.description}, ignore_index=True)\n  return list_of_bios\n \nlist_of_new_bios = get_bio(df[\"username\"].tolist())\n\nlist_of_new_bios[\"username\"] = list_of_new_bios[\"Users\"]\ndf_with_bio = df.merge(list_of_new_bios, on=\"username\")\ndf_with_bio = df_with_bio.rename(columns={\"Bios\":\"bio\"})\ndf_with_bio = df_with_bio.drop(\"Users\",axis=1)\n\nWith that all done, you can pull out a list of all your tweets that day, as well a the userbiography.\n\ndf_with_bio\n\n\n\n\n\n\n\n\n0\nusername\ntext\nbio\n\n\n\n\n0\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nClintClease\nHe‚Äôs got smug look on his face. A creature lik...\nAuthor, Designer, Composer, Photographer, Anti...\n\n\n1\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nListhebest2020\nLondon is safe huh?! Judging by the majority o...\nNobody you know. #NLF #TheKLF and #KBF. RTs ar...\n\n\n2\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nrepentandtrust\n@metpoliceuk @UKParliament why are you still a...\n‚úùÔ∏è Christian\\n‚úùÔ∏è Defending Christianity agains...\n\n\n3\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nhoxtonist\nIf you killed someone driving a car, you reall...\n\n\n\n4\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\njoepublic99\n@metpoliceuk @UKSupremeCourt @TheFCA @LibDems ...\nTrue Social Justice ¬¶ Thriving Fair Economy ¬¶ ...\n\n\n...\n...\n...\n...\n...\n\n\n175\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nMornnigGlory\n@metpoliceuk #metpoliceuk\nSelf-Made man - confident reliable honest cari...\n\n\n176\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nMatthewFahey15\nUm, not to sure what to say other than that's ...\nDisability rights advocate, Supporter of the a...\n\n\n177\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nAlfredWintle\nLondon isn‚Äôt safe so stop virtue signalling, s...\nWestern virtues such as democracy, free speech...\n\n\n178\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nPatriciaRickey2\nBut BLM riots allowed to happen\n\n\n\n179\n&lt;GetOldTweets3.models.Tweet.Tweet object at 0x...\nMe1Chri\n\nJournalist forever (I care, write, edit, desig...\n\n\n\n\n180 rows √ó 4 columns"
  },
  {
    "objectID": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html#coding-and-learning",
    "href": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html#coding-and-learning",
    "title": "Twitter Sentiment Analysis with FastAI",
    "section": "Coding and Learning",
    "text": "Coding and Learning\nThis is the painful bit: I had to manually go through a few hundred tweets and manually label them as ‚ÄúRight Wing‚Äù or not based on my own hunches.\nI went with the below\n\nAnti-immigration sentiment\nAnti-socialism/BLM\nPro-Brexit/Anti-EU\nPro-Trump/MAGA\n\nThe coded set can be found below.\n\nimport pandas as pd\nfrom fastai.text.all import *\n\n\ntraining = pd.read_csv(\"/content/drive/My Drive/data/sentiment training set.csv\")\ntraining\n\n\n\n\n\n\n\n\nusername\nbio\nRW\nis_valid\n\n\n\n\n0\nRichardSTOCKDA4\nRUGBY LOVING EX PLAYER NOW ONE OF THE PROUD RVS/NHS VOLUNTEERING TEAM ACTIVELY SEEKING OUT/EXPOSING GROOMING GANG MEMBERS\n1\n1\n\n\n1\ncold957\n#Veteran. 1st Bn Coldstream Guards/Guards Para.\\nWe can forgive a child who is afraid of the dark. The real tragedy of life is when men fear the light.\\nNO DMs.\n1\n1\n\n\n2\nleftwant2brite\nBe aware of leftwing politics, Jihadists & militant Communists like those in ANTIFA. fascism is totalitarianism born from socialism. It's also an economic systüßê\n1\n1\n\n\n3\nHermioneMidwife\nDigital Midwife at the RCM and King‚Äôs College Hospital. Women centred, clinically led, digitally driven. FNF Scholar\n0\n1\n\n\n4\nscoutingfamily\nProfessional Football Scout & Player Recruitment Specialist \\n@england @stokecity \\nEngland U17 World Cup Winners üèÜü•á\n0\n1\n\n\n...\n...\n...\n...\n...\n\n\n502\nAlfredWintle\nWestern virtues such as democracy, free speech, equal & human rights need to be preserved & protected before it‚Äôs too late. Love Europe & therefore hate the EU.\n1\n1\n\n\n503\nMe1Chri\nJournalist forever (I care, write, edit, design, research and study, cook, volunteer). Project Editor. Kingston University, London.\n0\n1\n\n\n504\nUK Cop Humour\nSimply trying to show the human side of our fantastic Police Officers & raise a bit of money for good causes whilst we're at it! RT's are not an endorsement.\n0\n1\n\n\n505\nCopThatCooks\nDetective with \\n@WMerciaPolice\\n working in #Worcester CID . Also tweeting as \\n@HistoryCop\\n. Here to engage, explain and encourage.\n0\n1\n\n\n506\nNorthern_Bobby\nTraffic Sgt on a Roads Policing Team somewhere in the UK. Passionate about Traffic and Road SafteyMen holding handsRainbow flag #Taser #TPAC\n0\n1\n\n\n\n\n507 rows √ó 4 columns\n\n\n\nFastAI includes the very convenient concept of a ‚ÄúDataLoader‚Äù: a python object that combines all the data you‚Äôre working with, split into a training and test set, in a format it can work with.\nFor text classication, that object does a lot of the heavy lifting for you. NLP normally requires an awful lot of data-cleaning: you break your text into words, then add special characters to capitalisation, sentence starts, etc. As you can see below, FastAI has done all of that in a few lines of code. The down-side is, I‚Äôm not hugely clear exactly what it‚Äôs done, or how I can now read my text.\n\ntraining = training.dropna(axis=0)\ntraining[\"RW\"] = training[\"RW\"].astype(\"boolean\")\ntraining[\"is_valid\"] = training[\"is_valid\"].astype(\"boolean\")\ndls = TextDataLoaders.from_df(training, text_col = \"bio\", label_col=\"RW\")\ndls.show_batch()\n\n\n\n\n\n\n\n\ntext\ncategory\n\n\n\n\n0\nxxbos xxmaj xxunk xxmaj united , xxmaj welsh xxmaj rugby , xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk , xxmaj the xxmaj xxunk , xxmaj small xxmaj xxunk , xxmaj the xxmaj who , xxmaj the xxmaj xxunk , xxmaj the xxmaj xxunk , xxmaj xxunk , xxmaj xxunk + xxmaj the xxmaj xxunk\nFalse\n\n\n1\nxxbos xxmaj ex xxunk . xxmaj xxunk . xxmaj xxunk living in xxmaj london . xxmaj england great . xxmaj dislike xxup xxunk . \\n\\n ( xxunk : xxmaj xxunk xxmaj xxunk xxmaj xxunk xxmaj xxunk , xxup xxunk , & xxunk ) \\n ( small : xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxup xxunk ) xxpad xxpad xxpad xxpad xxpad\nFalse\n\n\n2\nxxbos xxmaj peace , i am xxmaj xxunk , \\n xxmaj xxunk xxmaj xxunk xxmaj xxunk xxmaj xxunk \\n xxmaj xxunk xxmaj xxunk xxmaj xxunk xxmaj xxunk xxmaj xxunk \\n xxmaj follow xxmaj me xxmaj for xxmaj follow xxmaj back \\n xxmaj retweet xxmaj me xxmaj for xxmaj follow xxmaj back \\n xxmaj xxunk xxmaj love xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\n\n\n3\nxxbos xxmaj loves xxmaj xxunk , xxmaj cats , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk , xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj the xxmaj xxunk , \\n xxmaj xxunk & xxmaj xxunk . \\n xxmaj art xxunk xxunk - https : / / t.co / xxunk üôè xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\n\n\n4\nxxbos xxmaj xxunk me know when the xxunk xxunk , \\n xxmaj no xxmaj xxunk xxmaj brexit \\n xxmaj if its on the xxup bbc its not true , \\n‚ñÅ # defundthebbc \\n‚ñÅ # xxup maga , # xxup mbga , # xxunk , # xxunk \\n i xxunk not xxunk of xxmaj xxunk , xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\n\n\n5\nxxbos xxmaj love my xxunk xxmaj xxunk fc , xxmaj football , xxmaj xxunk , xxmaj xxunk , xxmaj xxunk and xxmaj xxunk . xxmaj but i love nothing more than my family and xxmaj xxunk xxmaj xxunk xxunk Ô∏è # xxup lfc # xxup xxunk # xxup ynwa \\n\\n xxmaj xxunk - xxmaj xxunk xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\n\n\n6\nxxbos xxmaj xxunk . xxup xxunk xxmaj member . xxup xxunk xxmaj member , xxmaj xxunk xxrep 3 i xxmaj member , xxmaj xxunk xxmaj member , xxmaj xxunk not xxmaj xxunk , # xxup xxunk , # xxup xxunk , # xxup xxunk , # xxup xxunk , üá¨ üáß üá® xxunk xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\n\n\n7\nxxbos xxmaj xxunk xxmaj west xxunk live for xxunk xxunk . xxmaj xxunk xxmaj xxunk xxmaj xxunk , hate xxmaj london xxmaj xxunk . xxmaj still a xxup xxunk xxunk block xxunk . xxunk Ô∏è # xxup xxunk # xxmaj xxunk xxrep 3 xxunk # xxunk # xxmaj brexit # xxmaj england xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\n\n\n8\nxxbos # xxmaj xxunk . 1st xxmaj xxunk xxmaj xxunk xxmaj xxunk / xxmaj xxunk xxmaj xxunk . \\n xxmaj we can xxunk a xxunk who is afraid of the xxunk . xxmaj the real xxunk of life is when men fear the light . \\n xxup no dms . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nTrue\n\n\n\n\n\nThis is the fun bit: learning. Notice hear we aren‚Äôt starting from scracth: we call the ‚Äúfine_tune‚Äù function, because fastAI already contains a model trained on text.\n\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(5)\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.881830\n0.655739\n0.861386\n00:12\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.825345\n0.595676\n0.851485\n00:34\n\n\n1\n0.796265\n0.541380\n0.891089\n00:29\n\n\n2\n0.784324\n0.509539\n0.881188\n00:28\n\n\n3\n0.764621\n0.497642\n0.871287\n00:30\n\n\n4\n0.742817\n0.503554\n0.801980\n00:29\n\n\n\n\n\n\nlearn.show_results()\n\n\n\n\n\n\n\n\ntext\ncategory\ncategory_\n\n\n\n\n0\nxxbos xxmaj xxunk xxmaj of # xxunk xxmaj with xxmaj old # xxup xxunk xxmaj xxunk / s , & # xxup xxunk / # xxup xxunk , u xxup must xxup be xxunk + xxunk xxup me . xxup i 'm xxup open & xxup xxunk xxup about xxup my # xxup xxunk , # xxup xxunk , & xxup life xxunk # xxup xxunk\nFalse\nFalse\n\n\n1\nxxbos xxmaj bye xxmaj bye xxup eu xxunk xxmaj xxunk so xxunk xxmaj i xxunk only xxunk % xxmaj english with xxunk % xxmaj welsh , xxmaj irish , xxmaj xxunk + little bit of xxmaj xxunk . that ‚Äôll be the xxmaj xxunk side . xxmaj xxunk ; i xxunk back ! xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nTrue\nTrue\n\n\n2\nxxbos xxmaj xxunk 1 xxmaj xxunk since ' xxunk ; xxup xxunk xxmaj football xxmaj referee ; xxup xxunk xxmaj referee xxmaj xxunk ; xxup xxunk ; xxmaj xxunk - time xxmaj xxunk & xxmaj xxunk xxmaj xxunk . i may xxunk with you but will never xxunk you ! xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\nFalse\n\n\n3\nxxbos xxmaj xxunk , the xxunk , xxunk xxunk . xxmaj xxunk xxmaj xxunk . ' i do n't agree xxunk / what u have 2 say , but xxmaj i 'll xxunk 2 xxunk xxunk right 2 say it . ' xxmaj xxunk xxmaj xxunk xxmaj xxunk xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\nFalse\n\n\n4\nxxbos xxmaj xxunk up with xxmaj all the xxmaj politicians xxunk to us . \\n xxunk \\n xxmaj do nt xxunk me off about xxmaj snowflakes of the world . \\n\\n xxmaj xxunk when xxmaj i 'm out xxunk my bike xxunk or walking my dog üê∂ xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\nFalse\n\n\n5\nxxbos xxmaj xxunk with xxup me / xxmaj xxunk 20 xxunk . xxmaj loves - xxmaj xxunk ( my xxunk ) xxunk xxmaj my xxunk , mum , xxunk family . xxmaj dogs ( more than people ! ) üê∂ \\n xxmaj xxunk / xxmaj xxunk xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\nFalse\n\n\n6\nxxbos xxmaj xxunk for xxmaj xxunk , xxmaj xxunk of xxmaj xxunk xxmaj xxunk xxmaj trust . xxmaj xxunk xxmaj xxunk xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj face xxmaj uk , xxmaj supports our xxmaj armed xxmaj forces and our xxmaj emergency services . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nTrue\nFalse\n\n\n7\nxxbos xxmaj xxunk in xxmaj xxunk xxmaj xxunk . xxmaj first xxmaj xxunk xxunk xxunk from xxmaj xxunk . xxmaj xxunk xxunk xxmaj may 1st xxunk . xxunk xxmaj xxunk to xxmaj prem , back to xxunk , back to xxmaj prem in xxunk years ! xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\nFalse\n\n\n8\nxxbos xxmaj xxunk . xxmaj author . xxmaj xxunk with the xxunk from xxmaj xxunk . xxmaj xxunk . xxmaj retweet / follow does n't xxunk xxunk - i use xxunk a lot . xxmaj xxunk but not xxmaj xxunk . xxup xxunk . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad\nFalse\nFalse\n\n\n\n\n\nNow, we can start predicting! FastAI will take any bundle of text and tell you whether it thinks it‚Äôs right wing or not (based on my coding). If I‚Äôm entirely honest, this doesn‚Äôt work fantastically - it seems to have decided that the phrase ‚ÄúJustice for‚Äù is essentially a right wing calling card, which I‚Äôm not entirely comfortable with. That said, the process works! Ish.\n\nlearn.predict(\"Justice for our Brexit\")\n\n\n\n\n('True', tensor(1), tensor([0.3523, 0.6477]))\n\n\n\nlearn.predict(\"Hate snowflakes, socialists, and the EU\")\n\n\n\n\n('True', tensor(1), tensor([0.2483, 0.7517]))\n\n\n\nlearn.predict(\"I love puppies and stuff\")\n\n\n\n\n('False', tensor(0), tensor([0.5286, 0.4714]))\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()"
  },
  {
    "objectID": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html#the-prototype",
    "href": "posts/FastAI_Twitter_Sentiment_Analysis_Tutorial/FastAI_Twitter_Sentiment_Analysis_Tutorial.html#the-prototype",
    "title": "Twitter Sentiment Analysis with FastAI",
    "section": "The Prototype",
    "text": "The Prototype\nSo, can we convert all this into a working application? Sure! Ish. My combining my tweet extractor with sentiment analysis of the tweet text itself (I found this medium blog post very helpful) we can analyse for any specific day, the volume of tweets flagged as ‚Äúright wing‚Äù, and contrast them to the overall messaging, and compare their ‚ÄúSubjectivity (how subjective or opinionated the text is ‚Äî a score of 0 is fact, and a score of +1 is very much an opinion) and the other to get the tweets called Polarity (how positive or negative the text is, ‚Äî score of -1 is the highest negative score, and a score of +1 is the highest positive score)‚Äù.\nWhile I would have loved to deploy this to Heroku or Binder using Voila, the Pytorch text classification model annoying takes far over 500mb of space, so neither free option will support it - it does look like you can chose to use the CPU pytorch option instead, but frankly it‚Äôs painful to get the implementation working!\n\nHelper Functions\nWe start by bringing together all my previous code (as well as some rough sentiment analysis of the text itself) into one chunk, then added widgets.\n\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\nimport plotly.express as px\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\nfrom ipywidgets import *\n\n\nimport tweepy\nimport logging\nfrom tweepy import API\n\nauth = tweepy.OAuthHandler(API_KEY, API_SECRET)\nauth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\napi = tweepy.API(auth)\n\nlearn_inf = learn\n\n#Tweet cleaning helper function\ndef cleanTxt(text):\n text = re.sub('@[A-Za-z0‚Äì9]+', '', text) #Removing @mentions\n text = re.sub('#', '', text) # Removing '#' hash tag\n text = re.sub('RT[\\s]+', '', text) # Removing RT\n text = re.sub('https?:\\/\\/\\S+', '', text) # Removing hyperlink\n \n return text\n\n#helper function that pulls the biography of a username using the Twitter api\ndef get_bio(user):\n  list_of_bios = pd.DataFrame(columns=[\"Users\",\"Bios\"])\n  i = 100\n  while i &lt; len(user):\n    users = user[i-100:i]\n    test = api.lookup_users(screen_names =users)\n    for person in test:\n      list_of_bios = list_of_bios.append({'Users' : person.screen_name , 'Bios' : person.description}, ignore_index=True)\n    i = i + 100\n  users = user[i-100:len(user)]\n  test = api.lookup_users(screen_names =users)\n  for person in test:\n      list_of_bios = list_of_bios.append({'Users' : person.screen_name , 'Bios' : person.description}, ignore_index=True)\n  return list_of_bios\n\n\ndef get_label(row):\n  return row[0]\n\ndef getSubjectivity(text):\n   return TextBlob(text).sentiment.subjectivity\n\n# Create a function to get the polarity\ndef getPolarity(text):\n   return  TextBlob(text).sentiment.polarity\n\ndef cleanTxt(text):\n text = re.sub('@[A-Za-z0‚Äì9]+', '', text) #Removing @mentions\n text = re.sub('#', '', text) # Removing '#' hash tag\n text = re.sub('RT[\\s]+', '', text) # Removing RT\n text = re.sub('https?:\\/\\/\\S+', '', text) # Removing hyperlink\n \n return text\n\n\nlist_of_new_bios = get_bio(df[\"username\"].tolist())\n\n\ndef run_process(user_name, date_from, date_to):\n\n  q='to:{}'.format(user_name)\n\n  tweetCriteria = got.manager.TweetCriteria().setQuerySearch(q)\\\n                                            .setSince(date_from)\\\n                                            .setUntil(date_to)\n  tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n\n  df = pd.DataFrame(tweets)\n\n  def get_text(tweet):\n    return tweet.text\n\n  def get_username(tweet):\n    return tweet.username\n\n  df[\"username\"] = df[0].apply(get_username)\n  df[\"text\"] = df[0].apply(get_text)\n\n  bios_df = df.copy()\n  bios = bios_df.dropna(axis=0, subset=[\"username\"])\n  bios = bios.drop_duplicates(subset=[\"username\"])\n\n  auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n  auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\n  api = tweepy.API(auth)\n\n  list_of_new_bios = get_bio(df[\"username\"].tolist())\n\n  list_of_new_bios[\"RW_prediction\"] = list_of_new_bios[\"Bios\"].apply(learn_inf.predict)\n\n  list_of_new_bios[\"RW_prediction\"] = list_of_new_bios[\"RW_prediction\"].apply(get_label)\n\n  list_of_new_bios = list_of_new_bios.rename({\"Users\":\"username\"}, axis=1)\n\n  new_df = df.merge(list_of_new_bios, on=[\"username\"])\n\n\n\n  # Clean the tweets\n  new_df['clean_text'] = new_df['text'].apply(cleanTxt)\n\n    # Create two new columns 'Subjectivity' & 'Polarity'\n  new_df['Subjectivity'] = new_df['clean_text'].apply(getSubjectivity)\n  new_df['Polarity'] = new_df['clean_text'].apply(getPolarity)\n\n  right_wing_mask = new_df[\"RW_prediction\"] == \"True\"\n  right_wing = new_df[right_wing_mask]\n\n  not_right_wing_mask = new_df[\"RW_prediction\"] == \"False\"\n  not_right_wing = new_df[not_right_wing_mask]\n\n  fig2 = px.scatter(new_df, x=\"Subjectivity\", y=\"Polarity\", color=\"RW_prediction\", trendline=\"ols\", hover_data=[\"Bios\"])\n  fig2.update_xaxes(range=[-0.1,1.1])\n  fig2.update_yaxes(range=[-1.1, 1.1])\n\n  grouped_by_RW = new_df.groupby(\"RW_prediction\").agg(\n    count= (\"text\", len),\n    Subjectivity = (\"Subjectivity\", np.mean),\n    Polarity = (\"Polarity\", np.mean))\n  \n  fig1 = px.bar(grouped_by_RW, y=\"count\", color=grouped_by_RW.index)\n\n  final = round(grouped_by_RW[\"count\"].iloc[1] / grouped_by_RW[\"count\"].iloc[0],3) * 100\n\n  return grouped_by_RW, fig1, fig2, final\n\n\nusername_select =widgets.Text(\n    value='@metpoliceuk',\n    placeholder='Type something',\n    description='Username:',\n    disabled=False\n)\n\ndate_from_widget =widgets.Text(\n    value='2020-08-27',\n    placeholder='Type something',\n    description='Date From:',\n    disabled=False\n)\n\ndate_to_widget =widgets.Text(\n    value='2020-08-28',\n    placeholder='Type something',\n    description='Date To:',\n    disabled=False\n)\n\ndef show_graph2(change):\n  with out_df:\n    print(\"analysing..\")\n  results = run_process(username_select.value, date_from_widget.value, date_to_widget.value)\n  out_df.clear_output()\n  out_analysis.clear_output()\n  out_graph1.clear_output()\n  out_graph2.clear_output()\n  with out_df:\n    print(str(results[3]) + \"% RW interactions\")\n  with out_analysis:\n    print(results[0])\n  with out_graph1:\n    results[1].show()\n  with out_graph2:\n    results[2].show()\n  \nanalyse_button = widgets.Button(description='Analyse')\n\nanalyse_button.on_click(show_graph2)\n\nout_df = widgets.Output(layout={'border': '1px solid black'})\nout_analysis = widgets.Output(layout={'border': '1px solid black'})\nout_graph1 = widgets.Output(layout={'border': '1px solid black'})\nout_graph2 = widgets.Output(layout={'border': '1px solid black'})\n\n\n\nThe Product\nFinally, my working product - given a user-name and two dates, it will pull all tweets on those dates, run the sentiment analysis, show the ratio of right-wing tweets, and the sentiment on those. Here I‚Äôve specifically picked the day Dawn Butler was stopped by police, and notice how right wing\n\n'''VBox([widgets.Label('Tweet Monitor'),\n      username_select, date_from_widget, date_to_widget, analyse_button, out_df, out_analysis, out_graph1, out_graph2])'''\n\n\"VBox([widgets.Label('Tweet Monitor'),\\n      username_select, date_from_widget, date_to_widget, analyse_button, out_df, out_analysis, out_graph1, out_graph2])\"\n\n\n\ndef run_process_2(user_name, date_from, date_to):\n\n  q='to:{}'.format(user_name)\n\n  tweetCriteria = got.manager.TweetCriteria().setQuerySearch(q)\\\n                                            .setSince(date_from)\\\n                                            .setUntil(date_to)\n  tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n\n  df = pd.DataFrame(tweets)\n\n  def get_text(tweet):\n    return tweet.text\n\n  def get_username(tweet):\n    return tweet.username\n\n  df[\"username\"] = df[0].apply(get_username)\n  df[\"text\"] = df[0].apply(get_text)\n\n  bios_df = df.copy()\n  bios = bios_df.dropna(axis=0, subset=[\"username\"])\n  bios = bios.drop_duplicates(subset=[\"username\"])\n\n  auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n  auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\n  api = tweepy.API(auth)\n\n  list_of_new_bios = get_bio(df[\"username\"].tolist())\n\n  list_of_new_bios[\"RW_prediction\"] = list_of_new_bios[\"Bios\"].apply(learn_inf.predict)\n\n  list_of_new_bios[\"RW_prediction\"] = list_of_new_bios[\"RW_prediction\"].apply(get_label)\n\n  list_of_new_bios = list_of_new_bios.rename({\"Users\":\"username\"}, axis=1)\n\n  new_df = df.merge(list_of_new_bios, on=[\"username\"])\n\n\n\n  # Clean the tweets\n  new_df['clean_text'] = new_df['text'].apply(cleanTxt)\n\n    # Create two new columns 'Subjectivity' & 'Polarity'\n  new_df['Subjectivity'] = new_df['clean_text'].apply(getSubjectivity)\n  new_df['Polarity'] = new_df['clean_text'].apply(getPolarity)\n\n  right_wing_mask = new_df[\"RW_prediction\"] == \"True\"\n  right_wing = new_df[right_wing_mask]\n\n  not_right_wing_mask = new_df[\"RW_prediction\"] == \"False\"\n  not_right_wing = new_df[not_right_wing_mask]\n\n  fig2 = px.scatter(new_df, x=\"Subjectivity\", y=\"Polarity\", color=\"RW_prediction\", trendline=\"ols\", hover_data=[\"Bios\"])\n  fig2.update_xaxes(range=[-0.1,1.1])\n  fig2.update_yaxes(range=[-1.1, 1.1])\n\n  import seaborn as sns; sns.set()\n  fig2 = sns.jointplot(new_df[\"Polarity\"], new_df[\"Subjectivity\"], kind=\"hex\", height=7, space=0)\n  fig2.fig.subplots_adjust(top=0.9)\n  fig2.fig.suptitle(\"All Tweets\")\n\n\n  grouped_by_RW = new_df.groupby(\"RW_prediction\").agg(\n    count= (\"text\", len),\n    Subjectivity = (\"Subjectivity\", np.mean),\n    Polarity = (\"Polarity\", np.mean))\n  \n  fig1 = sns.jointplot(right_wing[\"Polarity\"], right_wing[\"Subjectivity\"], kind=\"hex\", height=7, space=0)\n  fig1.fig.subplots_adjust(top=0.9)\n  fig1.fig.suptitle(\"Right Wing Tweets\")\n\n  final = round(grouped_by_RW[\"count\"].iloc[1] / grouped_by_RW[\"count\"].iloc[0],3) * 100\n\n  return final, fig1, fig2\n\n\nfinal, fig1, fig2 = run_process_2(\"@MetPoliceUK\", \"2020-08-09\",\"2020-08-10\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfinal, fig1, fig2 = run_process_2(\"@GuidoFawkes\", \"2020-08-09\",\"2020-08-10\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfinal, fig1, fig2 = run_process_2(\"@OwenJones84\", \"2020-08-09\",\"2020-08-10\")"
  },
  {
    "objectID": "posts/cccamp23/index.html",
    "href": "posts/cccamp23/index.html",
    "title": "Hackers aren‚Äôt excited about AI (and other bits from CCCamp23)",
    "section": "",
    "text": "Honestly, this wasn‚Äôt me.\n\n\nSo I‚Äôm sitting in Berlin airport, waiting to get home from Chaos Communication Camp 2023. It was, to put it bluntly, bloody fabulous‚Ä¶ and my god, I‚Äôve missed hanging out with a whole field of awesome nerds.\nThe enforced loneliness of the pandemic made me do a fair bit of reflecting on which social interactions I valued, and which I‚Äôm really not particularly fussed about, and community events organised by wholesome geeky folk to share what they‚Äôre passionate about are absolutely in the former. The younger me who nervously made his way to EMF Camp 2014 all by himself had no idea what he was signing up for, but I‚Äôll be forever glad I did (and I‚Äôm still bitter we‚Äôll never get another Nine Worlds).\n\n\n\nCamping with South London Makerspace at EMF Camp 2014\n\n\nBut with that gushing out of the way, here are a few interesting lessons I picked up this year‚Ä¶and I‚Äôll hopefully see everyone again for EMF 2024!\n\nThe hackers aren‚Äôt excited by AI‚Ä¶\nNow, this is far from universal given CCC are a quite distinct, very German subset of the hacker community (and DEFCON last week definitely had some LLM excitement), but the lack of LLM talks in the schedule was stark‚Ä¶ and and when they were mentioned, it was mostly critical. In hindsight, this isn‚Äôt so surprising given actual open-source LLMs essentially don‚Äôt exist and are dominated by US behemoths.\n\n\n‚Ä¶ but they‚Äôre still excited by SPACE\nHonestly, I was not ready for the amount of excitement about space. I‚Äôd mostly forgotten Starlink was a thing, but there was real enthusiasm for the impact of reduced cheap satelite launches. Given I know bugger all about space launches, microsats and so on, it was striking to meet more people who wanted to talk about Planet, OneWeb and Relativity Space than OpenAI.\n\n\nTwitter is dead, long live the Fediverse\nI moved over to Mastodon earlier this year, but it‚Äôs at times been tough to use it as my primary social media given how few of my colleagues have followed‚Ä¶annoyingly, Twitter still served up more relevant content, more frequently. Not so at camp: #CCCamp23 was buzzing during the entire event. Again, I‚Äôm not sure how much of this comes down to Europe versus the US (with the German government hosting it‚Äôs own server) but especially given just how much the AI and machine learning communities remained glue to the birdsite, this was striking.\n\n\nPeople are angry about digital identity\nNow, yes, Germany is especially privacy focused (and I still think paying for everything with cash is mad), but European efforts to introduce digital identity got a fair few mentions, and never in a good way. CCCamp is a broad church, with villages of techno-communists a few minutes walk from crypto-libertarians, but all of them were concerned by formalised state intrusion into the digital space.\n\n\nClosed systems are still insecure\nOkay, this one is a little niche (I needed a list of 5, sue me) but one of my highlight talks was All Cops Are Broadcasting, discussing how security researchers had compromised the TETRA (Airwave in the UK) communications system used by most police forces. I‚Äôve never really thought much about the architecture behind Airwave, but turns out it relies on a cryptographic ‚Äúsecret sauce‚Äù. This is never a good way of designing something secure: if your software/algorithm/cryptography relies on obscurity to stay safe, it will eventually become unsafe. So I guess it‚Äôs a good thing that the Airwave replacement is right around the corner."
  },
  {
    "objectID": "posts/lapd_and_prophet/lapd_and_prophet.html",
    "href": "posts/lapd_and_prophet/lapd_and_prophet.html",
    "title": "LAPD Call Prediction for Fun (and Prophet)",
    "section": "",
    "text": "Time series prediction is more complicated than I originally anticipated when I tackled the subject during my thesis - while you can treat the events independently, like geographic data, everything is related: what happened yesterday will affect what happened today, and a Friday in July is not the same as a Monday in October.\nThere are various weird and wonderful algorithms to cope with these complexities, but Facebook‚Äôs open source Prophet does a fantastic job of providing a ‚Äúfire and forget‚Äù solution that just works.\nThis is the code extract from my Medium blog here.\nimport pandas as pd\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly\nimport plotly.offline as py\nfrom fbprophet.diagnostics import cross_validation\nfrom fbprophet.diagnostics import performance_metrics\nfrom fbprophet.plot import plot_cross_validation_metric\n#import chart_studio"
  },
  {
    "objectID": "posts/lapd_and_prophet/lapd_and_prophet.html#data-cleaning-and-aggregating",
    "href": "posts/lapd_and_prophet/lapd_and_prophet.html#data-cleaning-and-aggregating",
    "title": "LAPD Call Prediction for Fun (and Prophet)",
    "section": "Data Cleaning and Aggregating",
    "text": "Data Cleaning and Aggregating\nWe‚Äôll be using four years of LAPD call data, aggregated to hourly intervals. Prophet actually copes with various intervals quite well, so don‚Äôt worry too much about how you do yours: just try and keep regular intervals, without too many missings bits.\n\ndf_by_hour = pd.read_csv(\"/content/total_calls_per_hour.csv\")\n\n\ndf_by_hour[\"ds\"] = pd.date_range(min(df_by_hour[\"ds\"]), max(df_by_hour[\"ds\"]), freq='H')\n\nYour final cleaned data-set must contain the below two columns, ds and y. Everything else, Prophet will deal with.\n\ndf_by_hour.head()\n\n\n\n\n\n\n\n\nDispatch Date_Dispatch Time\ncall volume\nds\ny\n\n\n\n\n0\n2015-01-01 00:00:00\n286\n2015-01-01 00:00:00\n286\n\n\n1\n2015-01-01 01:00:00\n265\n2015-01-01 01:00:00\n265\n\n\n2\n2015-01-01 02:00:00\n179\n2015-01-01 02:00:00\n179\n\n\n3\n2015-01-01 03:00:00\n152\n2015-01-01 03:00:00\n152\n\n\n4\n2015-01-01 04:00:00\n127\n2015-01-01 04:00:00\n127\n\n\n\n\n\n\n\n\ndf_by_hour.tail()\n\n\n\n\n\n\n\n\nDispatch Date_Dispatch Time\ncall volume\nds\ny\n\n\n\n\n43819\n2019-12-31 19:00:00\n310\n2019-12-31 19:00:00\n310\n\n\n43820\n2019-12-31 20:00:00\n320\n2019-12-31 20:00:00\n320\n\n\n43821\n2019-12-31 21:00:00\n373\n2019-12-31 21:00:00\n373\n\n\n43822\n2019-12-31 22:00:00\n354\n2019-12-31 22:00:00\n354\n\n\n43823\n2019-12-31 23:00:00\n281\n2019-12-31 23:00:00\n281\n\n\n\n\n\n\n\n##Fitting and Deploying Prophet works similar to most Python sklearn type implementations - just fit the data and you‚Äôre off.\nHelpfully, it will also make you a data-frame containing future dates for you to predict on. It will also provide a breakdown of seasonality trends.\n\nm = Prophet()\nm.fit(df_by_hour)\n\nINFO:numexpr.utils:NumExpr defaulting to 2 threads.\n\n\n&lt;fbprophet.forecaster.Prophet at 0x7fdbb146bdd8&gt;\n\n\n\nfuture = m.make_future_dataframe(periods=365)\nforecast = m.predict(future)\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\n\n\n\n\nfig2 = m.plot_components(forecast)\n\n\n\n\n\n\n\n\n\n\nfig = plot_plotly(m, forecast)  # This returns a plotly Figure\nfig.show()"
  },
  {
    "objectID": "posts/lapd_and_prophet/lapd_and_prophet.html#diagnostics-and-cross-validation",
    "href": "posts/lapd_and_prophet/lapd_and_prophet.html#diagnostics-and-cross-validation",
    "title": "LAPD Call Prediction for Fun (and Prophet)",
    "section": "Diagnostics and Cross Validation",
    "text": "Diagnostics and Cross Validation\nHelpfully, Prophet also contains cross-validation functionality - and performs quite well, very quickly!\n\ndf_cv = cross_validation(m, initial='730 days', period='180 days', horizon = '365 days')\ndf_cv.head()\n\nINFO:fbprophet:Making 5 forecasts with cutoffs between 2017-01-10 23:00:00 and 2018-12-31 23:00:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nds\nyhat\nyhat_lower\nyhat_upper\ny\ncutoff\n\n\n\n\n0\n2017-01-11 00:00:00\n116.692077\n89.859738\n142.382745\n79\n2017-01-10 23:00:00\n\n\n1\n2017-01-11 01:00:00\n95.572727\n69.135035\n122.850000\n71\n2017-01-10 23:00:00\n\n\n2\n2017-01-11 02:00:00\n71.996010\n45.771390\n98.558667\n55\n2017-01-10 23:00:00\n\n\n3\n2017-01-11 03:00:00\n49.332357\n23.467584\n76.244978\n51\n2017-01-10 23:00:00\n\n\n4\n2017-01-11 04:00:00\n33.137479\n4.387625\n60.912735\n41\n2017-01-10 23:00:00\n\n\n\n\n\n\n\n\ndf_p = performance_metrics(df_cv)\ndf_p.head()\n\nINFO:fbprophet:Skipping MAPE because y close to 0\n\n\n\n\n\n\n\n\n\nhorizon\nmse\nrmse\nmae\nmdape\ncoverage\n\n\n\n\n0\n36 days 12:00:00\n2938.141411\n54.204625\n33.287400\n0.167800\n0.660046\n\n\n1\n36 days 13:00:00\n2929.579619\n54.125591\n33.235735\n0.167594\n0.660731\n\n\n2\n36 days 14:00:00\n2919.565265\n54.033002\n33.187913\n0.167276\n0.660959\n\n\n3\n36 days 15:00:00\n2908.693562\n53.932305\n33.155190\n0.166688\n0.661187\n\n\n4\n36 days 16:00:00\n2910.565179\n53.949654\n33.167673\n0.166688\n0.660959\n\n\n\n\n\n\n\n\nfig = plot_cross_validation_metric(df_cv, metric='rmse')"
  },
  {
    "objectID": "posts/burglary_attendance/index.html",
    "href": "posts/burglary_attendance/index.html",
    "title": "What‚Äôs Happened to Burglary, and does Attending Help?",
    "section": "",
    "text": "It‚Äôs nearly 2023, and in the true spirit of Christmas, I‚Äôve used the cheese/wine filled nowhere time between the holidays to pick up an analytical side-project that‚Äôs been irking me for awhile‚Ä¶ what‚Äôs happened to burglary, and why are we suddenly so excited about mandatory attendance? It‚Äôs a thorny question, so I figured I‚Äôd document my analysis here‚Ä¶ All(ish) of the code is available, so this will hopefully be a useful tutorial for others. As usual, keep in mind this is a blog post, not an academic article - this analysis is fuelled by post-Christmas cheese, and peer reviewed by me after a glass of cherry, so if you want to rely on any of it, replicate it yourself first and make sure it‚Äôs right!\nThis took somewhat longer than I expected (hooray for Christmas breaks), so this will be a series in three parts:\nSo come with me on an analytical adventure through time, as we cast our minds back to the halcyon days of 2015. The pound is worth $1.5 dollars again, Boris Johnson is Mayor of London, and we‚Äôre all still fondly thinking back to how great the London Olympics were.\nAcross policing, the effects of austerity are starting to be acutely felt - having briefly been protected, chief officers are tightening their fiscal belt: swathes of policing real estate are sold off to protect the front-line, which in some places is starting to rely on volunteers and goodwill to avoid looking rather thin.\nMeanwhile, crime is changing: traditional crimes like burglary and violence continue their to fall, leading the soon-to-be Prime Minister Theresa May to tell the Police Federation to ‚Äústop crying wolf‚Äù about cuts while demand shrinks, but policing certainly isn‚Äôt feeling it, as an increase in ‚Äúhigh-harm‚Äù, complex offences more than made up for any shortfall.\nIn the face of these conflicting pressures, policing did something that seemed perfectly reasonable, announcing that officers may not attend every single burglary.\nLess than a decade later, we‚Äôre now on a very different course. Confidence in policing, which had previously seemed immutable, is down. Investigative performance has seemingly falling off a cliff. And so the NPCC has made the opposite commitment, pledging that every home burglary will now see police attendance, after forces who tested the approach claimed huge ‚Äúdramatic‚Äù crime reductions.\nBut has performance really declined that quickly, and is attendance to blame? I‚Äôll use this post to explore open police data from the last decade, and use data from pilot forces to see just how dramatic the effect is: just what‚Äôs happened to burglary, and will mandatory attendance help?"
  },
  {
    "objectID": "posts/burglary_attendance/index.html#whats-happened-to-burglary",
    "href": "posts/burglary_attendance/index.html#whats-happened-to-burglary",
    "title": "What‚Äôs Happened to Burglary, and does Attending Help?",
    "section": "What‚Äôs Happened to Burglary?",
    "text": "What‚Äôs Happened to Burglary?\nWe‚Äôll start by exploring how burglary has changed over time - for the purpose of this analysis, we‚Äôre focusing on domestic burglary (eg, non including commercial properties), and we‚Äôll use both police recorded crime data (eg, crimes reported to police) and the Crime Survey of England and Wales (a national survey to measure total crime) to compare trends.\n\n\n\n\n\n\nNoteOn Data Quality\n\n\n\n\n\nWhile the UK benefits from crime data that is comparatively clean (I do not envy our American cousins, let alone most of our colleagues in the rest of the world), this is less true for both historical data (eg, going back more than a few years) or the Crime Survey data, which isn‚Äôt readily available to the general public: the data is spread through various archive files, websites and reports, and there really isn‚Äôt a ‚Äúsingle source of truth‚Äù for crime counts over time.\nThe data for this analysis is a few hacked togther files: - Crime outcomes and crime counts by quarter, manually concatenated from the Police Open Data Tables - Crime Survey burglary counts per year, from the most recent ONS crime and justice quarterly\nI‚Äôm afraid that does mean this analysis isn‚Äôt quite as reproducible as I‚Äôd like - I had written some code to scrape the Home Office data page and aggregate all the outcomes data, but it‚Äôs such a mishmash of ODS,XLSX and various formats that everything fell over and I did it manually. That said, hopefully it‚Äôs not too much of a pain to reproduce if you feel the need to.\nA few caveats are worth considering:\n\nThe Crime Survey can‚Äôt always be easily compared to reported crime: police crime data is legal accounting, while the crime survey is asking the general public what happened. Domestic Burglary should be comparable, but this is all a little experimental.\nReproducibility and data cleaning: good analysis should be reproducible - you should be able to run it the same way, with new data. This isn‚Äôt true here because of my ugly cleaning.\nOutcomes aren‚Äôt instant: Crimes take time to solve, and so the most recent outcomes data will probably change. I‚Äôve used outcomes per quarter, which should work, but the most recent data is likely to change.\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport patsy\nimport statsmodels.api as sm\nimport plotly.io as pio\nfrom linearmodels import PanelOLS\n\npio.templates.default = \"plotly_dark\"\n\n\npolice_crime_df = pd.read_csv(\"../data/external/police_recorded_crime_combined.csv\")\n\n\nyearly_police_df = police_crime_df[['Financial Year','Number of Offences']].groupby('Financial Year').sum().reset_index()\nyearly_police_df['year'] = range(2002,2023,1)\nyearly_police_df = yearly_police_df.rename(columns={\"Number of Offences\":'police'})\n\ncsew_count = pd.read_csv(\"../data/external/csew_burglary_trends.csv\").rename(columns={'Domestic burglary':'csew'})\n\n\n\ncsew_count['year'] = range(2002,2021,1)\ncsew_count['csew'] = csew_count['csew']*1000\n\ncomparison_df = csew_count.merge(yearly_police_df,how='left',on='year').rename(columns={'csew':'Crime Survey',\n                                                                                        'police':'Police Reported'})\n\ncomparison_df[['year','Crime Survey','Police Reported']].set_index('year')\n\n\n\n\n\n\n\n\n\n\n\nCrime Survey\nPolice Reported\n\n\nyear\n\n\n\n\n\n\n2002\n1423000\n437583\n\n\n2003\n1357000\n402345\n\n\n2004\n1310000\n321507\n\n\n2005\n1059000\n300517\n\n\n2006\n1025000\n292260\n\n\n2007\n1006000\n280696\n\n\n2008\n960000\n284431\n\n\n2009\n992000\n268606\n\n\n2010\n917000\n258165\n\n\n2011\n1033000\n245312\n\n\n2012\n922000\n227276\n\n\n2013\n887000\n211988\n\n\n2014\n781000\n196554\n\n\n2015\n784000\n194700\n\n\n2016\n697000\n206051\n\n\n2017\n650000\n309867\n\n\n2018\n691000\n295556\n\n\n2019\n699000\n268715\n\n\n2020\n582000\n196214\n\n\n\n\n\n\n\nThe Crime Survey data is the best measure for long term crime trends - as it isn‚Äôt affected by reporting or policing practice. Taken in isolation, the most obvious takeaway here is that burglary is lower than it‚Äôs every been. If you acccept the Peelian view that ‚Äúthe test of police efficiency is the absence of crime and disorder‚Äù, policing is doing an excellent job. So why is police reported crime comparatively high, and why are perceptions of police performance seemingly so low?\n\n\nCode\ntidy_yearly = comparison_df[['year','Crime Survey','Police Reported']].melt(id_vars='year')\n\nfig = px.line(tidy_yearly, x='year', y='value', color='variable',color_discrete_sequence=['red','blue'])\n\nfig.update_layout(\n    title_text=\"Domestic Burglaries, Police Reported and Crime Survey\",\n        legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n        y=-0.2),\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Year\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Burglaries\")\n\nfig.show()\n\n\n                                                \n\n\nComparing Crime Survey to Police Recorded crime counts is generally considered bad form: while a police officer might always know the difference between a robbery, a burglary, and an affray to enable a theft, that‚Äôs not something most of the public worries about.\nThankfully, that shouldn‚Äôt be a problem for domestic burglary - most people know when they‚Äôve been burgled! That means we can produce a ratio of crime survey burglaries to police burglaries, and obtain a rough estimate of what proportion of burglaries are reported to police over time.\n\n\nCode\ncomparison_df['Proportion of Burglaries Reported'] = comparison_df['Police Reported'] / comparison_df['Crime Survey'] * 100\n\ntidy_yearly = comparison_df[['year','Crime Survey','Police Reported','Proportion of Burglaries Reported']].melt(id_vars='year')\n\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\ncsew_burglary = tidy_yearly[tidy_yearly['variable'] == 'Crime Survey']\npolice_burglary = tidy_yearly[tidy_yearly['variable'] == 'Police Reported']\nreported_ratio = tidy_yearly[tidy_yearly['variable'] == 'Proportion of Burglaries Reported']\n\n\n\nfig.add_trace(\n    go.Scatter(x=csew_burglary['year'], y=csew_burglary['value'], name=\"Crime Survey\", line=dict(color='red')),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=police_burglary['year'], y=police_burglary['value'], name=\"Police Reported\", line=dict(color='blue')),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=reported_ratio['year'], y=reported_ratio['value'], name=\"% Reported\", line=dict(color='grey')),\n    secondary_y=True,\n)\n\n# Add figure title\nfig.update_layout(\n    title_text=\"Crime Survey Burglaries per Year and Proportion Reported\",\n    legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n        y=-0.2),\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Year\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Burglaries\", secondary_y=False, color='black')\nfig.update_yaxes(title_text=\"Proportion of Burglaries Reported\", secondary_y=True, showgrid=False, color='grey')\n\n\nfig.show()\n\n\n                                                \n\n\nI‚Äôd once thought burglary reporting would be consistent - it‚Äôs an emotive crime, with a well established insurance industry - and yet we definitely see variation over time.\nNotice that dramatic spike in 2015? That‚Äôs the aftermath of the 2014 police crime recording scandal, which led to police crime statistics losing their designation as a national statistic, and accurate crime recording becoming a key metric for the policing regulator. To dig into this issue in more depth, I‚Äôd really recommend this blog by Gavin Hales.\nWhat does this tell us about burglary in England & Wales over the last decade though? Two things:\n\nDomestic Burglaries are probably rarer than ever (and that‚Äôs not down to COVID)\nThat‚Äôs not consistently reflected in police recorded crime - the likelihood of reporting seems to change over time"
  },
  {
    "objectID": "posts/burglary_attendance/index.html#how-many-burglars-are-we-catching",
    "href": "posts/burglary_attendance/index.html#how-many-burglars-are-we-catching",
    "title": "What‚Äôs Happened to Burglary, and does Attending Help?",
    "section": "How Many Burglars are we Catching?",
    "text": "How Many Burglars are we Catching?\nSo there aren‚Äôt many burglaries‚Ä¶so have we gotten better at catching the rest?\nWell‚Ä¶we‚Äôre certainly not catching any more than we used to. Whether you use charges (eg, burglars being sent to court) or ‚Äúdetections‚Äù (eg, including cautions, fines and similar), there are far fewer, thought it looks like that trend started long before 2015.\n\n\nCode\ndetections = pd.read_csv('../data/interim/burglary_detections.csv',index_col=0)\ndetections\n\nfig = px.line(detections, x='year', y='detections')\n\nfig.update_xaxes(title_text=\"Year\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Police Detections\", range=[0,60000])\n\n\nfig.update_layout(\n    title_text=\"Police Detections by Year\"\n)\n\nfig.show()\n\n\n                                                \n\n\nThat‚Äôs not hugely surprising: there are fewer burglars around to be caught. What‚Äôs perhaps more important is how many burglars we‚Äôre catching as a proportion of those remaining burglaries we know about.\nI‚Äôve visualised that below: on the left axis, all police reported burglaries (in blue), and how many lead to a positive outcome (in red), and on the right axis, the ratio of the two: an estimate of the proportion of police burglaries that are ‚Äúsolved‚Äù.\n\n\nCode\nyearly_df = detections.merge(comparison_df, how='left', on='year')\nyearly_df['detected_csew_ratio'] = yearly_df['detections'] / yearly_df['Crime Survey'] * 100\nyearly_df['detected_police_ratio'] = yearly_df['detections'] / yearly_df['Police Reported'] * 100\n\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['Police Reported'], name=\"Police Reported\", fillcolor='blue'),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['detections'], name=\"Police Detections\",  fillcolor='red'),\n    secondary_y=False,\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['detected_police_ratio'], name=\"Poportion of Police Reported Burglaries Detected\", fillcolor='green', visible=True),\n    secondary_y=True,\n)\n\n\n# Add figure title\nfig.update_layout(\n    title_text=\"Crime Survey Burglaries per Year and Proportion Reported\",\n    legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n        y=-0.2),\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Year\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Burglaries\", secondary_y=False)\nfig.update_yaxes(title_text=\"Proportion of Burglaries Solved\",range=[0,20], secondary_y=True, showgrid=False, color='green')\n\n\nfig.show()\n\n\n                                                \n\n\nIt doesn‚Äôt look good: performance drops sharply around the time we stopped attending all burglaries in 2015.\nOf course, plenty of other things happened in 2015: for instance, we know there was a radical shift in recording standards, where police recorded and invesigated thousands of burglaries which they would have previously not been aware of. So how do we measure how police performance changed irrespective of those recording changes? We can replicate that chart, but instead of using police recorded burglaries, we use all burglaries, as reported by the Crime Survey. rather than just police reported burglaries.\nThe below chart does exactly that - click on each of the buttons to see how the baseline figure affects perceived investigatory performance.\n\n\nCode\n# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['Police Reported'], name=\"Police Reported\", fillcolor='blue'),\n    secondary_y=False,\n)\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['detections'], name=\"Police Detections\", fillcolor='red'),\n    secondary_y=False,\n)\n\n\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['detected_police_ratio'], name=\"Poportion of Police Reported Burglaries Detected\", fillcolor='green', visible=False),\n    secondary_y=True,\n)\n\nfig.add_trace(\n    go.Scatter(x=yearly_df['year'], y=yearly_df['detected_csew_ratio'], name=\"Poportion of Crime Survey Burglaries Detected\", fillcolor='black', visible=False),\n    secondary_y=True,\n)\n\n# Add figure title\nfig.update_layout(\n    title_text=\"Crime Survey Burglaries per Year and Proportion Reported\"\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Year\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Burglaries\", secondary_y=False)\nfig.update_yaxes(title_text=\"Proportion of Burglaries Solved\",range=[0,20], secondary_y=True, showgrid=False, color='green')\n\n\nfig.update_layout(\n    legend=dict(\n    orientation=\"h\",\n    yanchor=\"bottom\",\n        y=-0.2),\n    updatemenus=[\n        dict(\n            type=\"buttons\",\n            direction=\"right\",\n            active=0,\n            x=1,\n            y=1.2,\n            bgcolor='grey',\n            bordercolor='white',\n            font=dict(color='white'),\n            showactive=False,\n            buttons=list([\n                dict(label=\"As Proportion of Police Reported\",\n                     method=\"update\",\n                     args=[{\"visible\": [True, True, True,False]}]),\n                dict(label=\"As Proportion of all Burglaries\",\n                     method=\"update\",\n                     args=[{\"visible\": [True, True, False,True]}])\n            ]),\n        )\n    ])\n\nfig.show()\n\n\n                                                \n\n\nSuddenly that sharp drop in 2015 is a lot less convincing. Looking at all burglaries - rather than just those reported to police - the proportion being solved has been going down, but that started happening long before 2015. Around 5% of all burglaries in the crime survey were ‚Äúsolved‚Äù in 2010, but by 2015, that had already dropped to around 2.5%, compared to just over 2% in 2020.\nWhat‚Äôs the lesson here? The perception of a recent, short term drop in police investigative performance are somewhat overstated. Yes, policing is catching fewer burglars than ever, but burglaries are rarer than ever - those that remain are less likely to be caught than they used to be, but that‚Äôs a trend that started long before 2015 (it might even have started as early as 2008).\nWhat explains that trend? We really don‚Äôt know. It‚Äôs possible that those few remaining burglars are the most persistent and sophisiticated: maybe opportunistic drug users desperate for a fix who were willing to smash your front window in a decade ago have been replaced by professional teams who will do a whole set of flats in minutes. We can‚Äôt really tell with the data we‚Äôve got here, but it‚Äôs probably not as simple as policing just not trying hard enough."
  },
  {
    "objectID": "posts/burglary_attendance/index.html#will-mandatory-attendance-help",
    "href": "posts/burglary_attendance/index.html#will-mandatory-attendance-help",
    "title": "What‚Äôs Happened to Burglary, and does Attending Help?",
    "section": "Will Mandatory Attendance Help?",
    "text": "Will Mandatory Attendance Help?\nGiven the above, will mandatory attendance actually reduce burglary? As far as I can tell, nobody has properly checked. The Telegraph suggests that burglaries ‚Äúcould fall by 60pc if officers visited every victim‚Äù, but given nobody has really rigorously tested the impact of burglary attendance, I‚Äôm not totally convinced. Leicestershire attempted to conduct a randomised control trial in 2015, but following press uproar the whole thing was shelved.\nThere is some good research that suggests it will make some difference: rapid attendance to crimes probably does help solve them, and finding a suspect is the most effective way of solving a burglary. If we randomly assigned some burglaries to be attended, and some to not, those we did attend would be solved more often.\nThat‚Äôs different to mandatory attendance though: where even if the responding officer thinks there isn‚Äôt any point (for example, if the burglary was many months ago, or very unlikely to be solved following a phone report), they‚Äôll attend anyway. So how have some reporters calculcated that policy could cut the number of offences by half?\nUnhelpfully, the researchers for the Mail and the Telegraph haven‚Äôt been very open with sharing their methodology, but it looks like they‚Äôve looked at those forces who already have a mandatory attendance policy, and tried to predict what would happen if that had been expanded nationally.\nThree forces in particular are repeatedly mentioned as having introduced mandatory attendance prior to the NPCC mandate:\n\nNorthamptonshire since April 2019, though it‚Äôs worth noting these are dedicated burglary teams rather than attendance in isolation\nBedfordshire Police using the codename ‚ÄúOperation Maze‚Äù, though again, it seems paired with dedicated burglary teams, and may in fact be attendance by forensics staff rather than police offiers. The earliest records are in early 2020 but it‚Äôs not clear at all this involves any mandatory attendance, rather than just increased resourcing.\nGreater Manchester Police since July 2021\n\nThe key question then is did the introduction of mandatory attendance in these three forces make any difference to burglary investigations?\nTo do that, we‚Äôll calculate: - What was the detection rate for burglary in all forces? - How did the detection rate change after the introduction of mandatory attendance? - How did other, similar forces fare during the same period?\nThere are real limitations to doing this with public data: we are limited to quarter by quarter analysis, and we don‚Äôt actually know exactly when these policies started (or, for that matter, exactly what they include) or who else might have been doing something similar, but we can still try and identify what performance boost (if any) mandatory attendance made for these forces.\n\n\nCode\ntheft_df = pd.read_excel(\"../data/external/burglary_outcomes_combined.ods\", engine=\"odf\")\nqs = theft_df['Financial year'].str.slice(0,4) + \"-Q\" + theft_df['Financial quarter'].astype('str')\n\ntheft_df['date'] = pd.PeriodIndex(qs, freq='Q').to_timestamp()\ntheft_df = theft_df.sort_values(by='date',ascending=False)\n\nburglary_df = theft_df[theft_df['Offence Subgroup'] == 'Domestic burglary'].rename(columns={'Force Name':'Force'})\nburglary_df['Force'] = burglary_df['Force'].astype('string')\nburglary_df\n\n\n\n\n\n\n\n\n\nFinancial year\nFinancial quarter\nForce\nOffence Description\nOffence Group\nOffence Subgroup\nOffence Code\nOffence code expired\nOutcome Description\nOutcome Group\nOutcome Type\nForce outcomes for offences recorded in quarter\nForce outcomes recorded in quarter\ndate\n\n\n\n\n160811\n2021/22\n4\nBritish Transport Police\nAttempted Burglary Residential\nTheft offences\nDomestic burglary\n28F\nNaN\nCommunity Resolution\nOut-of-court (informal)\n8\n0.0\n0.0\n2021-10-01\n\n\n164083\n2021/22\n4\nHampshire\nAttempted Distraction Burglary Residential\nTheft offences\nDomestic burglary\n28H\nNaN\nTaken into consideration\nTaken into consideration\n4\n0.0\n0.0\n2021-10-01\n\n\n164081\n2021/22\n4\nHampshire\nAttempted Distraction Burglary Residential\nTheft offences\nDomestic burglary\n28H\nNaN\nDiversionary, educational or intervention acti...\nDiversionary, educational or intervention acti...\n22\n0.0\n0.0\n2021-10-01\n\n\n164080\n2021/22\n4\nHampshire\nAttempted Distraction Burglary Residential\nTheft offences\nDomestic burglary\n28H\nNaN\nFurther investigation to support formal action...\nFurther investigation to support formal action...\n21\n0.0\n0.0\n2021-10-01\n\n\n164079\n2021/22\n4\nHampshire\nAttempted Distraction Burglary Residential\nTheft offences\nDomestic burglary\n28H\nNaN\nResponsibility for further investigation trans...\nResponsibility for further investigation trans...\n20\n0.0\n0.0\n2021-10-01\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n49689\n2017/18\n1\nNorthamptonshire\nAttempted Burglary Residential\nTheft offences\nDomestic burglary\n28F\nNaN\nFurther investigation to support formal action...\nFurther investigation to support formal action...\n21\n1.0\n0.0\n2017-01-01\n\n\n49688\n2017/18\n1\nNorthamptonshire\nAttempted Burglary Residential\nTheft offences\nDomestic burglary\n28F\nNaN\nResponsibility for further investigation trans...\nResponsibility for further investigation trans...\n20\n1.0\n1.0\n2017-01-01\n\n\n49687\n2017/18\n1\nNorthamptonshire\nAttempted Burglary Residential\nTheft offences\nDomestic burglary\n28F\nNaN\nCaution ‚Äì youths\nOut-of-court (formal)\n2\n0.0\n0.0\n2017-01-01\n\n\n49686\n2017/18\n1\nNorthamptonshire\nAttempted Burglary Residential\nTheft offences\nDomestic burglary\n28F\nNaN\nInvestigation complete ‚Äì no suspect identified\nInvestigation complete ‚Äì no suspect identified\n18\n114.0\n104.0\n2017-01-01\n\n\n50576\n2017/18\n1\nSouth Wales\nDistraction Burglary Residential\nTheft offences\nDomestic burglary\n28G\nNaN\nCommunity Resolution\nOut-of-court (informal)\n8\n0.0\n0.0\n2017-01-01\n\n\n\n\n212960 rows √ó 14 columns\n\n\n\nGiven we‚Äôre looking at quarterly data, we‚Äôll generate two temporal variables for our data:\n\nA running variable (running_var), which measures the number of quarters that have passed since the start of our dataset\nA quarter ‚Äúcategorical variable‚Äù, checking to see if there is any specific trend for burglary detection whether you‚Äôre in quarter 1, 2, 3 or 4\n\n\n\nCode\n\ntime_series = theft_df[['date','Financial quarter']].drop_duplicates().sort_values(by=['date']).reset_index(drop=True).reset_index().rename(columns={'index':'running_var'})\ntime_series['quarter'] = time_series['Financial quarter'].astype('string')\ntime_series = time_series.drop(columns=['Financial quarter'])\ntime_series\n\n\n\n\n\n\n\n\n\nrunning_var\ndate\nquarter\n\n\n\n\n0\n0\n2017-01-01\n1\n\n\n1\n1\n2017-04-01\n2\n\n\n2\n2\n2017-07-01\n3\n\n\n3\n3\n2017-10-01\n4\n\n\n4\n4\n2018-01-01\n1\n\n\n5\n5\n2018-04-01\n2\n\n\n6\n6\n2018-07-01\n3\n\n\n7\n7\n2018-10-01\n4\n\n\n8\n8\n2019-01-01\n1\n\n\n9\n9\n2019-04-01\n2\n\n\n10\n10\n2019-07-01\n3\n\n\n11\n11\n2019-10-01\n4\n\n\n12\n12\n2020-01-01\n1\n\n\n13\n13\n2020-04-01\n2\n\n\n14\n14\n2020-07-01\n3\n\n\n15\n15\n2020-10-01\n4\n\n\n16\n16\n2021-01-01\n1\n\n\n17\n17\n2021-04-01\n2\n\n\n18\n18\n2021-07-01\n3\n\n\n19\n19\n2021-10-01\n4\n\n\n\n\n\n\n\nFor each force, we then have a count of burglaries, and a ratio of how many solved burglaries (eg, burglary investigations where a burglary is identified and dealt with by police) we have that quarter.\n\n\nCode\ntotal_q_offences = burglary_df[['date','Force','Force outcomes for offences recorded in quarter']].groupby(['date','Force']).sum().reset_index()\ntotal_q_offences['total_offences'] = pd.to_numeric(total_q_offences['Force outcomes for offences recorded in quarter'],errors='coerce')\n\npositive_outcomes = ['Taken into consideration',\n 'Charged/Summonsed',\n 'Community Resolution',\n 'Cannabis/Khat Warning',\n 'Penalty Notices for Disorder',\n 'Caution ‚Äì adults',\n 'Diversionary, educational or intervention activity, resulting from the crime report, has been undertaken and it is not in the public interest to take any further action.',\n 'Caution ‚Äì youths']\n\nburglary_df['is_detected'] = burglary_df['Outcome Description'].isin(positive_outcomes)\n\npositive_burglary_df = burglary_df[burglary_df['is_detected']]\n\ntotal_detected_offences = positive_burglary_df[['date','Force','Force outcomes for offences recorded in quarter']].groupby(['date','Force']).sum().reset_index()\ntotal_detected_offences['total_detected'] = pd.to_numeric(total_detected_offences['Force outcomes for offences recorded in quarter'],errors='coerce')\n\nforce_comparison = total_q_offences[['date','Force','total_offences']].merge(total_detected_offences, how='left',on=['date','Force']).drop(columns=['Force outcomes for offences recorded in quarter'])\nforce_comparison['detected_rate'] = force_comparison['total_detected'] / force_comparison['total_offences']\nforce_comparison['detected_rate']  = force_comparison['detected_rate'].fillna(0)\nforce_comparison\n\n\n\n\n\n\n\n\n\ndate\nForce\ntotal_offences\ntotal_detected\ndetected_rate\n\n\n\n\n0\n2017-01-01\nAvon and Somerset\n2151.0\n112.0\n0.052069\n\n\n1\n2017-01-01\nBedfordshire\n1142.0\n120.0\n0.105079\n\n\n2\n2017-01-01\nBritish Transport Police\n0.0\n0.0\n0.000000\n\n\n3\n2017-01-01\nCambridgeshire\n1109.0\n99.0\n0.089270\n\n\n4\n2017-01-01\nCheshire\n790.0\n80.0\n0.101266\n\n\n...\n...\n...\n...\n...\n...\n\n\n875\n2021-10-01\nWarwickshire\n376.0\n8.0\n0.021277\n\n\n876\n2021-10-01\nWest Mercia\n857.0\n8.0\n0.009335\n\n\n877\n2021-10-01\nWest Midlands\n4061.0\n93.0\n0.022901\n\n\n878\n2021-10-01\nWest Yorkshire\n2406.0\n81.0\n0.033666\n\n\n879\n2021-10-01\nWiltshire\n265.0\n8.0\n0.030189\n\n\n\n\n880 rows √ó 5 columns\n\n\n\nNow, we identify our ‚Äútreated‚Äù forces, and the period in which the policy was in place. This isn‚Äôt exactly clean, but based on the newspaper articles and press releases, I‚Äôve picked out:\n\nBedfordshire from 2020 onwards\nGMP from July 2021 onwards\nNorthamptonshire from April 2019 onwards\nand of course, nationwide from October 2022\n\nWith those dates, we‚Äôll then build our completed dataset of quarter by quarter burglary detection rates, for each force, and whether or not a mandatory attendance policy was in place in that force/quarter.\n\n\nCode\nnorthamptonshire = (force_comparison['Force'].str.contains('Northamptonshire')) & (force_comparison['date'] &gt;= '2019-04-01')\nbedfordshire = (force_comparison['Force'].str.contains('Bedfordshire')) & (force_comparison['date'] &gt;= '2020-01-01')\nmanchester = (force_comparison['Force'].str.contains('Manchester')) & (force_comparison['date'] &gt;= '2020-07-01')\neveryone = (force_comparison['date'] &gt;= '2022-10-01')\n\n\nforce_comparison['mandatory_attendance'] = northamptonshire | bedfordshire | manchester | everyone\nforce_comparison\n\n\n\n\n\n\n\n\n\ndate\nForce\ntotal_offences\ntotal_detected\ndetected_rate\nmandatory_attendance\n\n\n\n\n0\n2017-01-01\nAvon and Somerset\n2151.0\n112.0\n0.052069\nFalse\n\n\n1\n2017-01-01\nBedfordshire\n1142.0\n120.0\n0.105079\nFalse\n\n\n2\n2017-01-01\nBritish Transport Police\n0.0\n0.0\n0.000000\nFalse\n\n\n3\n2017-01-01\nCambridgeshire\n1109.0\n99.0\n0.089270\nFalse\n\n\n4\n2017-01-01\nCheshire\n790.0\n80.0\n0.101266\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n875\n2021-10-01\nWarwickshire\n376.0\n8.0\n0.021277\nFalse\n\n\n876\n2021-10-01\nWest Mercia\n857.0\n8.0\n0.009335\nFalse\n\n\n877\n2021-10-01\nWest Midlands\n4061.0\n93.0\n0.022901\nFalse\n\n\n878\n2021-10-01\nWest Yorkshire\n2406.0\n81.0\n0.033666\nFalse\n\n\n879\n2021-10-01\nWiltshire\n265.0\n8.0\n0.030189\nFalse\n\n\n\n\n880 rows √ó 6 columns\n\n\n\nOne limitation of working by quarters is that we‚Äôre quite limited in numbers! Good inference in statistical modelling relies on detecting ‚Äúvariance‚Äù - eg, having enough data to identify both our effect, and general background levels, and separating out the two. Given GMP has only been running for 6 quarters, we‚Äôll focus on the other two forces for this analysis.\n\n\nCode\nforce_comparison[['Force','mandatory_attendance']].groupby('Force').sum().sort_values(by='mandatory_attendance', ascending=False).rename(columns={\"mandatory_attendance\":'Quarters of Mandatory Attendance'})\n\n\n\n\n\n\n\n\n\nQuarters of Mandatory Attendance\n\n\nForce\n\n\n\n\n\nNorthamptonshire\n11\n\n\nBedfordshire\n8\n\n\nGreater Manchester\n6\n\n\nSouth Wales\n0\n\n\nMerseyside\n0\n\n\nMetropolitan Police\n0\n\n\nNorfolk\n0\n\n\nNorth Wales\n0\n\n\nNorth Yorkshire\n0\n\n\nNorthumbria\n0\n\n\nNottinghamshire\n0\n\n\nAvon and Somerset\n0\n\n\nLondon, City of\n0\n\n\nStaffordshire\n0\n\n\nSuffolk\n0\n\n\nSurrey\n0\n\n\nSussex\n0\n\n\nThames Valley\n0\n\n\nWarwickshire\n0\n\n\nWest Mercia\n0\n\n\nWest Midlands\n0\n\n\nWest Yorkshire\n0\n\n\nSouth Yorkshire\n0\n\n\nLincolnshire\n0\n\n\nLeicestershire\n0\n\n\nLancashire\n0\n\n\nBritish Transport Police\n0\n\n\nCambridgeshire\n0\n\n\nCheshire\n0\n\n\nCleveland\n0\n\n\nCumbria\n0\n\n\nDerbyshire\n0\n\n\nDevon and Cornwall\n0\n\n\nDorset\n0\n\n\nDurham\n0\n\n\nDyfed-Powys\n0\n\n\nEssex\n0\n\n\nGloucestershire\n0\n\n\nGwent\n0\n\n\nHampshire\n0\n\n\nHertfordshire\n0\n\n\nHumberside\n0\n\n\nKent\n0\n\n\nWiltshire\n0\n\n\n\n\n\n\n\nNow we can start combining our data together, and seeing if we can observe any meaningful difference after the intervention takes place.\nFor meaningful comparisons, we‚Äôll seek to compare forces to other similar forces - there‚Äôs not much point in comparing GMP to Durham. We could do that with our existing data - eg, looking at forces with similar numbers of burglaries - but helpfully, the policing regulator HMIFRS already produces ‚ÄúMost Similar Forces‚Äù groups, which should let us match up forces based on size, demographics and other factors relevant to performance. We‚Äôll seperate out our group A (Bedfordshire and their group) from group B (Northamptonshire‚Äôs), though notice Kent is in both - unhelpfully, HMIC‚Äôs groups aren‚Äôt exclusive, though that‚Äôs not too much of a problem.\n\n\nCode\ngroup_a = [\"Bedfordshire\",\n\"Leicestershire\",\n\"Nottinghamshire\",\n\"Hertfordshire\",\n\"Kent\",\n\"Hampshire\",\n\"Essex\",\n\"South Yorkshire\"]\n\ngroup_b = [\"Northamptonshire\",\n           \"Cheshire\",\n\"Derbyshire\",\n\"Staffordshire\",\n\"Kent\",\n\"Avon and Somerset\",\n\"Essex\",\n\"Nottinghamshire\"]\n\nmsf_groups = force_comparison[['Force']].drop_duplicates().reset_index(drop=True)\nmsf_groups['group_A'] = msf_groups['Force'].isin(group_a)\nmsf_groups['group_B'] = msf_groups['Force'].isin(group_b)\nmsf_groups\n\n\n\n\n\n\n\n\n\nForce\ngroup_A\ngroup_B\n\n\n\n\n0\nAvon and Somerset\nFalse\nTrue\n\n\n1\nBedfordshire\nTrue\nFalse\n\n\n2\nBritish Transport Police\nFalse\nFalse\n\n\n3\nCambridgeshire\nFalse\nFalse\n\n\n4\nCheshire\nFalse\nTrue\n\n\n5\nCleveland\nFalse\nFalse\n\n\n6\nCumbria\nFalse\nFalse\n\n\n7\nDerbyshire\nFalse\nTrue\n\n\n8\nDevon and Cornwall\nFalse\nFalse\n\n\n9\nDorset\nFalse\nFalse\n\n\n10\nDurham\nFalse\nFalse\n\n\n11\nDyfed-Powys\nFalse\nFalse\n\n\n12\nEssex\nTrue\nTrue\n\n\n13\nGloucestershire\nFalse\nFalse\n\n\n14\nGreater Manchester\nFalse\nFalse\n\n\n15\nGwent\nFalse\nFalse\n\n\n16\nHampshire\nTrue\nFalse\n\n\n17\nHertfordshire\nTrue\nFalse\n\n\n18\nHumberside\nFalse\nFalse\n\n\n19\nKent\nTrue\nTrue\n\n\n20\nLancashire\nFalse\nFalse\n\n\n21\nLeicestershire\nTrue\nFalse\n\n\n22\nLincolnshire\nFalse\nFalse\n\n\n23\nLondon, City of\nFalse\nFalse\n\n\n24\nMerseyside\nFalse\nFalse\n\n\n25\nMetropolitan Police\nFalse\nFalse\n\n\n26\nNorfolk\nFalse\nFalse\n\n\n27\nNorth Wales\nFalse\nFalse\n\n\n28\nNorth Yorkshire\nFalse\nFalse\n\n\n29\nNorthamptonshire\nFalse\nTrue\n\n\n30\nNorthumbria\nFalse\nFalse\n\n\n31\nNottinghamshire\nTrue\nTrue\n\n\n32\nSouth Wales\nFalse\nFalse\n\n\n33\nSouth Yorkshire\nTrue\nFalse\n\n\n34\nStaffordshire\nFalse\nTrue\n\n\n35\nSuffolk\nFalse\nFalse\n\n\n36\nSurrey\nFalse\nFalse\n\n\n37\nSussex\nFalse\nFalse\n\n\n38\nThames Valley\nFalse\nFalse\n\n\n39\nWarwickshire\nFalse\nFalse\n\n\n40\nWest Mercia\nFalse\nFalse\n\n\n41\nWest Midlands\nFalse\nFalse\n\n\n42\nWest Yorkshire\nFalse\nFalse\n\n\n43\nWiltshire\nFalse\nFalse\n\n\n\n\n\n\n\nLet‚Äôs start with a simple visual observation: we look at the rate of detection per outcome for each force and group, with the start of mandatory attendance visible as the dashed vertical line.\n\n\nCode\nlinear_comparator_df = force_comparison.merge(msf_groups, how='right', on='Force')\nlinear_comparator_df = linear_comparator_df[linear_comparator_df['group_A'] | linear_comparator_df['group_B']].merge(time_series, how='left', on='date')\n\nlinear_comparator_df['detected_rate'] = linear_comparator_df['detected_rate']  * 100\n\ndf_a = linear_comparator_df[linear_comparator_df['group_A']]\ndf_b = linear_comparator_df[linear_comparator_df['group_B']]\n\nfig = make_subplots(rows=2, cols=1)\n\nfor force in df_a['Force'].unique():\n    force_df = df_a[df_a['Force'] == force]\n    if force == 'Bedfordshire':\n        fig.add_trace(\n            go.Scatter(\n            x=force_df['date'], y=force_df['detected_rate'], name=force, line=dict(color='red', width=4,\n                              dash='dash')),\n            row=1, col=1\n        )\n    else:\n        force_df = df_a[df_a['Force'] == force]\n        fig.add_trace(\n            go.Scatter(\n            x=force_df['date'], y=force_df['detected_rate'], name=force),\n            row=1, col=1\n        )\n\n\n\nfor force in df_b['Force'].unique():\n    force_df = df_b[df_b['Force'] == force]\n    if force == 'Northamptonshire':\n        fig.add_trace(\n            go.Scatter(\n            x=force_df['date'], y=force_df['detected_rate'], name=force, line=dict(color='blue', width=4,\n                              dash='dash')),\n            row=2, col=1\n        )\n    else:\n        fig.add_trace(\n            go.Scatter(\n            x=force_df['date'], y=force_df['detected_rate'], name=force),\n            row=2, col=1\n        )\n\n\nfig.add_vline(x='2020-01-01', row=1, col=1, line_dash=\"dash\")\nfig.add_vline(x='2019-04-01', row=2, col=1,line_dash=\"dash\")\n\nfig.update_xaxes(title_text=\"Date\")\nfig.update_layout(height=700, title_text=\"Detected Outcomes per Quarter and Force, and start of Mandatory Attendance\")\n\nfig.update_yaxes(title_text=\"Detected Rate\")\n\nfig.show()\n\n\n                                                \n\n\nIt looks, in a word, messy. There might be some sort of effect, but it certainly isn‚Äôt pronounced enough to visually observe. Instead, let‚Äôs use some statistical modelling to try and measure the average effect."
  },
  {
    "objectID": "posts/burglary_attendance/index.html#statistical-modelling",
    "href": "posts/burglary_attendance/index.html#statistical-modelling",
    "title": "What‚Äôs Happened to Burglary, and does Attending Help?",
    "section": "Statistical Modelling",
    "text": "Statistical Modelling\n\nLinear Models with Time Effects\nWith all our data, we now build a statistical model, predicting the detected outcome rate as a factor of force, their comparison group, their total offences, and the period in time.\n\n\nCode\nlinear_comparator_df['Force'] = linear_comparator_df['Force'].astype('category')\nlinear_comparator_df['quarter'] = linear_comparator_df['quarter'].astype('category')\nlinear_comparator_df['mandatory_attendance'] = linear_comparator_df['mandatory_attendance'].astype('int')\n\n\ny = linear_comparator_df['detected_rate']\n\nX = patsy.dmatrix(\"0 + C(Force) + total_offences + mandatory_attendance + group_A + group_B  + C(quarter) + running_var\", data=linear_comparator_df, return_type='dataframe')\n\n\nmodel = sm.OLS(y,X)\n\nresults = model.fit()\n\nresults.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndetected_rate\nR-squared:\n0.551\n\n\nModel:\nOLS\nAdj. R-squared:\n0.518\n\n\nMethod:\nLeast Squares\nF-statistic:\n16.44\n\n\nDate:\nSat, 31 Dec 2022\nProb (F-statistic):\n1.59e-32\n\n\nTime:\n16:25:24\nLog-Likelihood:\n-426.89\n\n\nNo. Observations:\n260\nAIC:\n891.8\n\n\nDf Residuals:\n241\nBIC:\n959.4\n\n\nDf Model:\n18\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nC(Force)[Avon and Somerset]\n3.6335\n0.530\n6.861\n0.000\n2.590\n4.677\n\n\nC(Force)[Bedfordshire]\n3.1250\n0.322\n9.720\n0.000\n2.492\n3.758\n\n\nC(Force)[Cheshire]\n4.9698\n0.291\n17.099\n0.000\n4.397\n5.542\n\n\nC(Force)[Derbyshire]\n1.3595\n0.322\n4.226\n0.000\n0.726\n1.993\n\n\nC(Force)[Essex]\n-2.4339\n0.274\n-8.877\n0.000\n-2.974\n-1.894\n\n\nC(Force)[Hampshire]\n2.8248\n0.468\n6.031\n0.000\n1.902\n3.747\n\n\nC(Force)[Hertfordshire]\n1.4803\n0.277\n5.341\n0.000\n0.934\n2.026\n\n\nC(Force)[Kent]\n-3.5083\n0.277\n-12.671\n0.000\n-4.054\n-2.963\n\n\nC(Force)[Leicestershire]\n2.8885\n0.294\n9.821\n0.000\n2.309\n3.468\n\n\nC(Force)[Northamptonshire]\n0.6208\n0.350\n1.774\n0.077\n-0.068\n1.310\n\n\nC(Force)[Nottinghamshire]\n-2.1424\n0.419\n-5.118\n0.000\n-2.967\n-1.318\n\n\nC(Force)[South Yorkshire]\n3.5578\n0.705\n5.045\n0.000\n2.169\n4.947\n\n\nC(Force)[Staffordshire]\n3.2984\n0.292\n11.288\n0.000\n2.723\n3.874\n\n\ngroup_A[T.True]\n5.7917\n0.632\n9.165\n0.000\n4.547\n7.037\n\n\ngroup_B[T.True]\n5.7974\n0.455\n12.730\n0.000\n4.900\n6.694\n\n\nC(quarter)[T.2]\n-0.0319\n0.231\n-0.138\n0.890\n-0.487\n0.423\n\n\nC(quarter)[T.3]\n-1.0204\n0.259\n-3.937\n0.000\n-1.531\n-0.510\n\n\nC(quarter)[T.4]\n-1.0813\n0.247\n-4.370\n0.000\n-1.569\n-0.594\n\n\ntotal_offences\n-0.0013\n0.000\n-2.707\n0.007\n-0.002\n-0.000\n\n\nmandatory_attendance\n0.7574\n0.442\n1.714\n0.088\n-0.113\n1.628\n\n\nrunning_var\n-0.1198\n0.027\n-4.481\n0.000\n-0.172\n-0.067\n\n\n\n\n\n\n\n\nOmnibus:\n1.754\nDurbin-Watson:\n1.763\n\n\nProb(Omnibus):\n0.416\nJarque-Bera (JB):\n1.487\n\n\nSkew:\n0.174\nProb(JB):\n0.475\n\n\nKurtosis:\n3.128\nCond. No.\n7.82e+19\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 8.11e-32. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\n\n\nOur model suggests that mandatory attendance may have had, on average, a small effect on performance, this was not significant - eg, it may have been down to sheer luck.\nLet‚Äôs go a bit further, and add a interaction effect between time and our force variable - eg, accounting for the fact that performance may vary differently over time force, by force.\n\n\nCode\ny = linear_comparator_df['detected_rate']\n\nX = patsy.dmatrix(\"C(Force)*running_var + total_offences + mandatory_attendance + group_A + group_B  + C(quarter)\", data=linear_comparator_df, return_type='dataframe')\n\n\nmodel = sm.OLS(y,X)\n\nresults = model.fit()\n\nresults.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ndetected_rate\nR-squared:\n0.616\n\n\nModel:\nOLS\nAdj. R-squared:\n0.565\n\n\nMethod:\nLeast Squares\nF-statistic:\n12.22\n\n\nDate:\nSat, 31 Dec 2022\nProb (F-statistic):\n6.57e-33\n\n\nTime:\n16:25:24\nLog-Likelihood:\n-406.76\n\n\nNo. Observations:\n260\nAIC:\n875.5\n\n\nDf Residuals:\n229\nBIC:\n985.9\n\n\nDf Model:\n30\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n6.2968\n0.805\n7.827\n0.000\n4.712\n7.882\n\n\nC(Force)[T.Bedfordshire]\n1.6786\n0.583\n2.879\n0.004\n0.530\n2.827\n\n\nC(Force)[T.Cheshire]\n2.9897\n0.959\n3.118\n0.002\n1.101\n4.879\n\n\nC(Force)[T.Derbyshire]\n-1.9116\n0.867\n-2.205\n0.028\n-3.620\n-0.204\n\n\nC(Force)[T.Essex]\n-0.2824\n0.500\n-0.565\n0.573\n-1.268\n0.703\n\n\nC(Force)[T.Hampshire]\n2.2860\n0.537\n4.256\n0.000\n1.228\n3.344\n\n\nC(Force)[T.Hertfordshire]\n-0.2522\n0.513\n-0.492\n0.623\n-1.263\n0.759\n\n\nC(Force)[T.Kent]\n-1.8168\n0.490\n-3.704\n0.000\n-2.783\n-0.850\n\n\nC(Force)[T.Leicestershire]\n-0.3087\n0.483\n-0.639\n0.523\n-1.260\n0.643\n\n\nC(Force)[T.Northamptonshire]\n-3.0584\n0.941\n-3.252\n0.001\n-4.912\n-1.205\n\n\nC(Force)[T.Nottinghamshire]\n-0.6398\n0.620\n-1.032\n0.303\n-1.862\n0.582\n\n\nC(Force)[T.South Yorkshire]\n0.8367\n0.745\n1.123\n0.263\n-0.632\n2.305\n\n\nC(Force)[T.Staffordshire]\n0.8295\n0.902\n0.919\n0.359\n-0.948\n2.607\n\n\ngroup_A[T.True]\n1.5015\n0.429\n3.496\n0.001\n0.655\n2.348\n\n\ngroup_B[T.True]\n2.0563\n0.418\n4.917\n0.000\n1.232\n2.880\n\n\nC(quarter)[T.2]\n-0.0358\n0.220\n-0.162\n0.871\n-0.470\n0.399\n\n\nC(quarter)[T.3]\n-1.0428\n0.256\n-4.077\n0.000\n-1.547\n-0.539\n\n\nC(quarter)[T.4]\n-1.1036\n0.240\n-4.601\n0.000\n-1.576\n-0.631\n\n\nrunning_var\n-0.0171\n0.056\n-0.305\n0.761\n-0.128\n0.093\n\n\nC(Force)[T.Bedfordshire]:running_var\n-0.1477\n0.088\n-1.670\n0.096\n-0.322\n0.027\n\n\nC(Force)[T.Cheshire]:running_var\n-0.1680\n0.070\n-2.388\n0.018\n-0.307\n-0.029\n\n\nC(Force)[T.Derbyshire]:running_var\n-0.0334\n0.069\n-0.487\n0.627\n-0.168\n0.102\n\n\nC(Force)[T.Essex]:running_var\n-0.1593\n0.069\n-2.320\n0.021\n-0.295\n-0.024\n\n\nC(Force)[T.Hampshire]:running_var\n-0.2698\n0.068\n-3.966\n0.000\n-0.404\n-0.136\n\n\nC(Force)[T.Hertfordshire]:running_var\n-0.1377\n0.068\n-2.032\n0.043\n-0.271\n-0.004\n\n\nC(Force)[T.Kent]:running_var\n-0.1106\n0.068\n-1.629\n0.105\n-0.244\n0.023\n\n\nC(Force)[T.Leicestershire]:running_var\n0.0146\n0.068\n0.216\n0.829\n-0.119\n0.148\n\n\nC(Force)[T.Northamptonshire]:running_var\n0.0375\n0.091\n0.413\n0.680\n-0.141\n0.216\n\n\nC(Force)[T.Nottinghamshire]:running_var\n-0.0865\n0.068\n-1.273\n0.204\n-0.220\n0.047\n\n\nC(Force)[T.South Yorkshire]:running_var\n-0.0445\n0.068\n-0.657\n0.512\n-0.178\n0.089\n\n\nC(Force)[T.Staffordshire]:running_var\n-0.1167\n0.068\n-1.707\n0.089\n-0.251\n0.018\n\n\ntotal_offences\n-0.0012\n0.001\n-2.279\n0.024\n-0.002\n-0.000\n\n\nmandatory_attendance\n0.3016\n0.777\n0.388\n0.698\n-1.229\n1.832\n\n\n\n\n\n\n\n\nOmnibus:\n3.734\nDurbin-Watson:\n2.028\n\n\nProb(Omnibus):\n0.155\nJarque-Bera (JB):\n3.443\n\n\nSkew:\n0.221\nProb(JB):\n0.179\n\n\nKurtosis:\n3.350\nCond. No.\n4.44e+18\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 2.52e-29. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\n\n\nNo matter how we model it, we‚Äôre again not finding meaningful effects - mandatory attendance does seem to be associated with slightly higher detected rates, on average, but this isn‚Äôt statistically significant. Put simply, examining the ‚Äústatistical noise‚Äù in the rest of our model, we can‚Äôt confidently say any increases aren‚Äôt just due to random luck.\nAs one final approach, we‚Äôll use panel data using the LinearModels library - far from the best way to examine how effects change over time, but not a bad starter for ten.\n\n\nCode\npanel_data = force_comparison.set_index([\"Force\", \"date\"])\npanel_data\n\n\n\n\n\n\n\n\n\n\ntotal_offences\ntotal_detected\ndetected_rate\nmandatory_attendance\n\n\nForce\ndate\n\n\n\n\n\n\n\n\nAvon and Somerset\n2017-01-01\n2151.0\n112.0\n0.052069\nFalse\n\n\nBedfordshire\n2017-01-01\n1142.0\n120.0\n0.105079\nFalse\n\n\nBritish Transport Police\n2017-01-01\n0.0\n0.0\n0.000000\nFalse\n\n\nCambridgeshire\n2017-01-01\n1109.0\n99.0\n0.089270\nFalse\n\n\nCheshire\n2017-01-01\n790.0\n80.0\n0.101266\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\nWarwickshire\n2021-10-01\n376.0\n8.0\n0.021277\nFalse\n\n\nWest Mercia\n2021-10-01\n857.0\n8.0\n0.009335\nFalse\n\n\nWest Midlands\n2021-10-01\n4061.0\n93.0\n0.022901\nFalse\n\n\nWest Yorkshire\n2021-10-01\n2406.0\n81.0\n0.033666\nFalse\n\n\nWiltshire\n2021-10-01\n265.0\n8.0\n0.030189\nFalse\n\n\n\n\n880 rows √ó 4 columns\n\n\n\n\n\nCode\nmod = PanelOLS.from_formula(\n    \"detected_rate ~ 1 + mandatory_attendance + EntityEffects + TimeEffects\", data=panel_data\n)\nmod.fit()\n\n\n\nPanelOLS Estimation Summary\n\n\nDep. Variable:\ndetected_rate\nR-squared:\n0.0013\n\n\nEstimator:\nPanelOLS\nR-squared (Between):\n-0.0444\n\n\nNo. Observations:\n880\nR-squared (Within):\n-0.0013\n\n\nDate:\nSat, Dec 31 2022\nR-squared (Overall):\n-0.0059\n\n\nTime:\n16:25:24\nLog-likelihood\n1326.8\n\n\nCov. Estimator:\nUnadjusted\n\n\n\n\n\n\nF-statistic:\n1.0290\n\n\nEntities:\n44\nP-value\n0.3107\n\n\nAvg Obs:\n20.000\nDistribution:\nF(1,816)\n\n\nMin Obs:\n20.000\n\n\n\n\nMax Obs:\n20.000\nF-statistic (robust):\n1.0290\n\n\n\n\nP-value\n0.3107\n\n\nTime periods:\n20\nDistribution:\nF(1,816)\n\n\nAvg Obs:\n44.000\n\n\n\n\nMin Obs:\n44.000\n\n\n\n\nMax Obs:\n44.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n0.0614\n0.0019\n31.890\n0.0000\n0.0576\n0.0652\n\n\nmandatory_attendance\n0.0155\n0.0153\n1.0144\n0.3107\n-0.0145\n0.0456\n\n\n\nF-test for Poolability: 2.3035P-value: 0.0000Distribution: F(62,816)Included effects: Entity, Timeid: 0x1ecad429930\n\n\nA similar pattern emerges - we see a small increase associated with mandatory attendance, but it‚Äôs non-significant - the increases are too small or too random to say the effect is associated due to attendance, rather than just random data noise.\nThat said, there are plenty of issues with our approach! We‚Äôve only tried straightforward linear approaches, and given we‚Äôre relying on two forces and quarterly data, we‚Äôd struggle to detect small effects‚Ä¶ hopefully we‚Äôll notice something more stark when I come back to this in a few months for part 2, time series approaches!"
  },
  {
    "objectID": "posts/copbot_analysis/analysis.html",
    "href": "posts/copbot_analysis/analysis.html",
    "title": "Paper in Progress: Variance in AI Perceptions of Risk",
    "section": "",
    "text": "NotePaper in Progress\n\n\n\nThis is a research paper in progress: it‚Äôs unfinished, messy, and maybe nonsensical, but feedback is always very welcome.\nThis paper is now available as a pre-print.\nCode\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nCode\ndef plot_coefficients_vertical_legend(group_name, group_df, color_map):\n    if group_df.empty:\n        print(f\"No data available for group {group_name}. Skipping plot.\")\n        return None\n\n    factors = group_df['new_name'].dropna().unique()\n    x_positions = np.arange(len(factors))\n    models = group_df['model'].unique()\n    bar_width = 0.8 / len(models)\n\n    fig = plt.figure(figsize=(12, 6))\n    for i, model in enumerate(models):\n        model_data = group_df[group_df['model'] == model]\n        aligned_data = model_data.set_index('new_name').reindex(factors).reset_index()\n        positions = x_positions + i * bar_width\n        coeffs = aligned_data['Coef.']\n        ci_low = aligned_data['ci_low']\n        ci_high = aligned_data['ci_high']\n\n        bar_style = {\"color\": 'dimgray', \"hatch\": '//', \"alpha\": 0.8, \"edgecolor\": 'black'} if model == \"human\" else {\"color\": color_map.get(model, 'blue'), \"alpha\": 0.8, \"edgecolor\": 'black'}\n        error_color = 'black'\n\n        plt.bar(positions, coeffs, bar_width, label=model, **bar_style)\n        plt.errorbar(positions, coeffs, yerr=[coeffs - ci_low, ci_high - coeffs], fmt='none',\n                     ecolor=error_color, capsize=3, alpha=0.8)\n\n    plt.xticks(x_positions + bar_width * (len(models) - 1) / 2, factors, rotation=45, ha='right')\n    plt.ylabel('Regression Coefficients')\n    plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n    plt.legend(title=\"Model\", loc='center left', bbox_to_anchor=(1.02, 0.5), ncol=1)\n    plt.tight_layout()\n    \n    return fig\n\ndef main(coefficients_path, new_names_path):\n    coefficients = pd.read_csv(coefficients_path)\n    new_names = pd.read_csv(new_names_path)\n    coefficients = coefficients.merge(new_names, left_on='Unnamed: 0', right_on='Original_name', how='left')\n    coefficients.rename(columns={'[0.025': 'ci_low', '0.975]': 'ci_high'}, inplace=True)\n    coefficients['significant'] = coefficients['P&gt;|t|'] &lt; 0.05\n\n    missing_name_mapping = {\n        \"C(age, Treatment(reference=25))[T.5]\": \"Age 5\",\n        \"C(age, Treatment(reference=25))[T.10]\": \"Age 10\",\n        \"C(age, Treatment(reference=25))[T.14]\": \"Age 14\",\n        \"C(age, Treatment(reference=25))[T.16]\": \"Age 16\",\n        \"C(age, Treatment(reference=25))[T.20]\": \"Age 20\",\n        \"C(age, Treatment(reference=25))[T.50]\": \"Age 50\",\n        \"C(age, Treatment(reference=25))[T.80]\": \"Age 80\",\n        \"C(hours_missing, Treatment(reference=8))[T.10]\": \"Hours Missing 10\",\n        \"C(hours_missing, Treatment(reference=8))[T.14]\": \"Hours Missing 14\",\n        \"C(hours_missing, Treatment(reference=8))[T.18]\": \"Hours Missing 18\",\n        \"C(ethnicity, Treatment(reference='White'))[T.Asian]\": \"Ethnicity: Asian\",\n        \"C(ethnicity, Treatment(reference='White'))[T.Black]\": \"Ethnicity: Black\",\n        \"C(ethnicity, Treatment(reference='White'))[T.mixed race]\": \"Ethnicity: Mixed Race\"\n    }\n    coefficients['new_name'] = coefficients['new_name'].fillna(\n        coefficients['Original_name'].map(missing_name_mapping)\n    )\n\n    grouped_coefficients = coefficients.groupby('Group')\n    unique_models = coefficients['model'].unique()\n    color_map = {model: plt.cm.tab10(i % 10) for i, model in enumerate(unique_models)}\n\n    plots_dict = {}\n    for group_name, group_df in grouped_coefficients:\n        plot = plot_coefficients_vertical_legend(group_name, group_df, color_map)\n        if plot:\n            plots_dict[group_name] = plot\n            plt.close(plot)  # Close the figure to free memory\n    \n    return plots_dict\n\n\ncoefficients_path = \"data/copbot_coefficients.csv\"  # Replace with your file path\nnew_names_path = \"data/copbot_new_names.csv\"    \n\ncoefficient_plots = main(coefficients_path, new_names_path)"
  },
  {
    "objectID": "posts/copbot_analysis/analysis.html#models",
    "href": "posts/copbot_analysis/analysis.html#models",
    "title": "Paper in Progress: Variance in AI Perceptions of Risk",
    "section": "Models",
    "text": "Models\nFor ease of analysis, we used LiteLLM to query a range of different LLM models using the same approach. we focus on models from OpenAI, as well as a range of ‚Äúopen source‚Äù or other permission models, provided via Groq, an online platform for serving language models.\nTo ensure the models returned measurable responses, we used ‚ÄúStructured Output‚Äù (we force the model to return entries in a numberical format). Not every model is compatible with this approach, and as such we only use a small number of models (this also allows us to reduce our costs).\n\nOpenAI Models\n\nGPT-3.5\n\nSummary: Released in March 2022, GPT-3.5 is a large language model with 175 billion parameters, and was a basis for ChatGPT, known for its improved ability to follow instructions compared to previous versions.\n\nGPT-4o\n\nSummary: Released in May 2024, GPT-4o is one of OpenAI‚Äôs current flagship models, featuring improved performance in text, vision, and audio tasks compared to its predecessors. Its parameter count is not publicly disclosed.\n\nGPT-4o-Mini\n\nSummary: Released in May 2024, GPT-4o-Mini is a smaller, faster variant of GPT-4o intended for testing purposes, and its performance is anticipated to be closer to that of GPT-4, but its parameter count is not publicly disclosed.\n\n\nGroq Models\n\nGemma2-9b-it\n\nSummary: Gemma2-9b-it is a 9 billion parameter, open-access language model from Google, scheduled for release in June 2024. It is the second generation of the Gemma models.\n\nLlama-3.1-70b-versatile\n\nSummary: Released in April 2024, Llama-3.1-70b-versatile is a 70 billion parameter, open-source language model from Meta, notable for its performance on various benchmarks, including reasoning and coding tasks.\n\nLlama-3.1-8b-versatile\n\nSummary: Released in April 2024, Llama-3.1-8b-versatile is an 8 billion parameter, open-source language model from Meta, designed to be more accessible for developers with limited computational resources.\n\nMixtral-8x7b\n\nSummary: Released in December 2023, Mixtral-8x7b is a 46.7 billion parameter, open-source language model from Mistral AI, utilizing a sparse mixture-of-experts architecture."
  },
  {
    "objectID": "posts/copbot_analysis/analysis.html#analytical-approach",
    "href": "posts/copbot_analysis/analysis.html#analytical-approach",
    "title": "Paper in Progress: Variance in AI Perceptions of Risk",
    "section": "Analytical Approach",
    "text": "Analytical Approach\nFor our analysis, we build a series of linear regression models, that attempt to forecast the predicted risk of any one entry based on other factors. While this approach isn‚Äôt perfect, it gives us a quick and replicable way of building a statistical model for how each language model makes decisions, and attempting to isolate the effect (if any) of any one factor.\nWe treat every variable as either categorical with a dummy variable or a boolean (for example, our sex variable with be 1 for female, and 0 for male). Again, while this approach is not perfect, it allows us rapid and interpretable analytical approach.\n\\[\n\\begin{align}\n\\text{RiskAssessment} = \\beta_0 &+ \\beta_1\\text{LocationUS} + \\beta_2\\text{LocationInternational} \\\\\n&+ \\beta_3\\text{RiskFactor}_\\text{out of character} \\\\\n&+ \\beta_4\\text{Gender}_\\text{male} \\\\\n&+ \\beta_5\\text{AgeGroup}_\\text{&gt;25} \\\\\n&+ \\beta_6\\text{TimeMissing}_\\text{&gt;8hrs} \\\\\n&+ \\beta_7\\text{Ethnicity}_\\text{white} + \\epsilon\n\\end{align}\n\\]\nWe also build an additional bespoke model for human decision makers, to allow us to consider additional variables (whether they are police affiliated, and their location).\n\\[\n\\begin{align}\n\\text{RiskAssessment} = \\beta_0 &+ \\beta_1\\text{IsPoliceOfficer} + \\beta_2\\text{PoliceFamily} + \\beta_3\\text{MemberOfPublic} \\\\\n&+ \\beta_4\\text{LocationUS} + \\beta_5\\text{LocationInternational} \\\\\n&+ \\beta_6\\text{RiskFactor}_\\text{out of character} \\\\\n&+ \\beta_7\\text{Gender}_\\text{male} \\\\\n&+ \\beta_8\\text{AgeGroup}_\\text{&gt;25} \\\\\n&+ \\beta_9\\text{TimeMissing}_\\text{&gt;8hrs} \\\\\n&+ \\beta_{10}\\text{Ethnicity}_\\text{white} + \\epsilon\n\\end{align}\n\\]\nUsing these models, we can then examine the coefficients for individual factors to understand their strength, and statistical significance to understand if they have a broadly meaningful impact."
  },
  {
    "objectID": "posts/copbot_analysis/analysis.html#model-performance",
    "href": "posts/copbot_analysis/analysis.html#model-performance",
    "title": "Paper in Progress: Variance in AI Perceptions of Risk",
    "section": "Model performance",
    "text": "Model performance\nTo obtain a high level view of how the model perceptions of risk compare to human submitted scores, we calculate a few metrics on a model by model basis, treating the human risk submission as ‚Äúcorrect‚Äù for the given scenario. For each model, we compute the mean error, mean absolute error, variance of error, and root mean square error (RMSE).\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Load the CSV file\ndef load_data(file_path):\n    data = pd.read_csv(file_path)\n    return data\n\n# Process the data to calculate metrics\ndef calculate_metrics(data):\n    # Filter out human predictions as the baseline\n    human_predictions = data[data['model'] == 'human'][['id', 'predicted_risk']]\n    human_predictions.rename(columns={'predicted_risk': 'human_risk'}, inplace=True)\n\n    # Merge human predictions into the main dataset\n    merged_data = data.merge(human_predictions, on='id', how='left')\n\n    # Calculate errors\n    merged_data['error'] = merged_data['predicted_risk'] - merged_data['human_risk']\n    merged_data['absolute_error'] = merged_data['error'].abs()\n\n    # Group by model to calculate metrics\n    model_metrics = merged_data.groupby('model').agg({\n        'error': ['mean', 'var'],\n        'absolute_error': 'mean',\n        'predicted_risk': lambda x: np.sqrt(np.mean((x - merged_data.loc[x.index, 'human_risk']) ** 2))\n    }).reset_index()\n\n    # Rename columns for clarity\n    model_metrics.columns = ['model', 'mean_error', 'variance_error', 'mean_absolute_error', 'rmse']\n    return model_metrics\n\n# Main function to process the file and return a dataframe\ndef main(file_path):\n    data = load_data(file_path)\n\n    # Rename specific models\n    data['model'] = data['model'].replace({\n        'groq/mixtral-8x7b-32768': 'mixtral-8x7b',\n        'groq/llama-3.1-8b-instant': 'llama-3.1-8b'\n    })\n\n    # Calculate and return metrics as a dataframe\n    return calculate_metrics(data)\n\nfile_path = 'data/results.csv'\nmetrics_df = main(file_path)\nmetrics_df['model'] = metrics_df['model'].str.replace('groq/', '')\nmetrics_df.columns = ['model', 'Mean Error', 'Variance of Error', 'Mean Absolute Error', 'RMSE']\nmetrics_df.drop(5).set_index('model').sort_values('RMSE').round(3)\n\n\n\n\nTable¬†1: Model error metrics (as compared to human risk scores)\n\n\n\n\n\n\n\n\n\n\nMean Error\nVariance of Error\nMean Absolute Error\nRMSE\n\n\nmodel\n\n\n\n\n\n\n\n\nllama-3.1-70b-versatile\n-0.030\n0.503\n0.561\n0.710\n\n\ngpt-4o-mini\n-0.027\n0.572\n0.567\n0.756\n\n\ngpt-4o\n0.346\n0.546\n0.588\n0.816\n\n\nllama-3.1-8b\n-0.245\n0.793\n0.732\n0.924\n\n\ngemma2-9b-it\n-0.423\n0.742\n0.780\n0.960\n\n\ngpt-3.5-turbo\n0.007\n0.966\n0.812\n0.983\n\n\nmixtral-8x7b\n-0.698\n0.725\n0.878\n1.101\n\n\n\n\n\n\n\n\n\n\nModel by model scores are listed in Table¬†1, and are ranked by RMSE, which suggests that the model least that is most closely aligned to human performance is Llama-3.1-70b, with a mean error of 0.561. Conversely, the model least aligned is mixtral, with a mean error of 0.878 (nearly double that of Llama 3.1).\nWhile it is challenging to interpret these results, two conclusions stand out. First, this is quite a large variance in ‚Äúperformance‚Äù: the mean absolute error for mixtral is more than 50% larger than that of Llama 70b, despite the fact they are models of broadly similar sizes (though with very different architectures.)\nSecondly, we also see variance in the direction of the mean error by model: while gpt-4o performs well on average, it seems to regularly over-estimate risk compared to human evaluators, with a mean error of 0.346, while mixtral seems to under under-estimate it instead, with a mean error of -0.698. Notably, while gpt-3.5 performs relatively poorly (with an RMSE of 0.983), it‚Äôs mean error is close to 0, suggesting it‚Äôs risk scores reflect human decision makers on average, but suffer from high variance - this is also reflected in the high variance of error.\nTo explore this variance futher, we provide, for each model, their current score on Chatbot Arena, an ‚Äúopen-source platform for evaluating AI through human preference, developed by researchers at UC Berkeley SkyLab and LMSYS‚Äù (see Chiang et al. 2024) - these provide a broad metric of how ‚Äúgood‚Äù human scorers consider the model to be on general tasks.\nWe also compare this with the r-squared value of the model by model regression, which will also reflect the consistency of decision making, with models with the highests r-squared exhibiting predictable and consistent decisions. These scores are listed in Table¬†2.\n\n\n\nTable¬†2: Model variance, RMSE and Arena score\n\n\n\n\n\n\n\n\n\n\n\nModel\nR2 (of model regression)\nRMSE (compared to human)\nArena Score\n\n\n\n\nllama-3.1-70b-versatile\n0.634\n0.710\n1248\n\n\ngpt-4o-mini\n0.825\n0.756\n1273\n\n\ngpt-4o\n0.796\n0.816\n1317\n\n\nllama-3.1-8b-instant\n0.303\n0.924\n1176\n\n\ngemma2-9b-it\n0.276\n0.960\n1191\n\n\ngpt-3.5-turbo\n0.662\n0.983\n1117\n\n\nmixtral-8x7b-32768\n0.786\n1.101\n1114\n\n\n\n\n\n\nFor ease of interpretation, we also visualise these scores in a scatter plot Figure¬†1.\nOn the X axis, we display our regression R-squared value (with models furthest to the right being most consistent), while the Y axis shows our RMSE value (with models nearest to 0 being the closest to human decision makers). Finally, the size of the ‚Äúbubble‚Äù reflects the model score on LM Arena, with larger bubbles indicating answers most preferred by human scorers.\nWe should expect our ‚Äúbest‚Äù models to be at the bottom right of the chart, with a large bubble, indicating highly consistent decisions, that reflect human scores, and are scored highly by humans on general tasks.\nLooking at the plot, while we do the models we expect to perform well in that corner (llama-70b, gpt-4o, and 4o-mini are all grouped in this region), where the rest of the models are placed is notable.\nOur two smallest models (gemma-9b and llama-8b) are clustered together, indicating similar variance and error, but also have comparatively high LM Arena scores, indicating that while they might generate satisfying answers to common tasks, they are comparatively poor in the domain of assessing risk.\nOur ‚Äúmid-range‚Äù models (gpt-3.5 and mixtral) fall broadly in between, but notably there is a fair bit of difference between the scores and consistency of both models (which will continued to be notable as we dig into model by model differences).\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Read and prepare the data\ndata = pd.read_csv('data/model_variance.csv')\n\n# Calculate normalized sizes\nscore_min = data['Arena Score'].min()\nscore_max = data['Arena Score'].max()\nnormalized_sizes = ((data['Arena Score'] - score_min) / (score_max - score_min) * 800) + 200\n\n# Create the plot with extra width to accommodate legend\nplt.figure(figsize=(12, 6))  # Increased width from 12 to 14\n\n# Create scatter plot\nscatter = plt.scatter(data['R2 (of model regression)'], \n                     data['RMSE (compared to human)'],\n                     s=normalized_sizes,\n                     alpha=0.6,\n                     color='blue',\n                     marker='o')\n\n# Add annotations for each point with adjusted offset\nfor i, model in enumerate(data['Model']):\n    plt.annotate(model, \n                (data['R2 (of model regression)'].iloc[i], data['RMSE (compared to human)'].iloc[i]),\n                xytext=(10, 10),  # Increased offset\n                textcoords='offset points',\n                fontsize=9,       # Slightly larger font\n                alpha=0.8)\n\n# Add axis labels and title\nplt.xlabel('R¬≤ (Model Regression)', fontsize=12)\nplt.ylabel('RMSE (Compared to Human)', fontsize=12)\n\n# Create legend with actual sizes matching the scatter plot\nreference_scores = [1100, 1200, 1300]\nreference_sizes = ((np.array(reference_scores) - score_min) / (score_max - score_min) * 800) + 200\n\nlegend_elements = []\nfor score, size in zip(reference_scores, reference_sizes):\n    legend_elements.append(\n        plt.scatter([], [], \n                   s=size,\n                   c='blue',\n                   alpha=0.6,\n                   marker='o',\n                   label=f'Arena Score: {score}')\n    )\n\n# Add legend with improved spacing\nplt.legend(handles=legend_elements,\n          title='Arena Score Reference',\n          bbox_to_anchor=(1.05, 0.5),\n          loc='center left',\n          title_fontsize=10,\n          frameon=True,\n          edgecolor='black',\n          fancybox=True,\n          shadow=True,\n          labelspacing=3,       # Vertical space between entries\n          borderpad=1,          # Padding between legend border and entries\n          handletextpad=8       # Space between marker and text\n          )\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: Scatter plot of model error against humans (RMSE), variance (R2) and arena score\n\n\n\n\n\n\nComparing regression models\nTo compare our models, we will examine the coefficients of each factor (meaning how much, on average, they seem to effect the risk score, attempting to hold other factors constant.) We also generate p-values and error bars at the 95% confidence interval, to examine if the individual factors are significant - p-values are not a perfect tool for this approach, but give us a repeatable rule of thumb we can use for our analysis, allowing us to see if individual factors seem to make a meaningful impact on perceptions of risk. As such, when we refer to significance, this refers to the p&lt;00.5 level, but this should be cautiously interpreted.\n\nHuman specific factors\nWe begin by examining human scorers in isolation in Table¬†3, to look at 2 factors which do not apply to our language models:\n\n\n\nTable¬†3: Regression coefficients for human decision model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n1.5388\n0.216\n7.139\n0.000\n1.115\n1.962\n\n\nis_police_officer\n-0.1656\n0.189\n-0.876\n0.382\n-0.537\n0.206\n\n\nis_police_family\n-0.2228\n0.195\n-1.142\n0.254\n-0.606\n0.161\n\n\nis_public\n-0.3689\n0.195\n-1.896\n0.058\n-0.751\n0.013\n\n\nus_based\n0.6840\n0.309\n2.211\n0.027\n0.076\n1.292\n\n\nbased_elsewhere\n0.4883\n0.230\n2.119\n0.035\n0.036\n0.941\n\n\n\n\n\n\nEach scorer was asked whether they were a police officer, part of the wider policing family, such as retired officers or police staff, or a member of the public (these were each binary variables, so in theory respondents could select all 3). While none are statistically significant, there is a broad trend downwards, with officers likely to perceive high risk than members of the police family, who in turn seem to perceive it higher than the general public.\nOn location, we do seem to have significant results (though our number of returns is relatively small), which suggest that UK based respondents (the reference category) consistently perceived a lower risk (around 0.5-0.7) than those outside the UK (though there is likely to be heavy overlap between this and the previous category)."
  },
  {
    "objectID": "posts/copbot_analysis/analysis.html#individual-factor-comparisons",
    "href": "posts/copbot_analysis/analysis.html#individual-factor-comparisons",
    "title": "Paper in Progress: Variance in AI Perceptions of Risk",
    "section": "Individual factor comparisons",
    "text": "Individual factor comparisons\n\nAge\n\n\nCode\ncoefficient_plots['Age (25 as reference category)']\n\n\n\n\n\n\n\n\nFigure¬†2: Regression coefficients by age (25 as reference category)\n\n\n\n\n\nFigure¬†2 suggests suggests strong age-related patterns in both human and model assessments. Humans show dramatically higher risk assessments for very young ages (5-14) compared to the reference age of 25, with coefficients declining steadily with age, before increasing for our eldest category (with 80 year olds being assessed somewhere between 10 and 14 year olds in terms of risk).\nThe best models, such as GPT4o, closely mirror this pattern, though notably GPT4o consistently has even higher coefficients. Other models show more varied responses, with Llama models and Gemma showing generally lower coefficients than humans, particularly for young ages. Notably, many models show no increase in risk from 50 to 80 years old, and mixtral actually displays a small (but significant) decrease.\n\n\nSex\n\n\nCode\ncoefficient_plots['Sex (female as reference category)']\n\n\n\n\n\n\n\n\nFigure¬†3: Regression coefficients by sex (female as reference category)\n\n\n\n\n\nFigure¬†3 suggests shows significant, but relatively small, effects of sex on risk assessment for both humans and models, with humans reducing their risk by about 0.2 for men. Again, we see the highest performing models behaving broadly similarly (with gpt-4o and llama-70b both reducing their risk by around 0.10-0.15), variance amongst smaller models, with mixtral actually seeing a small but significant increase in risk perception for men.\n\n\nTime Missing\n\n\nCode\ncoefficient_plots['Hours missing (8 as reference category)']\n\n\n\n\n\n\n\n\nFigure¬†4: Regression coefficients by hours missing (8 as reference category)\n\n\n\n\n\nFigure¬†4 is notable because our human risk scorers did not show significant variations in risk assessments based on how long the subject had been reported missing: there is simply too much variance to detect a consistent effect.\nThat is not the case for our models: gpt-4o and gpt-4o-mini both show increases at the ten and 14 hour marks, while interestingly, llama3.1-8b seems to exhibit a notable decrease.\nNotably, nearly all these effects become significantly weaker, or disappear entirely, at the 18 hour mark. The varying factor in the text of this vignette is the time at which they are being reported missing - at the 18 hour mark, the report is being made at 6AM, and the subject has been missing since midday the previous day - and this seems to have strong, unexpected effects on model behaviour.\n\n\nRisk Factors\n\n\nCode\ncoefficient_plots['Risk Factor (out of character as reference category)']\n\n\n\n\n\n\n\n\nFigure¬†5: Regression coefficients by risk factor (out of character as reference category\n\n\n\n\n\nFigure¬†4 shows that for involvement in crime or being a regular missing person (compared to the missing episode being out of character), humans show negative coefficients, suggesting they assess lower risk for these categories.\nThe regular missing person risk factor shows broad consistency along our models, with risk decreasing across the board. Involvement in crime however, shows more variance: gpt-3.5 shows a strong increase in risk, while gpt-4o-mini shows a strong decrease instead. This is quite striking variance, and may reflect the lengthier scenarios.\n\n\nEthnicity\n\n\nCode\ncoefficient_plots['Ethnicity (white as reference category)']\n\n\n\n\n\n\n\n\nFigure¬†6: Regression coefficients by ethnicity (white as reference category)\n\n\n\n\n\nFigure¬†6 shows that ethnicity made no significant difference to the risk assessment of human assessors - they show no apparent racial bias.\nHowever, nearly every single model does: while the differences are small, they all appear significant for at least one ethnicity. Gpt-4o-mini and llama-8b both show increased risk scores of at least 0.1 for subjects that are not white, and most models increase their risk scores for Black and mixed-race subjects.\nNotably, gpt-4o and mixtral both show no bias on Asian or Black subjects, but do for mixed-race."
  },
  {
    "objectID": "posts/copbot_analysis/analysis.html#discussion",
    "href": "posts/copbot_analysis/analysis.html#discussion",
    "title": "Paper in Progress: Variance in AI Perceptions of Risk",
    "section": "Discussion",
    "text": "Discussion\nIn this analysis, we‚Äôve compared responses from humans and large language models, using statistical models to measure where they agree, where they disagree, and where they exhibit bias.\nAt a high level, the results are promising: the most highly performing, ‚Äúflagship‚Äù models, seem to perceive risk and threat in the context of missing people in ways that are similar to human scorers. Scored on a scale from 0 to 4 (where 0 is very low risk and 4 is high risk), a mean error of well under 1 suggest they would lead to outcomes broadly similar to humans, if deployed in a policing context.\nFor certain risk-factors, such as age, these models also vary their perceptions of risk in ways that are broadly reasonable, and similar to human decision makers (for example, perceiving the very young, or very old, as being most at risk.) We also noted that, for at least one risk factor (the number of hours since the suspect had gone missing), some language models identified a clear risk to a subject, where human decision makers did not - this suggests models could be used as safeguards around inaccurate decision making.\nHowever, we also identify some causes for concern. Performance varied significantly across models, even for models of similar size, and some models that are generally perceived to do well on general tasks, did not perform as well in their perception of policing risk. We also saw broad variance with how models evaluated the importance of risk factors: models that might perform well overall might still display unexpected behaviours in certain contexts.\nIn some areas, these behaviours highlighted biases that are likely to be problematic for policing - most notably, when looking at race. While human decision makers exhibited no statistically significant racial bias in their perception of risk, each and every model we evaluated did, for at least one ethnicity category.\n\nLessons for policy and practice\nWhile the findings of this research should be caveated, it does suggest lessons for the application of LLMs in the context of policing, crime and justice, and other domains where perceived risk and threat may be relevant.\nNot all models are equal, but most importantly, models which perform best in generalist contexts might not be best suited to this context. Smaller models can be especially problematic, and while these might lead to savings through reduced costs, are likely to have significant other downsides due to their variabilities and biases.\nWhen Daniel Kahneman examined variance in decision making in ‚ÄúThinking, fast and slow‚Äù (Kahneman 2013), he recommended that companies or agencies where decision making consistency might be critical (such as insurance agencies or judges) undertake a ‚Äúnoise audit‚Äù to examine existing variance in their processes. Those considering the application of large language models to policing domains should aim to run a ‚Äúvirtual audit‚Äù on historical data, examining a range of language models to examine how they perform on the required task.\nThis audit should examine not just variance on the given task, but where variance does exist, does it behave in ways we would expect, and how does it compare to existing human processes. For example, our analysis found while some models treated regularly going missing as a factor that decreased risk, other models saw it as an aggravating factor - you should reassure yourself that any language model being considered for deployment into an operational context at least behaves in line with training and procedures you expect human decision makers to follow.\nFinally, any audit should pay special care biases around race, as our analysis suggest the majority of models attribute some bias in this domain, which does not reflect human decision making. Given the potentially catastrophic implications of algorithmic bias for embedding structural concerns, not to mention legal implications, special care should be paid to this category, as well as other protected characteristics."
  },
  {
    "objectID": "posts/copbot_analysis/analysis.html#footnotes",
    "href": "posts/copbot_analysis/analysis.html#footnotes",
    "title": "Paper in Progress: Variance in AI Perceptions of Risk",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.axon.com/products/draft-one‚Ü©Ô∏é\nhttps://policinginsight.com/feature/advertisement/fighting-the-bad-and-finding-the-good-in-generative-ai/‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andreas Varotsis",
    "section": "",
    "text": "I‚Äôm Andreas, a data scientist and public safety researcher working to improve the use of data and evidence in government. I enjoy coordinating exciting communities, and working with fantastic institutions to build projects that improve the world, including teaching on public sector innovation at Newspeak House. You can read more about me on my about page here.\nIf you‚Äôd like to discuss professional work or collaborating on a project, please say hello!"
  },
  {
    "objectID": "index.html#blog-feed",
    "href": "index.html#blog-feed",
    "title": "Andreas Varotsis",
    "section": "üìÆ Blog Feed",
    "text": "üìÆ Blog Feed\nI use this blog to journal on what I‚Äôve been learning about and building recently. If you‚Äôd like to see more of my formal work, you can see a selection of my papers and writing here.\n\n\n\n\n\n\n\n\n\n\nWhat I want from a National Data Library\n\n\nLots of robust, sustainable open-data, and vibe-coding\n\n\n\ngovtech\n\ncivic tech\n\nopen-source\n\ntimed-post\n\n\n\nI built a tool to fix my local police engagement problem in an hour of caf√© coding. But that was only possible because of a fading public data API from years ago‚Ä¶let‚Äôs revive the open data dream with the National Data Library!\n\n\n\n\n\nOct 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAI at Play - Lessons from a silly benchmark\n\n\n\n\n\n\npersonal\n\ntechnology\n\nai\n\nopen-source\n\ngames\n\n\n\n\n\n\n\n\n\nAug 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto comments, by the open social web\n\n\n\n\n\nAll the comments, all the time, on all the networks? Sure, we can do that.\n\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMy 2025 resolution: share more crappy fiddles (and unfinished research)\n\n\n‚Ä¶and why you should too\n\n\n\npersonal\n\nopen-source\n\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPaper in Progress: Variance in AI Perceptions of Risk\n\n\n\n\n\n\nai\n\npolicing\n\ndata-science\n\nwork-in-progress\n\n\n\n\n\n\n\n\n\nDec 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeeknote - 2024-W35\n\n\n\n\n\n\nweek-notes\n\n\n\nReflections on post-holiday productivity slumps and UK‚Äôs AI education gaps. Exploring tools for converting scientific papers to HTML and AI-powered front-end development. Thoughts on social media disinformation, crime perception, and the allure of new note-taking apps. Plus, intriguing reads on slow productivity and necromancers in space.\n\n\n\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWeeknote - 2024-W34 (2024-08-19)\n\n\n\n\n\n\nweek-notes\n\n\n\nOn productivity during holidays, sharing thoughts on Copbot Online, FastHTML, and a new Obsidian plugin. Plus, interesting reads on Llama 3 models and the challenges of ‚Äúdeliverism‚Äù in government.\n\n\n\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCopbot Online: bias and variance in AI perceptions of risk\n\n\n\n\n\n\npolicing\n\nai\n\nweb-development\n\n\n\nI built a website to look at whether language models could help articulate (or maybe even predict) police risk assessments for missing people. The conclusion? Eeeeh. Maybe. Ish?\n\n\n\n\n\nAug 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Comments, Powered by the Fediverse\n\n\nI built a Quarto extension!\n\n\n\npersonal\n\ntechnology\n\nopen-source\n\n\n\nI built an extension, and now my blog has fancy comments (and yours can too!)\n\n\n\n\n\nApr 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I‚Äôm Contributing to OpenStreetMap for Christmas‚Ä¶\n\n\n‚Ä¶and maybe you should too!\n\n\n\npersonal\n\ntechnology\n\nopen-source\n\ngeospatial\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching OpenAI to assess risk, with CopBot!\n\n\n\n\n\n\npolicing\n\nai\n\ndata-science\n\n\n\n\n\n\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhat‚Äôs Happened to Burglary, and does Attending Help?\n\n\n\n\n\n\npolicing\n\ncrime\n\ndata-science\n\n\n\n\n\n\n\n\n\nDec 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nI‚Äôve migrated to Quarto!\n\n\n\n\n\n\nnews\n\n\n\nI‚Äôve migrated to a fancy new technical writing platform, and I‚Äôm a little in love with it\n\n\n\n\n\nSep 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHackers aren‚Äôt excited about AI (and other bits from CCCamp23)\n\n\n\n\n\n\npersonal\n\ntechnology\n\nai\n\ndata-science\n\n\n\n\n\n\n\n\n\nAug 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLearning GIS in Python - Robbery and Police Searches in Space\n\n\n\n\n\n\ncrime\n\ngeospatial\n\n\n\nUsing Python GIS libraries to explore the spatial distribution of robberies in London\n\n\n\n\n\nMar 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLearning R - Exploring the COVID Crime Effect in London\n\n\n\n\n\n\ndata-science\n\nforecasting\n\ncrime\n\ngeospatial\n\n\n\nI use public London crime data on robbery and burglary to examine where this COVID crime shift was strongest, and whether any specific drivers or correlates can be identified.\n\n\n\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nLAPD Call Prediction for Fun (and Prophet)\n\n\n\n\n\n\ndata-science\n\nforecasting\n\ncrime\n\n\n\nPredicting LAPD police call demand using the Prophet time series forecasting library\n\n\n\n\n\nNov 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter Sentiment Analysis with FastAI\n\n\n\n\n\n\ndata-science\n\nnlp\n\n\n\nUsing open source twitter data to explore sentiment in public debate\n\n\n\n\n\nSep 10, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "PROJECT_NOTES.html",
    "href": "PROJECT_NOTES.html",
    "title": "Project Update Overview",
    "section": "",
    "text": "Established a neon terminal-inspired visual language featuring dark backgrounds, glowing accents, and monospace typography across the site.\nRebuilt the homepage hero with .intro-panel wrappers so Andreas‚Äô introduction and portrait sit naturally side-by-side without excess whitespace.\nRefined blog listings, cards, and interactive elements to better fill available space and maintain the cyberpunk terminal aesthetic.\nTuned Quarto configuration to disable auto tables of contents, open external links in new tabs, and better support the custom styling experience.\n\n\n\n\n\nVerified the site builds and renders locally with quarto check and quarto preview during the redesign iterations.\n\n\n\n\n\nConsider extending the terminal motif to slides and other Quarto documents for complete brand cohesion.\nExplore subtle animated accents (e.g., scanning cursor, grid glow) to further emphasize the sci-fi inspiration while keeping performance in mind."
  },
  {
    "objectID": "PROJECT_NOTES.html#summary",
    "href": "PROJECT_NOTES.html#summary",
    "title": "Project Update Overview",
    "section": "",
    "text": "Established a neon terminal-inspired visual language featuring dark backgrounds, glowing accents, and monospace typography across the site.\nRebuilt the homepage hero with .intro-panel wrappers so Andreas‚Äô introduction and portrait sit naturally side-by-side without excess whitespace.\nRefined blog listings, cards, and interactive elements to better fill available space and maintain the cyberpunk terminal aesthetic.\nTuned Quarto configuration to disable auto tables of contents, open external links in new tabs, and better support the custom styling experience."
  },
  {
    "objectID": "PROJECT_NOTES.html#testing-tooling",
    "href": "PROJECT_NOTES.html#testing-tooling",
    "title": "Project Update Overview",
    "section": "",
    "text": "Verified the site builds and renders locally with quarto check and quarto preview during the redesign iterations."
  },
  {
    "objectID": "PROJECT_NOTES.html#next-steps-ideas",
    "href": "PROJECT_NOTES.html#next-steps-ideas",
    "title": "Project Update Overview",
    "section": "",
    "text": "Consider extending the terminal motif to slides and other Quarto documents for complete brand cohesion.\nExplore subtle animated accents (e.g., scanning cursor, grid glow) to further emphasize the sci-fi inspiration while keeping performance in mind."
  },
  {
    "objectID": "recent_work.html",
    "href": "recent_work.html",
    "title": "Andreas Varotsis",
    "section": "",
    "text": "Talks\n\nTransforming government with AI (TransformGov Talks - October 2024)\nSpotlight on AI Fairness and AI in Government (AI and Deep Learning for Enterprise - July 2024)\nExamining Policing Use of Force and Disproportionality through Data (Global Evidence Based Policing Conference - October 2022)\nThe importance of a multi-disciplinary approach: How data science helps us understand policing & how policing helps us understand our own data (Society of Evidence Based Policing Conference - May 2021)\n\n\n\nPapers\n\nMeasuring Bias and Variance in AI Perceptions of Policing Risk (2025)\nThe Impact of CONNECT Drop 1 on Crime Rates and Detection in the Metropolitan Police: Synthetic Control Analysis (2024)\nImpact of Police Bail on Domestic Revictimisation in London - Evidence from 2017 Reforms in England and Wales (2022)\n\n\n\nArticles (See more at Medium)\n\nAutomating Knife Classification with Machine Learning (April 2021)\nPredicting Police Call Demand for Fun (and Prophet) (Towards Data Science - May 2020)\nLessons from Scraping a DarkNet Market (Towards Data Science - April 2020)"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "",
    "text": "Amongst stuffing my face with wine and cheese, I‚Äôve used this Christmas break to learn more about geospatial modelling in Python.\nThis blog post is largely intended for my reference and to act as a useful example for others‚Ä¶as such, it may be messy! I‚Äôll try and tidy it into a Medium post in the coming weeks.\nSpace is an often disregarded dimension of modelling within policing research. As per Tobler‚Äôs first law of geography, ‚Äúeverything is related to everything else, but near things are more related than distant things‚Äù, and this is probably especially true of crime, that tenders to cluster in both time and space‚Ä¶treating your models as not having distinct physical locations that influence how they behave is likely to miss crucial information.\nNearly all of the below is adapted from a fantastic work in progress book, Geographic Data Science with PySAL and the PyData Stack - I‚Äôve found it hugely helpful, and the code examples are very approachable. I‚Äôd also recommend browsing the Pysal documentation.\nAll of the below is based on public data: - Police recorded crime and searches from January 2019 through October 2020 (data.police.uk) - London MSOA geographic and demographic data (MOPAC)\nThe key libraries used are: - Standard Python libraries as included in Anaconda (statsmodels, pandas, sklearn, seaborn) - Geopandas - allowing you to read, write, and plot spatial data - Pysal - a collection of modules for geospatial data science\n#both of the below are used to read your directory\nimport glob \nimport os\n\n#core DS libraries\nimport pandas as pd\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nimport numpy as np\n\n#graphic libraries\nimport matplotlib.pyplot as plt  \nfrom matplotlib import colors\nimport seaborn as sns             \n\n#geographic analysis\nimport geopandas\nfrom pysal.explore import esda   # Exploratory Spatial analytics\nfrom pysal.lib import weights\nimport contextily                # Background tiles\nfrom pysal.viz import splot\nfrom splot.esda import plot_moran\n\n#positive outcomes to obtain detection rate\npositive_outcomes = [\n       'Offender given conditional discharge', 'Offender fined',\n       'Offender given a drugs possession warning',\n       'Court result unavailable', 'Local resolution',\n       'Offender given community sentence',\n       'Offender given penalty notice', 'Offender given a caution',\n       'Offender sent to prison', 'Court case unable to proceed',\n       'Defendant found not guilty',\n       'Offender given suspended prison sentence',\n       'Awaiting court outcome', 'Offender otherwise dealt with',\n       'Defendant sent to Crown Court', 'Offender deprived of property',\n       'Offender ordered to pay compensation',\n       'Offender given absolute discharge',\n       'Formal action is not in the public interest',\n       'Suspect charged as part of another case']"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-data",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-data",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Spatial Data",
    "text": "Spatial Data\nWe begin by importing our spatial border data. Spatial coordinates are just coordinates, so without understanding what those coordinates mean (for instance, where you are in a city, or in the world, at what altitude, etc), they‚Äôre essentially points on a chart.\nFor us, this is provided by the Mayor‚Äôs Office for Policing and Crime, and also conveniently contains some area characteristics. We use Geopandas to read the file.\nGeospatial modelling relies on assigning events to a unit of space. You could theoretically make this as detailed as possible - for instance, meter squares - but given we‚Äôre going to analyse how our units are interconnected, that‚Äôs probably not computationally feasible (if everything is connected to everything, you‚Äôre going to need a really big computer). You‚Äôll need to reach a suitable compromise. Helpfully, the UK government provides various statistical units, including border coordinates, for download. Lower Super Output Areas (LSOAs) and Middle Super Output Areas (MSOAs) contain populations of between 5,000 and 7,200, and as such should be (partly) comparable.\nGeospatial data will use a specific Coordinate Reference System, or CRS which will affect how your data is processed. Make sure you‚Äôre using the right one - worldwide data that assumes it is on a sphere will behave very differently to data from a specific country on a flat plane.\nUK policing data often uses National Grid Easting/Northing coordinates, rather than the more common Lat/Longs. Daata.Police.uk comes pre-processed into lat/long coordinates. Thankfully, whichever you have, geopandas can easily convert (or ‚Äúre-project‚Äù) to another system.\n\nmsoas = geopandas.read_file(\"statistical-gis-boundaries-london/MapInfo/MSOA_2011_London_gen_MHW.tab\")\nmsoas = msoas.to_crs(epsg=4326)\n\nC:\\Users\\Admin\\Anaconda3\\envs\\geospatial\\lib\\site-packages\\geopandas\\geodataframe.py:422: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance.\n  for feature in features_lst:\n\n\nOur data on both stop and search and robberies was manually downloaded from data.police.uk, and extracted into our working folder. Given this is then returned into a folder per month, I have written a series of helper functions to read the files, assign them to an MSOA, and combine them into a total per MSOA.\nTo assign to an MSOA, we use a ‚Äúspatial join‚Äù: like a normal table join (think vlookup in Excel), this connects elements from one table, to elements from another, via a common column. In our case, the common column is the geographic location: which MSOA is our search/crime located in.\nCrime data from data.police.uk is separated into ‚Äúmajor‚Äù crime types, which is very helpful for anonymisation (we can‚Äôt figure out who the victim was if we don‚Äôt know specific crime types), but does mean that all violent and sexual crime is agglomerated into one - given I think it‚Äôs unlikely searches deter sexual offences, that‚Äôs unhelpful. As such, we‚Äôll focus on robbery, which is disaggregated. This isn‚Äôt ideal, but robbery remains a serious, violent, high priority crime, and as such you‚Äôd hope they are connected, and robbery might in fact be a rough proxy for overall violence.\n\ndef read_and_add(file_path):\n    df = pd.read_csv(file_path)\n    robberies = df[df[\"Crime type\"] == 'Robbery']\n    \n    #we create a geopandas object that uses the lat/long coordinates, using the appropriate CRS code.\n    robberies = geopandas.GeoDataFrame(robberies, geometry=geopandas.points_from_xy(robberies.Longitude, robberies.Latitude), crs=\"epsg:4326\")\n    detected_mask = robberies[\"Last outcome category\"].isin(positive_outcomes)\n    \n    # identify which crimes resulted in a detected outcome \n    robberies.loc[detected_mask, \"Detected\"] = 1\n    detected_mask = robberies[\"Last outcome category\"].isin(positive_outcomes)\n\n    detected_df = robberies[detected_mask]\n    robberies.loc[detected_mask, \"Detected\"] = 1\n\n    dfsjoin_r = geopandas.sjoin(msoas, robberies) #Spatial join Points to polygons\n    dfsjoin_d = geopandas.sjoin(msoas, detected_df) #Spatial join Points to polygons\n\n    #aggregate to a pivot table that will count our outputs.\n    pivot_r = pd.pivot_table(dfsjoin_r,index='MSOA11CD',fill_value=0,columns='Crime type',aggfunc={'Crime type':len})\n    pivot_d = pd.pivot_table(dfsjoin_d,index='MSOA11CD',fill_value=0,columns='Crime type',aggfunc={'Crime type':len})\n    all_pivot = pivot_r.join(pivot_d, rsuffix=\"detected\").reset_index()\n    return all_pivot.fillna(0)\n\n\ndef read_and_add_search(file_path):\n    df = pd.read_csv(file_path)\n    df = geopandas.GeoDataFrame(df, geometry=geopandas.points_from_xy(df.Longitude, df.Latitude), crs=\"epsg:4326\")\n    dfsjoin = geopandas.sjoin(msoas, df) #Spatial join Points to polygons\n    pivot_d = dfsjoin.groupby([\"MSOA11CD\"])['Object of search'].count()\n\n    #pivot_d = pd.pivot_table(dfsjoin,index='MSOA11CD',fill_value=0,columns='Object of search', aggfunc=len)\n    \n    return pd.DataFrame(pivot_d.fillna(0)).reset_index()\n\n\ndef merge_dfs(list_of_df):\n    initial_df = list_of_df[0]\n    remaining = list_of_df[1:]\n    for pivot in remaining:\n        initial_df.groupby(\"MSOA11CD\", as_index=False).sum()\n        initial_df = pivot.merge(initial_df.groupby(\"MSOA11CD\", as_index=False).sum(), how=\"outer\")\n    #initial_df.columns=[\"MSOA11CD\",\"Robbery\",\"Detected\"]\n    print (\"dataframe length is \" + str(initial_df.shape[0]))\n    return initial_df\n\n\n#we iterate over all files and subfolders in our data directory, and use the appropriate helper functions based on their last characters.\nlist_of_pivot = []\nlist_of_searches = []\nrootdir = os.getcwd()\n\n\nfor subdir, dirs, files in os.walk(\"data\"):\n    for file in files:\n        filepath = subdir + os.sep + file\n\n        if filepath.endswith(\"street.csv\"):\n            list_of_pivot.append(read_and_add(filepath))\n            \n        if filepath.endswith(\"search.csv\"):\n            list_of_searches.append(read_and_add_search(filepath))\n\n#now we concatenate all our individual dataframes into a single one\nprint(\"aggregating robbery\")\nfinal_pivot = merge_dfs(list_of_pivot)\n\nprint(\"aggregating searches\")\nfinal_search = merge_dfs(list_of_searches)\n\naggregating robbery\n\n\nC:\\Users\\Admin\\Anaconda3\\envs\\geospatial\\lib\\site-packages\\pandas\\core\\generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n\n\ndataframe length is 1705\naggregating searches\ndataframe length is 1953\n\n\n\n#finally, merge those into a single pivot with the sum of all incidents during our data, by MSOA.\nfinal_pivot.columns=[\"MSOA11CD\",\"Robbery\",\"Detected\"]\nfinal_pivot = final_pivot.groupby(\"MSOA11CD\").sum()\n\nfinal_search = final_search.groupby(\"MSOA11CD\").sum()\n\nfinal_join = final_search.join(final_pivot).reset_index().copy()\nfinal_join.columns=[\"MSOA11CD\", \"search_count\", \"robbery_count\",\"detected_robbery\"]\n\n\nfinal_join\n\n\n\n\n\n\n\n\nMSOA11CD\nsearch_count\nrobbery_count\ndetected_robbery\n\n\n\n\n0\nE02000001\n268\n37\n0.0\n\n\n1\nE02000002\n173\n38\n2.0\n\n\n2\nE02000003\n294\n115\n7.0\n\n\n3\nE02000004\n216\n19\n0.0\n\n\n4\nE02000005\n132\n61\n1.0\n\n\n...\n...\n...\n...\n...\n\n\n978\nE02006927\n581\n68\n7.0\n\n\n979\nE02006928\n638\n89\n5.0\n\n\n980\nE02006929\n1373\n146\n7.0\n\n\n981\nE02006930\n168\n56\n2.0\n\n\n982\nE02006931\n393\n105\n5.0\n\n\n\n\n983 rows √ó 4 columns\n\n\n\n\nfinal_join.sum()\n\nMSOA11CD            E02000001E02000002E02000003E02000004E02000005E...\nsearch_count                                                   477668\nrobbery_count                                                   60093\ndetected_robbery                                                 3054\ndtype: object\n\n\nOur final file then, contains nearly 500,000 searches and just over 60,000 robberies (of which a suspect was found in around 3,000) across London‚Äôs 938 MSOAs.\n\nmsoas[\"MSOA11CD\"] = msoas[\"MSOA11CD\"].astype(\"string\")\nfinal_join[\"MSOA11CD\"] = final_join[\"MSOA11CD\"].astype(\"string\")\n\n\nmsoas = msoas.merge(final_join, on=[\"MSOA11CD\"])\n\nWe then re-combine this with our geometry file, before calculating a proportion of robbery detected figure, and a search per robbery rate - the final table is below.\n\nmsoas[\"robbery_solve\"] = msoas[\"detected_robbery\"] / msoas[\"robbery_count\"]\nmsoas[\"search_rate\"] = msoas[\"search_count\"] / msoas[\"robbery_count\"]\n\n\nmsoas.index = msoas[\"MSOA11NM\"]\n\nThe data is now processed. Now is probably a good time to write this to a file to retrieve it in the future.\n\nmsoas.to_file(\"msoas.shp\")\n\nmsoas\n\n\n\n\n\n\n\n\nMSOA11CD\nMSOA11NM\nLAD11CD\nLAD11NM\nRGN11CD\nRGN11NM\nUsualRes\nHholdRes\nComEstRes\nPopDen\nHholds\nAvHholdSz\ngeometry\nsearch_count\nrobbery_count\ndetected_robbery\nrobbery_solve\nsearch_rate\n\n\nMSOA11NM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCity of London 001\nE02000001\nCity of London 001\nE09000001\nCity of London\nE12000007\nLondon\n7375\n7187\n188\n25.5\n4385\n1.6\nMULTIPOLYGON (((-0.09676 51.52325, -0.09644 51...\n268\n37\n0.0\n0.000000\n7.243243\n\n\nBarking and Dagenham 001\nE02000002\nBarking and Dagenham 001\nE09000002\nBarking and Dagenham\nE12000007\nLondon\n6775\n6724\n51\n31.3\n2713\n2.5\nPOLYGON ((0.14811 51.59678, 0.14809 51.59640, ...\n173\n38\n2.0\n0.052632\n4.552632\n\n\nBarking and Dagenham 002\nE02000003\nBarking and Dagenham 002\nE09000002\nBarking and Dagenham\nE12000007\nLondon\n10045\n10033\n12\n46.9\n3834\n2.6\nPOLYGON ((0.15065 51.58306, 0.14841 51.58075, ...\n294\n115\n7.0\n0.060870\n2.556522\n\n\nBarking and Dagenham 003\nE02000004\nBarking and Dagenham 003\nE09000002\nBarking and Dagenham\nE12000007\nLondon\n6182\n5937\n245\n24.8\n2318\n2.6\nPOLYGON ((0.18511 51.56480, 0.18403 51.56391, ...\n216\n19\n0.0\n0.000000\n11.368421\n\n\nBarking and Dagenham 004\nE02000005\nBarking and Dagenham 004\nE09000002\nBarking and Dagenham\nE12000007\nLondon\n8562\n8562\n0\n72.1\n3183\n2.7\nPOLYGON ((0.14990 51.56807, 0.15078 51.56778, ...\n132\n61\n1.0\n0.016393\n2.163934\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nGreenwich 034\nE02006927\nGreenwich 034\nE09000011\nGreenwich\nE12000007\nLondon\n8315\n8241\n74\n33.0\n3338\n2.5\nPOLYGON ((0.02900 51.46779, 0.02995 51.46592, ...\n581\n68\n7.0\n0.102941\n8.544118\n\n\nGreenwich 035\nE02006928\nGreenwich 035\nE09000011\nGreenwich\nE12000007\nLondon\n7341\n6410\n931\n136.0\n2977\n2.2\nMULTIPOLYGON (((-0.00961 51.48366, -0.00979 51...\n638\n89\n5.0\n0.056180\n7.168539\n\n\nGreenwich 036\nE02006929\nGreenwich 036\nE09000011\nGreenwich\nE12000007\nLondon\n7490\n7489\n1\n29.4\n3333\n2.2\nPOLYGON ((0.01619 51.49578, 0.01854 51.49498, ...\n1373\n146\n7.0\n0.047945\n9.404110\n\n\nGreenwich 037\nE02006930\nGreenwich 037\nE09000011\nGreenwich\nE12000007\nLondon\n6561\n6557\n4\n75.6\n2876\n2.3\nPOLYGON ((0.00866 51.48917, 0.00837 51.48877, ...\n168\n56\n2.0\n0.035714\n3.000000\n\n\nGreenwich 038\nE02006931\nGreenwich 038\nE09000011\nGreenwich\nE12000007\nLondon\n9186\n8973\n213\n46.1\n4113\n2.2\nPOLYGON ((-0.00201 51.48155, -0.00136 51.48145...\n393\n105\n5.0\n0.047619\n3.742857\n\n\n\n\n983 rows √ó 18 columns"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#analysis",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#analysis",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Analysis",
    "text": "Analysis\nLet‚Äôs start exploring our data. Before doing anything, we‚Äôll use Pysal to create ‚Äúspatial weights‚Äù - this let‚Äôs us understand our objects in space, and how they interact. There are a myriad of different methods, but we‚Äôll just pick the 8 nearest MSOAs in space.\n\n# Generate W from the GeoDataFrame\nw = weights.distance.KNN.from_dataframe(msoas, k=8)\nw.neighbors['City of London 001']\n\n['Islington 023',\n 'Southwark 002',\n 'Islington 022',\n 'Hackney 027',\n 'Hackney 026',\n 'Southwark 006',\n 'Southwark 003',\n 'Southwark 009']\n\n\n\n# Row-standardization\nw.transform = 'R'\n\nWe also create a spatial ‚Äúlag‚Äù for our key values - this is one of the most straightforward measures of spatial relationships, and captures the products and weights of our value in neighbouring observations. Essentially, it‚Äôs the value of our metric, weighted by the value of the metric in neighbourhing observations - so clusters will expect to be high, while a single high value surrounded by low values will be diminished.\n\nmsoas['w_search_count'] = weights.spatial_lag.lag_spatial(w, msoas['search_count'])\nmsoas['w_robbery_count'] = weights.spatial_lag.lag_spatial(w, msoas['robbery_count'])\nmsoas['w_rate'] = weights.spatial_lag.lag_spatial(w, msoas['search_rate'])\n\n\nLet‚Äôs start by mapping out our key crime and search figures, and seeing if anything stands out.\n\nf, axs = plt.subplots(nrows=2, ncols=2, figsize=(20, 12))\nmsoas.plot(column='search_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[0,0]\n       )\n\naxs[0,0].title.set_text('Search Count')\n\nmsoas.plot(column='robbery_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[0,1]\n       )\n\naxs[0,1].title.set_text('Robbery Count')\n\nmsoas.plot(column='robbery_solve', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[1,0]\n       )\n\naxs[1,0].title.set_text('Detected Robbery Proportion')\n\nmsoas.plot(column='search_rate', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[1,1]\n       )\n\n#contextily pulls a background tile from online providers\naxs[1,1].title.set_text('Search per Robbery')\nfor ax in axs.reshape(-1):\n        contextily.add_basemap(ax, \n                           crs=msoas.crs, \n                           source=contextily.providers.Stamen.TerrainBackground)\n\n#ax1.set_axis_off()\n#ax2.set_axis_off()\n\n\n\n\n\n\n\n\nThere are a few apparently spatial trends at play: - Searches and robberies seem most common in central London, though searches also seem to cluster on the Western area of the city - The proportion of robberies that are detected seems to be scaterred pretty randomly - The rate of search per robbery seems to form a ‚ÄúU‚Äù shape around the soutehrn side of the city, with the Northern edge standing out as potentially ‚Äúunder-policed‚Äù\nLet‚Äôs ignore the spatial dimension for a second, and just create a correlation heatmap. This should let us identify any commonalities between our measured characteristics\n\ncorr_cols = ['UsualRes', 'HholdRes', 'ComEstRes', 'PopDen', 'Hholds', 'AvHholdSz',\n             'search_count', 'robbery_count',\n       'robbery_solve', 'search_rate',\n             'w_search_count', 'w_robbery_count','w_rate']\n\nsns.set_theme(style=\"white\")\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n\nplt.figure(figsize=(16, 6))\n# define the mask to set the values in the upper triangle to True\nmask = np.triu(np.ones_like(msoas[corr_cols].corr(), dtype=np.bool))\nheatmap = sns.heatmap(msoas[corr_cols].corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap=cmap)\n\n\n\n\n\n\n\n\nThe good news is that search and robbery counts are closely correlated: those MSOAs where most robberies occur also see the most searches.\nThere is also a weak positive correlation (0.3) between commercial households and robberies - this is probably due to some combination of high footfall and affuluence.\nThere is also a negative relationship between the spatial lag of robberies, and the average household size - I‚Äôm not quite sure how to interpret this. Potentially those areas with lots of focused large households are large nexuses of robbery?\nLet‚Äôs go a bit further by building a scatter plot between robbery and search counts.\n\nsns.scatterplot(x=\"search_count\", y=\"robbery_count\", data=msoas)\n\n\n\n\n\n\n\n\nIt‚Äôs a bit of a mess. Most of our robberies are focused on the lower angle, with a few huge outliers. This might benefit from a log transformaton - keeping the key characteristics the same, but normalising our distribution and making the relationship linear.\n\nsns.scatterplot(x=np.log(msoas[\"search_count\"]), y=np.log(msoas[\"robbery_count\"]), data=msoas)\n\n\n\n\n\n\n\n\nAs planned, a lot better - this suggests a linear relationship between both variables, and they should now be normally distributed, allowing us to model them.\n\nsns.displot(x=np.log(msoas[\"search_rate\"]), kde=True)"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-structures",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-structures",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Spatial Structures",
    "text": "Spatial Structures\nTo identify whether geography plays a role in how our values change, we‚Äôll contrast the values to their spatial lag - the local value, impacted by their neighbours. This should smooth out any outliers, and let us get a general value per area, rather than by MSOA.\n\nf, axs = plt.subplots(nrows=3, ncols=2, figsize=(20, 12))\nmsoas.plot(column='search_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[0,0]\n       )\n\naxs[0,0].title.set_text('Search Count')\n\nmsoas.plot(column='w_search_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[0,1]\n       )\n\naxs[0,1].title.set_text('Spatial Lag - Search Count')\n\nmsoas.plot(column='robbery_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[1,0]\n       )\n\naxs[1,0].title.set_text('Robbery Count')\n\nmsoas.plot(column='w_robbery_count', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[1,1]\n       )\n\naxs[1,1].title.set_text('Spatial Lag - Robbery Count')\n\nmsoas.plot(column='search_rate', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[2,0]\n       )\n\naxs[2,0].title.set_text('Search per Robbery')\n\nmsoas.plot(column='w_rate', \n        cmap='viridis', \n        scheme='quantiles',\n        k=5, \n        edgecolor='white', \n        linewidth=0., \n        alpha=0.75, \n        legend=False,\n        legend_kwds={\"loc\": 2},\n        ax=axs[2,1]\n       )\n\naxs[2,1].title.set_text('Spatial Lag - Search Rate')\nfor ax in axs.reshape(-1):\n        contextily.add_basemap(ax, \n                           crs=msoas.crs, \n                           source=contextily.providers.Stamen.TerrainBackground)\n\n#ax1.set_axis_off()\n#ax2.set_axis_off()\n\n\n\n\n\n\n\n\nOur spatial weights enable us to clearly identy trends in space: - the search count and robbery count is now far more focused on centralon London, with the exception of a few pockets - the north of London cluster stands out more distinctly as a location where the rate of search per robbery is distinctively lower"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#clustering",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#clustering",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Clustering",
    "text": "Clustering\nAs an alternative method for exploring our data, we‚Äôll use some machine learning techniques to undertake some clustering. We‚Äôll be using K Means clustering (which focuses on the mean of all our values, and picks the most similar other MSOAs) - given we‚Äôre now focusing on modelling, we‚Äôll use the log values.\n\nmsoas['ln_search_count'] = np.log(msoas['search_count'])\nmsoas['ln_robbery_count'] = np.log(msoas['robbery_count'])\nmsoas['ln_rate'] = np.log(msoas['search_rate'])\n\nmsoas['w_search_count'] = weights.spatial_lag.lag_spatial(w, msoas['ln_search_count'])\nmsoas['w_robbery_count'] = weights.spatial_lag.lag_spatial(w, msoas['w_robbery_count'])\nmsoas['w_rate'] = weights.spatial_lag.lag_spatial(w, msoas['w_rate'])\n\nw = weights.distance.KNN.from_dataframe(msoas, k=8)\n\n# Row-standardization\nw.transform = 'R'\n\n\nWe‚Äôll also include the demographic information provided by MOPAC (household numbers and populations), and use the KMeans Clustering implementation from sklearn. A number of clusters must be specified - I‚Äôve gone for 5 on this occasion. We‚Äôll then map them.\nEssentially, we‚Äôre trying to aggregate MSOAs into 5 distinct groups, based on our crime and demographic characteristics.\n\ncluster_variables = ['UsualRes', 'HholdRes', 'ComEstRes', 'PopDen', 'Hholds', 'AvHholdSz', \n       'robbery_solve', 'w_search_count', 'w_robbery_count',\n       'w_rate', 'ln_search_count', 'ln_robbery_count', 'ln_rate']\n\n# Initialise KMeans instance\nkmeans = KMeans(n_clusters=5)\n\n# Set the seed for reproducibility\nnumpy.random.seed(1234)\n# Run K-Means algorithm\nk5cls = kmeans.fit(msoas[cluster_variables])\n\n# Assign labels into a column\nmsoas['k5cls'] = k5cls.labels_\n# Setup figure and ax\nf, ax = plt.subplots(1, figsize=(14, 9))\n# Plot unique values choropleth including a legend and with no boundary lines\nmsoas.plot(column='k5cls', categorical=True, legend=True, linewidth=0, ax=ax)\n# Remove axis\nax.set_axis_off()\n# Keep axes proportionate\nplt.axis('equal')\n# Add title\nplt.title(r'Geodemographic Clusters (k-means, $k=5$)')\n\ncontextily.add_basemap(ax, crs=msoas.crs, \n                           source=contextily.providers.Stamen.TerrainBackground)\n# Display the map\nplt.show()\n\n\n\n\n\n\n\n\nInterestingly, there are no clear patterns here: cluster membership is spread out throughout London. That said, we still don‚Äôt know what these actually mean - to understand that, we‚Äôll visualise the distributions of our characteristics in each cluster.\n\n# Index db on cluster ID\ntidy_db = msoas.set_index('k5cls')\n# Keep only variables used for clustering\ntidy_db = tidy_db[cluster_variables]\n# Stack column names into a column, obtaining \n# a \"long\" version of the dataset\ntidy_db = tidy_db.stack()\n# Take indices into proper columns\ntidy_db = tidy_db.reset_index()\n# Rename column names\ntidy_db = tidy_db.rename(columns={\n                        'level_1': 'Attribute', \n                        0: 'Values'})\n\n# Setup the facets\nfacets = seaborn.FacetGrid(data=tidy_db, col='Attribute', hue='k5cls', \\\n                  sharey=False, sharex=False, aspect=2, col_wrap=3)\n# Build the plot from `sns.kdeplot`\n_ = facets.map(seaborn.kdeplot, 'Values', shade=True).add_legend()\n\n\n\n\n\n\n\n\nThe defining metric seems to be density, but some other trends stand out: the densest group (3) also sees the largest proportion of robberies and searches, while conversely group 2 is the very opposite."
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-autocorrelation",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#spatial-autocorrelation",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\nWe‚Äôre now quite happy there are some spatial relationships in our data: where you are in London matters. It appears that an MSOA in the South and North of London might expect differnet numbers of searches and robberies, even were all there other characteristics the same. We can test this by checking for spatial auto-correlation in our data, the primary metric of which is Moran‚Äôs I.\n\nmoran = esda.moran.Moran(msoas['search_count'], w)\n\nmoran.I\n\n0.28837097453822225\n\n\nOur Moran‚Äôs I shows weak, positive autocorrelation accross our entire dataset: an MSOA with a high search count will generally be next to similar MSOAs.\n\nmoran.p_sim\n\n0.001\n\n\nThe P statistic suggests this Moran‚Äôs I is statistically different to what we would expect if it were random (at P&lt;0.05) - we reject the hypothesis that there is no spatial dimension.\n\nplot_moran(moran);"
  },
  {
    "objectID": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#regression-modelling",
    "href": "posts/robbery_and_searches_in_space/robbery_and_searches_in_space.html#regression-modelling",
    "title": "Learning GIS in Python - Robbery and Police Searches in Space",
    "section": "Regression Modelling",
    "text": "Regression Modelling\nFinally, we‚Äôre explore regression models. We‚Äôll build a series of models, each attempting to predict the number of searches in an MSOA, based on the number of robberies and the population density. We‚Äôll start by simple OLS models, and then attempt variations that account for spatial weights.\n\nfrom pysal.model import spreg\nfrom pysal.lib import weights\nfrom pysal.explore import esda\nfrom scipy import stats\nimport statsmodels.formula.api as sm\nimport numpy\nimport pandas\nimport geopandas\nimport matplotlib.pyplot as plt\nimport seaborn\nimport patsy\n\n\nOLS\nFirst, we run a simple log OLS model, using the Pysal spreg implementation (you could also get identical results using StatsModels or SKLearn). This doesn‚Äôt use any spatial modelling at all. I‚Äôm using Patsy to transform our Dataframe appropriately.\n\ny, X =  patsy.dmatrices(\"np.log(search_count) ~ np.log(robbery_count) + np.log(PopDen)\", msoas, return_type=\"matrix\")\n\n\ny = np.array(y)\nX = np.array(X)\n\n\nm1 = spreg.OLS(y, X,\n                name_y='Log Search Count', name_x=[\"Constant\",\"Log Robbery Count\",\"Log Pop Density\"])\n\n\nprint(m1.summary)\n\nREGRESSION\n----------\nSUMMARY OF OUTPUT: ORDINARY LEAST SQUARES\n-----------------------------------------\nData set            :     unknown\nWeights matrix      :        None\nDependent Variable  :Log Search Count                Number of Observations:         983\nMean dependent var  :      5.7488                Number of Variables   :           3\nS.D. dependent var  :      0.9163                Degrees of Freedom    :         980\nR-squared           :      0.4181\nAdjusted R-squared  :      0.4169\nSum squared residual:     479.795                F-statistic           :    352.0632\nSigma-square        :       0.490                Prob(F-statistic)     :  5.987e-116\nS.E. of regression  :       0.700                Log likelihood        :   -1042.289\nSigma-square ML     :       0.488                Akaike info criterion :    2090.577\nS.E of regression ML:      0.6986                Schwarz criterion     :    2105.249\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     t-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT       3.1161071       0.1416924      21.9920574       0.0000000\n   Log Robbery Count       0.5845225       0.0260850      22.4083679       0.0000000\n     Log Pop Density       0.1141935       0.0357637       3.1929958       0.0014529\n------------------------------------------------------------------------------------\nWarning: Variable(s) ['Constant'] removed for being constant.\n\nREGRESSION DIAGNOSTICS\nMULTICOLLINEARITY CONDITION NUMBER           15.197\n\nTEST ON NORMALITY OF ERRORS\nTEST                             DF        VALUE           PROB\nJarque-Bera                       2           4.305           0.1162\n\nDIAGNOSTICS FOR HETEROSKEDASTICITY\nRANDOM COEFFICIENTS\nTEST                             DF        VALUE           PROB\nBreusch-Pagan test                2           4.046           0.1322\nKoenker-Bassett test              2           3.543           0.1700\n================================ END OF REPORT =====================================\n\n\nThe model is actually pretty good - the R2 suggests we account for over 40% of the variance, and each of our variables are significant. This is what we‚Äôd expect given the correlation between robbery and search counts.\n\n\nExogeneous Spatial Effects Model\nNow, let‚Äôs use a model including the spatial lag of our exogenous variables. We use the same OLS implementation, and just add the weights as a variable.\n\ny, X =  patsy.dmatrices(\"np.log(search_count) ~ np.log(robbery_count) + np.log(PopDen) + np.log(w_robbery_count)\", msoas, return_type=\"matrix\")\n\n\ny = np.array(y)\nX = np.array(X)\n\n\nm2 = spreg.OLS(y, X,\n                name_y='Log Search Count', name_x=[\"Constant\",\"Log Robbery Count\",\"Log Pop Density\",\"Robbery Spatial Weights\"])\n\n\nprint(m2.summary)\n\nREGRESSION\n----------\nSUMMARY OF OUTPUT: ORDINARY LEAST SQUARES\n-----------------------------------------\nData set            :     unknown\nWeights matrix      :        None\nDependent Variable  :Log Search Count                Number of Observations:         983\nMean dependent var  :      5.7488                Number of Variables   :           4\nS.D. dependent var  :      0.9163                Degrees of Freedom    :         979\nR-squared           :      0.4188\nAdjusted R-squared  :      0.4170\nSum squared residual:     479.214                F-statistic           :    235.1495\nSigma-square        :       0.489                Prob(F-statistic)     :   7.02e-115\nS.E. of regression  :       0.700                Log likelihood        :   -1041.693\nSigma-square ML     :       0.488                Akaike info criterion :    2091.386\nS.E of regression ML:      0.6982                Schwarz criterion     :    2110.948\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     t-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT       2.9955315       0.1797701      16.6631230       0.0000000\n   Log Robbery Count       0.5587104       0.0352339      15.8571653       0.0000000\n     Log Pop Density       0.0987403       0.0384697       2.5666998       0.0104148\nRobbery Spatial Weights       0.0711112       0.0652601       1.0896585       0.2761318\n------------------------------------------------------------------------------------\nWarning: Variable(s) ['Constant'] removed for being constant.\n\nREGRESSION DIAGNOSTICS\nMULTICOLLINEARITY CONDITION NUMBER           27.105\n\nTEST ON NORMALITY OF ERRORS\nTEST                             DF        VALUE           PROB\nJarque-Bera                       2           4.293           0.1169\n\nDIAGNOSTICS FOR HETEROSKEDASTICITY\nRANDOM COEFFICIENTS\nTEST                             DF        VALUE           PROB\nBreusch-Pagan test                3           5.101           0.1645\nKoenker-Bassett test              3           4.506           0.2118\n================================ END OF REPORT =====================================\n\n\nInterestingly, adding a spatial lag does not improve our model at all - the lag is not statistically significant.\n\n\nSpatial Error Models\nInstead of adding a spatial lag, we add a spatial error - effectively assuming we‚Äôre slightly off, in a consistent way, because of our spatial characteristics, and accounting for it.\n\ny, X =  patsy.dmatrices(\"np.log(search_count) ~ 0 + np.log(robbery_count) + np.log(PopDen)\", msoas, return_type=\"matrix\")\ny = np.array(y)\nX = np.array(X)\n\nm6 = spreg.GM_Error_Het(y, X, \n                           w=w, name_y='Log Search Count', name_x=[\"Log Robbery Count\",\"Log Pop Density\"])\nprint(m6.summary)\n\nREGRESSION\n----------\nSUMMARY OF OUTPUT: SPATIALLY WEIGHTED LEAST SQUARES (HET)\n---------------------------------------------------------\nData set            :     unknown\nWeights matrix      :     unknown\nDependent Variable  :Log Search Count                Number of Observations:         983\nMean dependent var  :      5.7488                Number of Variables   :           3\nS.D. dependent var  :      0.9163                Degrees of Freedom    :         980\nPseudo R-squared    :      0.4134\nN. of iterations    :           1                Step1c computed       :          No\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     z-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT       3.5341582       0.2141206      16.5054581       0.0000000\n   Log Robbery Count       0.5823628       0.0365549      15.9311915       0.0000000\n     Log Pop Density       0.0124371       0.0397069       0.3132221       0.7541119\n              lambda       0.6810127       0.0298280      22.8313537       0.0000000\n------------------------------------------------------------------------------------\n================================ END OF REPORT =====================================\n\n\nInterestingly, while our model seems to be just as accurate, population density is no longer significant - this suggests density may be acting as a proxy for our spatial characteristics.\n\n\nSpatial Lag Model\nFinally, we‚Äôll test a model that includes the spatial lag of our dependent variable - the search count varies based on nearby search counts. That breaks the assumptions of traditional OLS regression - we‚Äôre essentially forcing the search count to be on either side of the equation, meaning both will be correlated. Pysal has a specific implementation using two stage least squares to tackle these issues.\n\ny, X =  patsy.dmatrices(\"np.log(search_count) ~ np.log(robbery_count) + np.log(PopDen)\", msoas, return_type=\"matrix\")\ny = np.array(y)\nX = np.array(X)\n\nm7 = spreg.GM_Lag(y, X, w=w, name_y='Log of Search Count', name_x=[\"Constant\",\"Log Robbery Count\",\"Log Pop Density\"])\nprint(m7.summary)\n\nREGRESSION\n----------\nSUMMARY OF OUTPUT: SPATIAL TWO STAGE LEAST SQUARES\n--------------------------------------------------\nData set            :     unknown\nWeights matrix      :     unknown\nDependent Variable  :Log of Search Count                Number of Observations:         983\nMean dependent var  :      5.7488                Number of Variables   :           4\nS.D. dependent var  :      0.9163                Degrees of Freedom    :         979\nPseudo R-squared    :      0.4584\nSpatial Pseudo R-squared:  0.4175\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     z-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT       2.7219978       0.3420296       7.9583682       0.0000000\n   Log Robbery Count       0.5614888       0.0311151      18.0455524       0.0000000\n     Log Pop Density       0.0818749       0.0430088       1.9036779       0.0569522\nW_Log of Search Count       0.1066105       0.0848248       1.2568327       0.2088142\n------------------------------------------------------------------------------------\nInstrumented: W_Log of Search Count\nInstruments: W_Log Pop Density, W_Log Robbery Count\nWarning: Variable(s) ['Constant'] removed for being constant.\n================================ END OF REPORT =====================================\n\n\nThis is a our best model yet. Note that density is once again no longer significant, but our model has improved somewhat - this seems the best fit yet.\n\ny, X =  patsy.dmatrices(\"np.log(search_count) ~ 0 + np.log(robbery_count)\", msoas, return_type=\"matrix\")\ny = np.array(y)\nX = np.array(X)\n\nm7 = spreg.GM_Lag(y, X, w=w, name_y='Log of Search Count', name_x=[\"Log Robbery Count\"])\nprint(m7.summary)\n\nREGRESSION\n----------\nSUMMARY OF OUTPUT: SPATIAL TWO STAGE LEAST SQUARES\n--------------------------------------------------\nData set            :     unknown\nWeights matrix      :     unknown\nDependent Variable  :Log of Search Count                Number of Observations:         983\nMean dependent var  :      5.7488                Number of Variables   :           3\nS.D. dependent var  :      0.9163                Degrees of Freedom    :         980\nPseudo R-squared    :      0.4544\nSpatial Pseudo R-squared:  0.4130\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     z-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT       2.9856130       0.3464531       8.6176549       0.0000000\n   Log Robbery Count       0.5879825       0.0320598      18.3401657       0.0000000\nW_Log of Search Count       0.1040083       0.0724840       1.4349150       0.1513113\n------------------------------------------------------------------------------------\nInstrumented: W_Log of Search Count\nInstruments: W_Log Robbery Count\n================================ END OF REPORT =====================================\n\n\nBased on the pseudo-R2, this is our best model, though I‚Äôm not sure whether it‚Äôs directly comparable, and the spatial term is not significant.\nThat said, we‚Äôve now created a few simple models, using various techniques. We could do an awful lot more to improve these: I suspect accounting for more local characteristics might help (such as presence of tube stations), as well as distance from central London.\nI‚Äôve quite enjoyed learning this, and it‚Äôs been far more approachable than I expected - I‚Äôll keep expanding on this and clean it up in the future!"
  },
  {
    "objectID": "posts/copbot_online/index.html",
    "href": "posts/copbot_online/index.html",
    "title": "Copbot Online: bias and variance in AI perceptions of risk",
    "section": "",
    "text": "Just over a year ago, hot after the release of ChatGPT, I wrote Copbot to examine whether large language models could help articulate (or maybe even predict) police risk assessments for missing people. The conclusion? Eeeeh. Maybe. Ish. Like plenty of LLM use cases, they did a perfectly plausible job, but how comparable to human decision making they were was a whole different kettle of fish.\nThankfully, quite a bit has changed in the interim: models have become better, cheaper, easier to compare, and a nifty new web-framework came out I wanted to try‚Ä¶ so I used all of that to build Copbot Online, a web-service to crowd-source human risk-predictions, and see how they compare to a number of language models. It‚Äôs been running for just over a week now, and the initial results tell us some interesting things about AI decision making around risk.\nCopbot was written with FastHTML, a delightful Python web-framework I‚Äôve falled in love with. I‚Äôll do a post about it in the next few days, but in the meantime, you can find read the source code here.\nAt it‚Äôs simplest, Copbot online is a survey: you‚Äôre given a random scenario (with random variables for factors such as age and ethnicity), and asked to submit a response, on a scale from very low risk to high risk. More importantly, we also ask at least 2 large language models the exact same question, 20 times over, and record their responses. So far, Copbot has recorded just over 50 human predictions, as well as well over 2000 comparable AI predictions.\nIf you haven‚Äôt tried it yet, I‚Äôd really appreciate if you submitted a response‚Ä¶ if you do, you‚Äôll get to see our full results page!\nYou might think this all sounds like an entertaining academic exercise, but you‚Äôd be surprised how many language models we‚Äôre already interacting with today: small models are helping to classify previously unread legislative documents, and police forces have started exploring their use in writing statements. But understanding risk remains a uniquely human task - articulating that risk might be taught to lawyers and police officers in training school, but it‚Äôs not a task commonly documented in public internet documents used to train these models. If different models perceive fear of crime differently, or more worryingly, vary it based on the ethnicity of the subject or victim, then we should understand that before using them in those context."
  },
  {
    "objectID": "posts/copbot_online/index.html#results-so-far",
    "href": "posts/copbot_online/index.html#results-so-far",
    "title": "Copbot Online: bias and variance in AI perceptions of risk",
    "section": "Results so far",
    "text": "Results so far\nWith just over 50 responses, there‚Äôs only so much we can say about how models compare to human perceptions of risk‚Ä¶ but we do see some interesting patterns from our AI generated answers (these might all change, so please treat all the below with caution!)\n\nThe best models broadly reflect some parts of human risk perception‚Ä¶\nThe plot below shows perceived risk, based on the age of the missing person, for both human decision makers, and GPT4o, one of the top performing models.\n\n\n\n\nPredicted risk by subject age\n\n\n\nGPT4 broadly reflects human decision making here: risk is highest for the very youngest and oldest, and shrinks in between. That‚Äôs also the case for other factors: GPT4 is more worried when someone goes missing unexpectedly rather than frequently, or if they‚Äôre known to be involved in crime.\n\n\n‚Ä¶ wherease the smallest models make some very random decisions.\nGPT4o is very consistent in it‚Äôs decision making: if we build a regression model to predict risk based on it‚Äôs answers, the model will explain (eg, the R squared) over 85% of the variance in decision making (quite a bit higher than our human decision makers, where just under 60% of the variance is accounted for).\nThat is absolutely not the case for smaller models: Gemma2-9b and Llama3.1-8b both have an R-squared of around 0.3, meaning around 30% of their decisions are consistently modelled.\n\n\n\n\nRegression summary for Llama3.1-8b\n\n\n\nThis shouldn‚Äôt be overly surprising: these are really small models, with around 9 billion parameters, while GPT4o is estimated to have nearly 2 trillion‚Ä¶ but we shoul be aware that while they might appear to perform acceptably on the surface, individual decisions will be impacted by plenty of random noise. Especially in policing and criminal justice, that‚Äôs something we should be aware of.\n\n\nThey‚Äôre biased too.\nBy now, we‚Äôre probably all familiar with the term unconscious bias, and perhaps also the associated shooter bias - put simply, it‚Äôs possible that an individuals (conscious or unconscious) bias might make them more or less likely to fire on an individual of a given ethnicity. Now, language models aren‚Äôt ‚Äúconscious‚Äù in a traditional sense of the term‚Ä¶ but are they biased? Will ethnicity affect their perception of threat and risk?\nIn some cases, yes. Below we can see the outputs of our regression model for Llama3.1-70b and Gpt4o, and if we take P&lt;0.05 to be meaningful, we can see that the ethnicity of the subject does affect the Llama model‚Äôs perception of risk, with Asian and Black subjects more likely to be graded as high risk, while conversely mixed race subjects are less likely. Conversely, this effect either does not exist or is far less striking for GPT4o, although some bias for Black subjects might still be apparent.\n\n\n\n\n\n\nRegression coefficients for Llama-70b\n\n\n\n\n\n\n\n\n\nRegression coefficients for GPT4o\n\n\n\n\n\nOf course, there are plenty of other variables we could examine: as an examine, most models don‚Äôt seem to alter their assessment based on the sex of the subject, though some do."
  },
  {
    "objectID": "posts/copbot_online/index.html#whats-next",
    "href": "posts/copbot_online/index.html#whats-next",
    "title": "Copbot Online: bias and variance in AI perceptions of risk",
    "section": "What‚Äôs next",
    "text": "What‚Äôs next\nSo, what should we take from the above? Well, not too much, yet: these are only a few hundred results, and they might change‚Ä¶ but we should absolutely not assume models perceive risk and threat in the same way humans do. There is plenty of variance, both between smaller and larger models, and between model families, and they exhibit interesting biases that I wouldn‚Äôt have expected, and are probably caused by some mix of training data and architecture. They‚Äôre also noisy in their decision making: even when they appear accurate overall, they can make some really weird individual decisions.\nI‚Äôm going to keep working on Copbot for awhile, and I hope you‚Äôll help me out by submitting a response (or more)!"
  },
  {
    "objectID": "posts/migrated_to_quarto/index.html",
    "href": "posts/migrated_to_quarto/index.html",
    "title": "I‚Äôve migrated to Quarto!",
    "section": "",
    "text": "Well, isn‚Äôt this all fancy and posh! After much back and forth over a decent blogging platform, I think I‚Äôve finally found a decent, well maintained home ‚Ä¶ meet Quarto!\nSo, what did I want from a blogging platform, and why did it take so long to get here?\nSo, does it work? Is it pretty? It‚Äôs gorgeous.\nI‚Äôve tried a variety of options over the past few years (notably Pelican, Nikola and Hugo) but none of them have quite fit the bill\nYou‚Äôve probably also noted some of the niftier other features of Quarto sprinkled through this, especially for technical writing‚Ä¶ callouts, citations, bibliographies ahoy!\nWith native support for interactive Jupyter widgets, I‚Äôm looking forward to seeing just how many plots and maps I can show off‚Ä¶ It‚Äôs a really promising start! I haven‚Äôt got anything too much to show for now, so instead check out this nifty, automatically generated plot.\nflowchart LR\n  A[Interactive plot?] --&gt; B(Excited audience...)\n  B --&gt; C[Excitement!]\n  B --&gt; D[Confusion!]\nHopefully this is the first post of many on my new and improved blog, and I hope you enjoy it!"
  },
  {
    "objectID": "posts/migrated_to_quarto/index.html#footnotes",
    "href": "posts/migrated_to_quarto/index.html#footnotes",
    "title": "I‚Äôve migrated to Quarto!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are plenty of fancy companies that probably advertise on your favourite podcast or Youtube video, but I wanted to keep it simple.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/OSM_for_christmas/osm_for_xmas.html",
    "href": "posts/OSM_for_christmas/osm_for_xmas.html",
    "title": "Why I‚Äôm Contributing to OpenStreetMap for Christmas‚Ä¶",
    "section": "",
    "text": "In the eternal words of The Thick of it, everything that happens in the world has to happen somewhere. It sounds obvious, but so many good pieces of research or analysis forget just how much where you are matters‚Ä¶and that‚Äôs why, this christmas, I‚Äôve spent an inordinate amount of time contributing to Open Street Map.\nI still remember when I first got really excited about OSM - we were doing some analysis on how to identify a town centre, and needed data on where every single cash machine in the capital was. Now, that‚Äôs a tough problem - banks move them pretty regularly, so short of asking them (or paying Mr and Ms Google), it‚Äôs actually a pretty thorny problem to get to the bottom of. Unless you just ask OpenStreetMap, in which case up to date, surprisingly high-quality data is only an API call away.\n\n\n\nEver wanted to know where all the ATMs in London are?\n\n\nBut OSM isn‚Äôt some flashy, perfect effort by some faceless corporation: it‚Äôs entirely open-souce, with each ATM, building, fire-hydrant and tree painstakingly placed by evvery day people like you‚Ä¶and me. I‚Äôm aware I could just use Google Maps for so much of this stuff, and I risk coming across as some ageing techno-hippy, but I honestly think what the OSM community have achieved is one of the great wonders of the internet - wonderful mapping, available to all for everything from day to day navigation to humanitarian aid, a communal effort rivalling Wikipedia at a fraction of the cost.\nAnd the final cherry on top? Contributing to OSM is fun. I‚Äôve always loved efforts to gamify moving through the real world (from Pokemon Go to Zombies, Run!). So if you‚Äôre looking for some wholesome Christmas fun that will get you moving, and contribute to some real good, try making a few contributions.\n\n\n\nThe full OSM editor, which isn‚Äôt nearly as scary as it looks\n\n\nIn case I‚Äôve inspired you, here are the tools I found really got me started:\n\nStreetComplete - the easiest, smoothest, most gamified OSM experience you can get. This whole app is honestly delightful - just open it up as you walk around, and I‚Äôll guarantee you‚Äôll feel like you‚Äôve achieved something useful in only a few minutes.\nEveryDoor - If you‚Äôve played with StreetComplete, and feel ready to step it up a notch, this app is a great way to get most of the detail of OSM without too much friction.\nThe full OSM editor! Honestly, I spent so long feeling like the proper editor wasn‚Äôt for me - I‚Äôm a newbie enthusiast, not a surveyor - but honestly, I could not have been more wrong. There‚Äôs an interactive tutorial and everything.\n\nSo this Christmas, why not do some good, and go map some stuff!"
  },
  {
    "objectID": "posts/quarto_comments/open-social.html",
    "href": "posts/quarto_comments/open-social.html",
    "title": "Quarto comments, by the open social web",
    "section": "",
    "text": "A few months ago, I released Quarto-mastodon-comments, a quarto extension to add comment functionality to this blog via Mastodon (and the wider fediverse).\nWell, inspired by Terence‚Äôs excellent blog functionality, and building on the web-components by LoueeD and Dpecos, I‚Äôve now added Bluesky (and in theory, AT Protocol) functionality! You can now comment on either network, with all the comments coming together in a nice unified interface. To reflect these changes, I‚Äôve renamed the extension to open-social-comments.\nTo use the extension (or migrate from the old version), just install the new version:\nquarto install extension AndreasThinks/open-social-comments\nYou can then just add the relevant Bluesky and/or Mastodon comments into the header of your post:\nfilters:\n  - open-social-comments\nmastodon_comments:\n  user: \"AndreasThinks\"\n  host: \"fosstodon.org\"\n  toot_id: \"111995180253316042\"\nbluesky_comments:\n  post_uri: \"https://bsky.app/profile/theradr.bsky.social/post/3knoaw5z4ek2v\"\nThis does, sadly, still have the old ‚Äúchicken or egg‚Äù problem, in that you need to publish your social toots/skeets before you can publish the post itself‚Ä¶ but sadly, I haven‚Äôt quite figured out how to get around that yet (I figure someone could build a nice GitHub Actions workflow if they felt suitably inspired).\nIf everything has gone according to plan, you should be able to see comments from both networks below‚Ä¶ I hope you all find it useful!"
  },
  {
    "objectID": "posts/lockdown_effect/index.html",
    "href": "posts/lockdown_effect/index.html",
    "title": "Learning R - Exploring the COVID Crime Effect in London",
    "section": "",
    "text": "The lockdown and social distancing measures that were brought in throughout the world to tackle COVID in 2020 have had a significant, widespread effect on crime. In this notebook, I use public London crime data on robbery and burglary to examine where this ‚ÄúCOVID crime shift‚Äù was strongest, and whether any specific drivers or correlates can be identified. I use three years of Metropolitan Police Service data from data.police.uk.\nThe findings suggest that the relative change in burglary and robbery in April and May 2020 was heavily affected by local characteristics: areas with a high residential population saw the sharpest decreases in burglary (likely due to a reduction in available targets) while the reduction in robberies instead seem to be driven by geographic features and indicators of deprivation (potentially suggesting more available targets for robbery in communities least able to work for from home).\nThe primary purpose of this exercise was to learn R - I‚Äôve previously worked entirely in Python, which is more than sufficient 99% of the time, but has at times proved a blocker when I want to tackle some more experimental geospatial and statistical methods. With that in mind, this is likely to be a little messy, and I‚Äôll aim to condense my main lessons into a blog post in the future. The models are not heavily tuned (aiming to explore correlates rather than provide accurate predictions) and there are likely to be correlation between our various predictors - as such these should not be taken to suggest direct causation.\nThe full code and data for this exercise are available on my Github repo. I‚Äôm hoping to summarise my key lessons in the Python to R journey in Medium post in the next few weeks."
  },
  {
    "objectID": "posts/lockdown_effect/index.html#ingest-data",
    "href": "posts/lockdown_effect/index.html#ingest-data",
    "title": "Learning R - Exploring the COVID Crime Effect in London",
    "section": "Ingest Data",
    "text": "Ingest Data\nFor this exercise, I‚Äôll be importing crime and robbery data by MSOA.MSOAs are geographical units specifically designed for analysis, and to be comparable: they all have an average population of just over 8,000. There is a compromise here between smaller geographical units (that create more variance that may help us identify predictors), but the necessity for enough crime per unit to identify meaningful trends - MSOAs should be suitable.\nTo build our process, we‚Äôll start by taking one month of crime data, exploring it, and writing all our steps for automation.\n\ntest_df &lt;- read.csv(\"crimes/2018-01/2018-01-metropolitan-street.csv\")\n\nOur crime data is categorised according to the Home Office major crime types, and like Python, we can list them all through the ‚Äúunique‚Äù function. Here I‚Äôll be focusing on robbery and burglary: two crime types that are heavily reliant on encountering victim‚Äôs in public spaces, and as such should be affected by the ‚ÄúCOVID effect‚Äù.\nTo avoid this getting particularly computationally intensive, let‚Äôs write a function to pull out robberies and burglaries, and assign them a specific MSOA. Then we can iterate over all our months and get monthly counts for each offence type.\n\nsubset_df &lt;- filter(test_df, Crime.type==\"Burglary\" | Crime.type==\"Robbery\")\nhead(subset_df)\n\n\n\n\n\n\nCrime.ID\n\n\nMonth\n\n\nReported.by\n\n\nFalls.within\n\n\nLongitude\n\n\nLatitude\n\n\nLocation\n\n\nLSOA.code\n\n\nLSOA.name\n\n\nCrime.type\n\n\nLast.outcome.category\n\n\nContext\n\n\n\n\n\n\n628e0d673aa1b6a70479342a64b02884499df85b18dcd63cc9bff3cff9f704bc\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.140035\n\n\n51.58911\n\n\nOn or near Beansland Grove\n\n\nE01000027\n\n\nBarking and Dagenham 001A\n\n\nBurglary\n\n\nOffender sent to prison\n\n\nNA\n\n\n\n\nf8e9db16dca534a83493198a838567aa5adc9dd56496edc2fff5bb4c62b8303e\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.140035\n\n\n51.58911\n\n\nOn or near Beansland Grove\n\n\nE01000027\n\n\nBarking and Dagenham 001A\n\n\nBurglary\n\n\nInvestigation complete; no suspect identified\n\n\nNA\n\n\n\n\ncc34822074b130f141f16d02fdb2d500c86e22ae18324b43a3231b381af3f45c\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.135554\n\n\n51.58499\n\n\nOn or near Rose Lane\n\n\nE01000027\n\n\nBarking and Dagenham 001A\n\n\nBurglary\n\n\nStatus update unavailable\n\n\nNA\n\n\n\n\n10de581c3cd0a8c9b970824cd7589d13148d63a70b3115d95ef6c24dc0bd2c3b\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.140035\n\n\n51.58911\n\n\nOn or near Beansland Grove\n\n\nE01000027\n\n\nBarking and Dagenham 001A\n\n\nBurglary\n\n\nStatus update unavailable\n\n\nNA\n\n\n\n\n50ad5d2dfea24afec9e17218db62b3d29786775db1060634ae7d4a6e7cafc3ff\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.127794\n\n\n51.58419\n\n\nOn or near Hope Close\n\n\nE01000028\n\n\nBarking and Dagenham 001B\n\n\nBurglary\n\n\nStatus update unavailable\n\n\nNA\n\n\n\n\n95abc6eb0b755c9250d19bbe0062fcd4a509b701964d89667401c9dc96ca257d\n\n\n2018-01\n\n\nMetropolitan Police Service\n\n\nMetropolitan Police Service\n\n\n0.138439\n\n\n51.57850\n\n\nOn or near Geneva Gardens\n\n\nE01000029\n\n\nBarking and Dagenham 001C\n\n\nBurglary\n\n\nInvestigation complete; no suspect identified\n\n\nNA\n\n\n\n\n\n\n\nOur single month of data contains 10,501 crimes.\nWe now need to link this to our spatial data. We use the MSOA borders provided by MOPAC, and use the UK National Grid coordinate system. Police.uk does not use that system, so we‚Äôll need to reproject our crime data.\n\nlsoa_borders &lt;- st_read(\"msoa_borders/MSOA_2011_London_gen_MHW.tab\", crs=27700)\n\nReading layer `MSOA_2011_London_gen_MHW' from data source \n  `D:\\Dropbox\\Data Projects\\Covid_crime_shift\\msoa_borders\\MSOA_2011_London_gen_MHW.tab' \n  using driver `MapInfo File'\nSimple feature collection with 983 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB 1936 / British National Grid\n\nplot(lsoa_borders)\n\n\n\n\n\n\n\n\nBefore we can link our crimes to MSOA, we‚Äôll need to ensure identical coordinate systems, and remove any non-geolocated values we‚Äôll need to erase any missing values (while checking we retain enough data for analysis.)\n\n#count missing values in the longitude column\nprint(\"Missing values identified:\")\n\n[1] \"Missing values identified:\"\n\nsum(is.na(subset_df[\"Longitude\"]))\n\n[1] 82\n\n\nThankfully, we only identify 82 crimes which we need to remove, leaving plenty for analysis.\n\nclean_df &lt;- subset_df[!rowSums(is.na(subset_df[\"Longitude\"])), ]\n\nWe can now convert our crime data to spacial data, using our longitude and latitude coordinates - this allows us to quickly plot our data, and confirm it looks right.\n\n\nWarning: plotting the first 9 out of 12 attributes; use max.plot = 12 to plot\nall\n\n\n\n\n\n\n\n\n\nWith our data now mapped, we ensure everything is aligned to the appropriate coordinate system, and assign each crime to an MSOA from our data - the data is then aggregated into a monthly MSOA crime count, to which we assign our monthly date.\n\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\n\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\nMSOA11CD\n\n\nCrime.type\n\n\ncount_by_msoa\n\n\nMonth\n\n\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-01\n\n\n\n\nE02000002\n\n\nBurglary\n\n\n9\n\n\n2018-01\n\n\n\n\nE02000002\n\n\nRobbery\n\n\n1\n\n\n2018-01\n\n\n\n\nE02000003\n\n\nBurglary\n\n\n11\n\n\n2018-01\n\n\n\n\nE02000003\n\n\nRobbery\n\n\n2\n\n\n2018-01\n\n\n\n\nE02000004\n\n\nBurglary\n\n\n10\n\n\n2018-01\n\n\n\n\n\n\n\nBringing together all the code so far into a function, we can create an pipeline to generate our crime count per MSOA time series for the entirety of our dataset.\nFor this project, I haven‚Äôt used the Police.uk API (which would have enabled me to automate the downloads and query the data directly) - as such, we have to iterate over our subfolders, ingesting our CSV data and running through our process.\n\n\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'MSOA11CD'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nMSOA11CD\n\n\nCrime.type\n\n\ncount_by_msoa\n\n\nMonth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-01\n\n\n\n\nE02000002\n\n\nBurglary\n\n\n9\n\n\n2018-01\n\n\n\n\nE02000002\n\n\nRobbery\n\n\n1\n\n\n2018-01\n\n\n\n\nE02000003\n\n\nBurglary\n\n\n11\n\n\n2018-01\n\n\n\n\nE02000003\n\n\nRobbery\n\n\n2\n\n\n2018-01\n\n\n\n\n\n\n\nWe now have a combined dataframe of 71,848 rows, from January 2018 through December 2020.\n\n#saving file to CSV\n#write.csv(empty_df,\"msoa_crime_matrix.csv\")"
  },
  {
    "objectID": "posts/lockdown_effect/index.html#predict-trend-by-msoa",
    "href": "posts/lockdown_effect/index.html#predict-trend-by-msoa",
    "title": "Learning R - Exploring the COVID Crime Effect in London",
    "section": "2. Predict trend by MSOA",
    "text": "2. Predict trend by MSOA\n\nVisualisation and Exploration\nWith our data now cleaned and aggregated, we can focus on the more interesting part - forecasting our ‚Äúexpected‚Äù pandemic crime, and examining how much it diverges from our ‚Äúactual‚Äù crime.\n\nempty_df &lt;- read.csv(\"msoa_crime_matrix.csv\")\nempty_df &lt;- empty_df[2:70848,2:5]\nhead(empty_df)\n\n\n\n\n\n\n\n\nMSOA11CD\n\n\nCrime.type\n\n\ncount_by_msoa\n\n\nMonth\n\n\n\n\n\n\n2\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-01\n\n\n\n\n3\n\n\nE02000002\n\n\nBurglary\n\n\n9\n\n\n2018-01\n\n\n\n\n4\n\n\nE02000002\n\n\nRobbery\n\n\n1\n\n\n2018-01\n\n\n\n\n5\n\n\nE02000003\n\n\nBurglary\n\n\n11\n\n\n2018-01\n\n\n\n\n6\n\n\nE02000003\n\n\nRobbery\n\n\n2\n\n\n2018-01\n\n\n\n\n7\n\n\nE02000004\n\n\nBurglary\n\n\n10\n\n\n2018-01\n\n\n\n\n\n\n\nBefore going any further, let‚Äôs use this to explore and visualise the distribution of robbery and burglary across time and space during our ‚Äúpre-pandemic‚Äù period, in March 2020 - based on London mobility indicators, this is when movement accross London began to be heavily affected, and the disruption was most notable in April\n\n\n\nLondon mobility data\n\n\n\nburglary_df&lt;-empty_df\n\n#add a \"1\" so our month can be converted to a full date\nburglary_df$DateString &lt;- paste(burglary_df$Month, \"-01\", sep=\"\")\n\n#convert to date format\nburglary_df$DateClean &lt;- ymd(burglary_df$DateString)\n\n#filter out only burglary prior to the pandemic\nburglaryExplore &lt;- filter(burglary_df,  DateClean &lt; \"2020-03-01\" & Crime.type==\"Burglary\")\n\nhead(burglaryExplore)\n\n\n\n\n\n\nMSOA11CD\n\n\nCrime.type\n\n\ncount_by_msoa\n\n\nMonth\n\n\nDateString\n\n\nDateClean\n\n\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\nE02000002\n\n\nBurglary\n\n\n9\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\nE02000003\n\n\nBurglary\n\n\n11\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\nE02000004\n\n\nBurglary\n\n\n10\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\nE02000005\n\n\nBurglary\n\n\n6\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\nE02000007\n\n\nBurglary\n\n\n7\n\n\n2018-01\n\n\n2018-01-01\n\n\n2018-01-01\n\n\n\n\n\n\n\nLooking at the aggregate counts of burglary across London, a visual observation suggests yearly trends (which we‚Äôll have to consider in our forecast), which sharp peaks during the Winter months and the lowest numbers in summer (when the days are longest).\n\n#group burglary count by months and plot\nburglary_by_month &lt;- burglaryExplore %&gt;%\n  group_by(DateClean) %&gt;%\n  summarize(total_burglaries = sum(count_by_msoa))\n\nggplot(burglary_by_month, aes(x=DateClean, y=total_burglaries)) +\n  geom_line()\n\n\n\n\n\n\n\n\nTo observe how crime counts are distributed in space, let‚Äôs map both counts by MSOA. As previously mentioned, MSOAs are designed to be comparable units, at least from a population perspective - we don‚Äôt need to produce per population rates.\n\nburglary_by_msoa &lt;- burglaryExplore %&gt;%\n  group_by(MSOA11CD) %&gt;%\n  summarize(total_burglaries = sum(count_by_msoa))\n\n#we join our burglary counts to their geographic msoa\nburglary_map &lt;- left_join(lsoa_borders, burglary_by_msoa, by = \"MSOA11CD\")\n\n#user brewer colour palette https://colorbrewer2.org\npal &lt;- brewer.pal(5,\"BuGn\")\n\n#create our map, and add the layout options\nburglary_map &lt;-tm_shape(burglary_map) +\n  tm_fill(col = \"total_burglaries\", title = \"Total Burglary Count by MSOA\", style=\"quantile\", palette=\"BuGn\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\nrobbery_df&lt;-empty_df\n\nrobbery_df$DateString &lt;- paste(robbery_df$Month, \"-01\", sep=\"\")\nrobbery_df$DateClean &lt;- ymd(robbery_df$DateString)\nrobberyExplore &lt;- filter(robbery_df,  DateClean &lt; \"2020-03-01\" & Crime.type==\"Robbery\")\n\nrobbery_by_msoa &lt;- robberyExplore %&gt;%\n  group_by(MSOA11CD) %&gt;%\n  summarize(total_robberies = sum(count_by_msoa))\n\nrobbery_map &lt;- left_join(lsoa_borders, robbery_by_msoa, by = \"MSOA11CD\")\n\npal &lt;- brewer.pal(5,\"BuGn\")\n\n\nrobbery_map &lt;-tm_shape(robbery_map) +\n  tm_fill(col = \"total_robberies\", title = \"Total Robbery Count by MSOA\", style=\"quantile\", palette=\"BuGn\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\n#arrange the maps together\ntmap_arrange(burglary_map, robbery_map, nrow = 2)\n\n\n\n\n\n\n\n\nWe notice that robbery is noticeably more concentrated in central London, with burglary remaining quite common across the city. That said, there are also obvious spatial patterns here - these crimes are clustered in certain geographies.\n\n\nModelling\nWe can now begin the forecasting process. To design our process, we‚Äôll start by focusing on a single MSOA - the first in our dataset, E02000001, or the City of London.\n\nsingle_msoa_df &lt;- filter(empty_df, MSOA11CD == \"E02000001\" & Crime.type==\"Burglary\")\n\n#we add a 01 to our date to ensure R recognises the date format\nsingle_msoa_df$DateString &lt;- paste(single_msoa_df$Month, \"-01\")\n\n\nsingle_msoa_df$DateClean &lt;- ymd(single_msoa_df$DateString)\nsingle_msoa_df\n\n\n\n\n\n\nMSOA11CD\n\n\nCrime.type\n\n\ncount_by_msoa\n\n\nMonth\n\n\nDateString\n\n\nDateClean\n\n\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-01\n\n\n2018-01 -01\n\n\n2018-01-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-02\n\n\n2018-02 -01\n\n\n2018-02-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2018-03\n\n\n2018-03 -01\n\n\n2018-03-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-04\n\n\n2018-04 -01\n\n\n2018-04-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n4\n\n\n2018-05\n\n\n2018-05 -01\n\n\n2018-05-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n0\n\n\n2018-06\n\n\n2018-06 -01\n\n\n2018-06-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n3\n\n\n2018-07\n\n\n2018-07 -01\n\n\n2018-07-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2018-08\n\n\n2018-08 -01\n\n\n2018-08-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-09\n\n\n2018-09 -01\n\n\n2018-09-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n0\n\n\n2018-10\n\n\n2018-10 -01\n\n\n2018-10-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2018-11\n\n\n2018-11 -01\n\n\n2018-11-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n0\n\n\n2018-12\n\n\n2018-12 -01\n\n\n2018-12-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2019-01\n\n\n2019-01 -01\n\n\n2019-01-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n7\n\n\n2019-02\n\n\n2019-02 -01\n\n\n2019-02-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2019-03\n\n\n2019-03 -01\n\n\n2019-03-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n3\n\n\n2019-04\n\n\n2019-04 -01\n\n\n2019-04-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n6\n\n\n2019-05\n\n\n2019-05 -01\n\n\n2019-05-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n6\n\n\n2019-06\n\n\n2019-06 -01\n\n\n2019-06-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2019-07\n\n\n2019-07 -01\n\n\n2019-07-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2019-08\n\n\n2019-08 -01\n\n\n2019-08-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2019-09\n\n\n2019-09 -01\n\n\n2019-09-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n5\n\n\n2019-10\n\n\n2019-10 -01\n\n\n2019-10-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2019-11\n\n\n2019-11 -01\n\n\n2019-11-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2019-12\n\n\n2019-12 -01\n\n\n2019-12-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n4\n\n\n2020-01\n\n\n2020-01 -01\n\n\n2020-01-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-02\n\n\n2020-02 -01\n\n\n2020-02-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n0\n\n\n2020-03\n\n\n2020-03 -01\n\n\n2020-03-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n0\n\n\n2020-04\n\n\n2020-04 -01\n\n\n2020-04-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-05\n\n\n2020-05 -01\n\n\n2020-05-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-06\n\n\n2020-06 -01\n\n\n2020-06-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2020-07\n\n\n2020-07 -01\n\n\n2020-07-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-08\n\n\n2020-08 -01\n\n\n2020-08-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-09\n\n\n2020-09 -01\n\n\n2020-09-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-10\n\n\n2020-10 -01\n\n\n2020-10-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n1\n\n\n2020-11\n\n\n2020-11 -01\n\n\n2020-11-01\n\n\n\n\nE02000001\n\n\nBurglary\n\n\n2\n\n\n2020-12\n\n\n2020-12 -01\n\n\n2020-12-01\n\n\n\n\n\n\n\nFrom a forecasting/time-series perspective, this is a very small dataset - 36 monthly observations. We will be shrinking this further to only 26 by focusing on data prior to March 2020, when the COVID crime impact is felt. This significantly limits our forecasting options, and will impact accuracy, if we treat each MSOA in isolation - we could explore some sort of Vector Autoregressive Model to limit this, but given that we‚Äôre then going to be exploring the error of all our models in aggregation, this isn‚Äôt crucial. Our focus is on models that we can accurately deploy without needing to tune each of them individually, and that can capture the seasonal trend, and generate reliable predictions on our limited dataset.\nGiven these limitations, I‚Äôve opted for the Prophet algorith. While it‚Äôs more opaque than a auto-arima or VAR model, it works well with monthly data, and extracting seasonal trends. It also requires very little tuning.\nAs such, we‚Äôll extract our ‚Äútraining set‚Äù prior to March, and start forecasting.\n\ntraining_set &lt;- filter(single_msoa_df, DateClean &lt; \"2020-03-01\")\n\ntraining_df &lt;- tibble(\n  ds=training_set$DateClean,\n  y=training_set$count_by_msoa\n)\nhead(training_df)\n\n\n\n\n\n\nds\n\n\ny\n\n\n\n\n\n\n2018-01-01\n\n\n1\n\n\n\n\n2018-02-01\n\n\n1\n\n\n\n\n2018-03-01\n\n\n2\n\n\n\n\n2018-04-01\n\n\n1\n\n\n\n\n2018-05-01\n\n\n4\n\n\n\n\n2018-06-01\n\n\n0\n\n\n\n\n\n\n\n\nlibrary(prophet)\n\nLoading required package: Rcpp\n\n\nLoading required package: rlang\n\n\n\nAttaching package: 'rlang'\n\n\nThe following object is masked from 'package:Metrics':\n\n    ll\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,\n    flatten_lgl, flatten_raw, invoke, splice\n\nm &lt;- prophet(training_df)\n\nDisabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.\n\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n\nn.changepoints greater than number of observations. Using 19\n\n\nFor now, we‚Äôll forecast on a 6 month horizon - we obviously wouldn‚Äôt expect it to be accurate that far into the future.\n\n#prophet generates a future dataframe using our data, for 6 mperiods\nfuture &lt;- make_future_dataframe(m, periods = 6, freq = 'month')\n\n\nforecast &lt;- predict(m, future)\n\nplot(m, forecast)\n\n\n\n\n\n\n\n\nAs we can see, the model seems consistent on a short horizon, and gets very wide as it goes further into the future. More importantly however, it has extracted a yearly seasonal compontent - the summer decrease we identified previously - as well as a long term trend.\n\nprophet_plot_components(m, forecast)\n\n\n\n\n\n\n\n\nThese predictions seem far-fetched, but remember we will be observing a London wide error rate. As such, we must now isolate our ‚Äúpandemic period‚Äù - which we define as April and May 2020 - and compare the predicted crime counts to the actual crime counts to obtain a metric of our ‚ÄúCOVID crime shift‚Äù, or our error rate.\n\nforecast$Month &lt;- month(forecast$ds)\nforecast$Year &lt;- year(forecast$ds)\n\n\nthis_year &lt;- filter(forecast, Year &gt; 2019)\npeak_pandemic &lt;- filter(this_year, Month== 4 | Month== 5 )\n\npredictionPivot &lt;- peak_pandemic %&gt;%\n  group_by(Month) %&gt;%\n  summarize(predicted_burglary = mean(yhat))\n\n\nsingle_msoa_df$MonthNum &lt;- month(single_msoa_df$DateClean)\nsingle_msoa_df$YearNum &lt;- year(single_msoa_df$DateClean)\n\nthis_year_actual &lt;- filter(single_msoa_df, YearNum &gt; 2019)\npeak_pandemic_actual &lt;- filter(this_year_actual, MonthNum== 4 | MonthNum== 5 )\n\nactual_burglary &lt;- sum(peak_pandemic_actual$count_by_msoa)\npred_burglary &lt;- sum(predictionPivot$predicted_burglary)\n\nerror &lt;- actual_burglary - pred_burglary\npercentage_error &lt;- error / pred_burglary \n\nprint(\"Burglary Count\")\n\n[1] \"Burglary Count\"\n\nprint(actual_burglary)\n\n[1] 1\n\nprint(\"Predicted\")\n\n[1] \"Predicted\"\n\nprint(pred_burglary)\n\n[1] 7.625019\n\nprint(\"Actual Error\")\n\n[1] \"Actual Error\"\n\nprint(error)\n\n[1] -6.625019\n\nprint(\"Percentage Error\")\n\n[1] \"Percentage Error\"\n\nprint(percentage_error)\n\n[1] -0.8688528\n\n\nIn this MSOA, our model predicted nearly 8 burglaries would occur in these two months, based on pre-pandemic trends. In reality, 1 took place - a large error rate, suggesting a strong ‚ÄúCOVID effect‚Äù.\nThis process can now be replicated for every MSOA in London, to obtain this metric for each MSOA.\n\nlength(unique(empty_df$MSOA11CD))\n\n[1] 984\n\n\n\n## NULL\n\n\nhead(msoa_error_tibble)\n\n\n\n\n\n\nMSOA11CD\n\n\nburglaryActual\n\n\nburglaryPredicted\n\n\nburglaryError\n\n\nburglaryPercentError\n\n\nrobberyActual\n\n\nrobberyPredicted\n\n\nrobberyError\n\n\nrobberyPercentError\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE02000001\n\n\n1\n\n\n7.62501853994038\n\n\n-6.62501853994038\n\n\n-0.868852777896614\n\n\n1\n\n\n-1.97666206600358\n\n\n2.97666206600358\n\n\n-1.50590336972561\n\n\n\n\nE02000002\n\n\n8\n\n\n-9.23326713435216\n\n\n17.2332671343522\n\n\n-1.86643220472157\n\n\n0\n\n\n1.12957561845153\n\n\n-1.12957561845153\n\n\n-1\n\n\n\n\nE02000003\n\n\n11\n\n\n12.3480006370534\n\n\n-1.34800063705343\n\n\n-0.109167522473914\n\n\n10\n\n\n6.9224283913514\n\n\n3.0775716086486\n\n\n0.444579768061392\n\n\n\n\nE02000004\n\n\n2\n\n\n-4.71960263280976\n\n\n6.71960263280976\n\n\n-1.42376448942892\n\n\n0\n\n\n-1.27129225183737\n\n\n1.27129225183737\n\n\n-1\n\n\n\n\nE02000005\n\n\n4\n\n\n4.58490182624086\n\n\n-0.584901826240862\n\n\n-0.127571286890655\n\n\n1\n\n\n9.21402738381341\n\n\n-8.21402738381341\n\n\n-0.89146982547971\n\n\n\n\n\n\n\nOur process has completed: we have a ‚ÄúCOVID shift‚Äù measure for all of London."
  },
  {
    "objectID": "posts/lockdown_effect/index.html#measuring-local-covid-crime-shifts",
    "href": "posts/lockdown_effect/index.html#measuring-local-covid-crime-shifts",
    "title": "Learning R - Exploring the COVID Crime Effect in London",
    "section": "3. Measuring Local COVID Crime Shifts",
    "text": "3. Measuring Local COVID Crime Shifts\nWe now need to use our forecasts to measure the ‚Äúerror‚Äù - this should provide an indication of the ‚ÄúCOVID Crime Shift‚Äù, or how much the actual crime diverted from the previous forecasts.\nI explored various avenues for this: the ideal solution would be a relative rate of the error, as MSOAs with large crime numbers will likely generate large errors, and so a rate would be ideal, though this is complicated by our erratic prediction and mix of positive and negative numbers.\nOur final solution has explored two options: - the absolute error number - the relative error once the crime and predictions have been transformed (by adding 50)\n\\[\nactual_{k} = actual + 50\n\\]\n\\[\npredicted_{k} = predicted + 50\n\\]\n\\[\nRPD = \\frac{(actual_{k} - predicted_{k})}  {(actual_{k} + predicted_{k})/2}\n\\]\nWe visualise and describe these statistics first to ensure they appear sensible.\n\nmsoa_error_tibble &lt;- read_csv(\"msoa_error_table2.csv\")\n\nRows: 980 Columns: 9\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): MSOA11CD\ndbl (8): burglaryActual, burglaryPredicted, burglaryError, burglaryPercentEr...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmsoa_error_tibble[,2:9] &lt;- lapply(msoa_error_tibble[,2:9], as.numeric)\n\nmsoa_error_tibble &lt;- msoa_error_tibble[2:980, ]\n\nmsoa_error_tibble &lt;- left_join(msoa_error_tibble, robbery_by_msoa, by = \"MSOA11CD\")\nmsoa_error_tibble &lt;- left_join(msoa_error_tibble, burglary_by_msoa, by = \"MSOA11CD\")\n\n\nmsoa_error_tibble$RPDBurglary &lt;- (msoa_error_tibble$burglaryActual - msoa_error_tibble$burglaryPredicted)/((msoa_error_tibble$burglaryPredicted + msoa_error_tibble$burglaryActual)/2)\n\nmsoa_error_tibble$RPDRobbery &lt;- (msoa_error_tibble$robberyActual - msoa_error_tibble$robberyPredicted)/((msoa_error_tibble$robberyPredicted + msoa_error_tibble$robberyActual)/2)\n\nmsoa_error_tibble$robberyActualShifted &lt;- msoa_error_tibble$robberyActual + 50\nmsoa_error_tibble$robberyPredictedShifted &lt;- msoa_error_tibble$robberyPredicted + 50\n\n\nmsoa_error_tibble$RPDRobberyShifted &lt;- (msoa_error_tibble$robberyActualShifted - msoa_error_tibble$robberyPredictedShifted)/((msoa_error_tibble$robberyPredictedShifted + msoa_error_tibble$robberyActualShifted)/2)\n\nmsoa_error_tibble$burglaryActualShifted &lt;- msoa_error_tibble$burglaryActual + 50\nmsoa_error_tibble$burglaryPredictedShifted &lt;- msoa_error_tibble$burglaryPredicted + 50\n\n\nmsoa_error_tibble$RPDburglaryShifted &lt;- (msoa_error_tibble$burglaryActualShifted - msoa_error_tibble$burglaryPredictedShifted)/((msoa_error_tibble$burglaryPredictedShifted + msoa_error_tibble$burglaryActualShifted)/2)\n\n\nprint(\"Burglary Error\")\n\n[1] \"Burglary Error\"\n\nsummary(msoa_error_tibble$burglaryError)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-97.191 -12.870  -5.271  -6.090   2.111  48.727 \n\nprint(\"Burglary Relative Error\")\n\n[1] \"Burglary Relative Error\"\n\nsummary(msoa_error_tibble$RPDburglaryShifted)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.81266 -0.20689 -0.08826 -0.07976  0.03612  1.29467 \n\nprint(\"Robbery Error\")\n\n[1] \"Robbery Error\"\n\nsummary(msoa_error_tibble$robberyError)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-309.618   -6.577   -2.150   -3.840    2.205   25.564 \n\nprint(\"Robbery Relative Error\")\n\n[1] \"Robbery Relative Error\"\n\nsummary(msoa_error_tibble$RPDRobberyShifted)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-1.45491 -0.11808 -0.04081 -0.04876  0.04309  0.62020 \n\n\nAs we can see, the average London MSOA experienced a negative COVID crime shift for both burglary and robbery, but this is far from equally distributed - at the extremes, some areas actually see large increases on our predicted values.\n\nburg_hist &lt;- ggplot(msoa_error_tibble, aes(x=burglaryError)) + geom_histogram()\nrob_hist &lt;-ggplot(msoa_error_tibble, aes(x=robberyError)) + geom_histogram()\nburg_r_hist &lt;- ggplot(msoa_error_tibble, aes(x=RPDburglaryShifted)) + geom_histogram()\nrob_r_hist &lt;- ggplot(msoa_error_tibble, aes(x=RPDRobberyShifted)) + geom_histogram()\nscatter &lt;- ggplot(msoa_error_tibble, aes(x = RPDRobberyShifted, y = RPDburglaryShifted)) +\n  geom_point()\n\nr_scatter &lt;- ggplot(msoa_error_tibble, aes(x = robberyError, y = burglaryError)) +\n  geom_point()\n\nggarrange(rob_hist, burg_hist, rob_r_hist, burg_r_hist,scatter, r_scatter, ncol=2, nrow=3 )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nOur shifted relative error rate seems to function as intended: while there are still outliers, they are more concentrated than they are for the pure error term, and the overall distribution is more focused, while still indicating the direction and relative strength of our COVID effect.\nLet‚Äôs map this effect visually, and see if any particular areas stand out.\n\n#re-ingest our geographic MSOA borders\nmsoa_borders &lt;- st_read(\"msoa_borders/MSOA_2011_London_gen_MHW.tab\", crs=27700)\n\nReading layer `MSOA_2011_London_gen_MHW' from data source \n  `D:\\Dropbox\\Data Projects\\Covid_crime_shift\\msoa_borders\\MSOA_2011_London_gen_MHW.tab' \n  using driver `MapInfo File'\nSimple feature collection with 983 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB 1936 / British National Grid\n\ngeographic_error_map &lt;- left_join(msoa_borders, msoa_error_tibble, by = \"MSOA11CD\")\n\nburg_map &lt;- tm_shape(geographic_error_map) +\n  tm_fill(col = \"robberyError\", title = \"Robbery Error\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\nrob_map &lt;-tm_shape(geographic_error_map) +\n  tm_fill(col = \"burglaryError\", title = \"Burglary  Error\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\nburg_map_rate &lt;- tm_shape(geographic_error_map) +\n  tm_fill(col = \"RPDRobberyShifted\", title = \"Robbery Error Relative\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\nrob_map_rate &lt;-tm_shape(geographic_error_map) +\n  tm_fill(col = \"RPDburglaryShifted\", title = \"Burglary  Error Relative\", palette=\"-PuOr\")+\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\ntmap_arrange(burg_map, rob_map, burg_map_rate, rob_map_rate , nrow = 2, ncol=2)\n\nVariable(s) \"robberyError\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nVariable(s) \"burglaryError\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nVariable(s) \"RPDRobberyShifted\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nVariable(s) \"RPDburglaryShifted\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\n\n\nIt‚Äôs hard to identify any obvious effect visually, but we do notice that while central London sees some very strong reductions, it also sees some increases. Conversely, the outskirts of London (notably to the south and West) are a near continuous area of large decreases. The effect does vary by offence type, but the pattern seen in South and West London appears broadly consistent."
  },
  {
    "objectID": "posts/lockdown_effect/index.html#identifying-correlates-and-modelling",
    "href": "posts/lockdown_effect/index.html#identifying-correlates-and-modelling",
    "title": "Learning R - Exploring the COVID Crime Effect in London",
    "section": "Identifying Correlates and Modelling",
    "text": "Identifying Correlates and Modelling\nWe‚Äôve identified that the COVID crime effect was felt unequally accross London, and varies by offence type. To finalise our project, we will be linking our data to demographic data provided by MOPAC, and aiming to use it to identify correlates to our ‚Äúcovid shift‚Äù, and hopefully build models disentangling the effect.\n\nlibrary(readxl)\n#ingest ATLAS\nmsoa_atlas &lt;- read_excel(\"msoa_atlas/msoa-data.xls\")\n\nNew names:\n‚Ä¢ `House Prices Sales 2011` -&gt; `House Prices Sales 2011...129`\n‚Ä¢ `House Prices Sales 2011` -&gt; `House Prices Sales 2011...130`\n\n#join by MSOA\ngeographic_msoa_matrix &lt;- left_join(geographic_error_map, msoa_atlas, by = \"MSOA11CD\")\n\n#convert to tibble\nmsoa_matrix_tbl &lt;- as_tibble(geographic_msoa_matrix)\nwrite_csv(msoa_matrix_tbl, \"msoa_matrix.csv\")\n\n\n#select only numeric data\nmsoa_matrix_numeric &lt;-dplyr::select_if(msoa_matrix_tbl, is.numeric)\nhead(msoa_matrix_numeric)\n\n\n\n\n\n\nUsualRes\n\n\nHholdRes\n\n\nComEstRes\n\n\nPopDen\n\n\nHholds\n\n\nAvHholdSz\n\n\nburglaryActual\n\n\nburglaryPredicted\n\n\nburglaryError\n\n\nburglaryPercentError\n\n\nrobberyActual\n\n\nrobberyPredicted\n\n\nrobberyError\n\n\nrobberyPercentError\n\n\ntotal_robberies\n\n\ntotal_burglaries\n\n\nRPDBurglary\n\n\nRPDRobbery\n\n\nrobberyActualShifted\n\n\nrobberyPredictedShifted\n\n\nRPDRobberyShifted\n\n\nburglaryActualShifted\n\n\nburglaryPredictedShifted\n\n\nRPDburglaryShifted\n\n\nAge Structure (2011 Census) All Ages\n\n\nAge Structure (2011 Census) 0-15\n\n\nAge Structure (2011 Census) 16-29\n\n\nAge Structure (2011 Census) 30-44\n\n\nAge Structure (2011 Census) 45-64\n\n\nAge Structure (2011 Census) 65+\n\n\nAge Structure (2011 Census) Working-age\n\n\nMid-year Estimate totals All Ages 2002\n\n\nMid-year Estimate totals All Ages 2003\n\n\nMid-year Estimate totals All Ages 2004\n\n\nMid-year Estimate totals All Ages 2005\n\n\nMid-year Estimate totals All Ages 2006\n\n\nMid-year Estimate totals All Ages 2007\n\n\nMid-year Estimate totals All Ages 2008\n\n\nMid-year Estimate totals All Ages 2009\n\n\nMid-year Estimate totals All Ages 2010\n\n\nMid-year Estimate totals All Ages 2011\n\n\nMid-year Estimate totals All Ages 2012\n\n\nMid-year Estimates 2012, by age % 0 to 14\n\n\nMid-year Estimates 2012, by age % 15-64\n\n\nMid-year Estimates 2012, by age % 65+\n\n\nMid-year Estimates 2012, by age 0-4\n\n\nMid-year Estimates 2012, by age 5-9\n\n\nMid-year Estimates 2012, by age 10-14\n\n\nMid-year Estimates 2012, by age 15-19\n\n\nMid-year Estimates 2012, by age 20-24\n\n\nMid-year Estimates 2012, by age 25-29\n\n\nMid-year Estimates 2012, by age 30-34\n\n\nMid-year Estimates 2012, by age 35-39\n\n\nMid-year Estimates 2012, by age 40-44\n\n\nMid-year Estimates 2012, by age 45-49\n\n\nMid-year Estimates 2012, by age 50-54\n\n\nMid-year Estimates 2012, by age 55-59\n\n\nMid-year Estimates 2012, by age 60-64\n\n\nMid-year Estimates 2012, by age 65-69\n\n\nMid-year Estimates 2012, by age 70-74\n\n\nMid-year Estimates 2012, by age 75-79\n\n\nMid-year Estimates 2012, by age 80-84\n\n\nMid-year Estimates 2012, by age 85-89\n\n\nMid-year Estimates 2012, by age 90+\n\n\nHouseholds (2011) All Households\n\n\nHousehold Composition (2011) Numbers Couple household with dependent children\n\n\nHousehold Composition (2011) Numbers Couple household without dependent children\n\n\nHousehold Composition (2011) Numbers Lone parent household\n\n\nHousehold Composition (2011) Numbers One person household\n\n\nHousehold Composition (2011) Numbers Other household Types\n\n\nHousehold Composition (2011) Percentages Couple household with dependent children\n\n\nHousehold Composition (2011) Percentages Couple household without dependent children\n\n\nHousehold Composition (2011) Percentages Lone parent household\n\n\nHousehold Composition (2011) Percentages One person household\n\n\nHousehold Composition (2011) Percentages Other household Types\n\n\nEthnic Group (2011 Census) White\n\n\nEthnic Group (2011 Census) Mixed/multiple ethnic groups\n\n\nEthnic Group (2011 Census) Asian/Asian British\n\n\nEthnic Group (2011 Census) Black/African/Caribbean/Black British\n\n\nEthnic Group (2011 Census) Other ethnic group\n\n\nEthnic Group (2011 Census) BAME\n\n\nEthnic Group (2011 Census) White (%)\n\n\nEthnic Group (2011 Census) Mixed/multiple ethnic groups (%)\n\n\nEthnic Group (2011 Census) Asian/Asian British (%)\n\n\nEthnic Group (2011 Census) Black/African/Caribbean/Black British (%)\n\n\nEthnic Group (2011 Census) Other ethnic group (%)\n\n\nEthnic Group (2011 Census) BAME (%)\n\n\nCountry of Birth (2011) United Kingdom\n\n\nCountry of Birth (2011) Not United Kingdom\n\n\nCountry of Birth (2011) United Kingdom (%)\n\n\nCountry of Birth (2011) Not United Kingdom (%)\n\n\nHousehold Language (2011) At least one person aged 16 and over in household has English as a main language\n\n\nHousehold Language (2011) No people in household have English as a main language\n\n\nHousehold Language (2011) % of people aged 16 and over in household have English as a main language\n\n\nHousehold Language (2011) % of households where no people in household have English as a main language\n\n\nReligion (2011) Christian\n\n\nReligion (2011) Buddhist\n\n\nReligion (2011) Hindu\n\n\nReligion (2011) Jewish\n\n\nReligion (2011) Muslim\n\n\nReligion (2011) Sikh\n\n\nReligion (2011) Other religion\n\n\nReligion (2011) No religion\n\n\nReligion (2011) Religion not stated\n\n\nReligion (2011) Christian (%)\n\n\nReligion (2011) Buddhist (%)\n\n\nReligion (2011) Hindu (%)\n\n\nReligion (2011) Jewish (%)\n\n\nReligion (2011) Muslim (%)\n\n\nReligion (2011) Sikh (%)\n\n\nReligion (2011) Other religion (%)\n\n\nReligion (2011) No religion (%)\n\n\nReligion (2011) Religion not stated (%)\n\n\nTenure (2011) Owned: Owned outright\n\n\nTenure (2011) Owned: Owned with a mortgage or loan\n\n\nTenure (2011) Social rented\n\n\nTenure (2011) Private rented\n\n\nTenure (2011) Owned: Owned outright (%)\n\n\nTenure (2011) Owned: Owned with a mortgage or loan (%)\n\n\nTenure (2011) Social rented (%)\n\n\nTenure (2011) Private rented (%)\n\n\nDwelling type (2011) Household spaces with at least one usual resident\n\n\nDwelling type (2011) Household spaces with no usual residents\n\n\nDwelling type (2011) Whole house or bungalow: Detached\n\n\nDwelling type (2011) Whole house or bungalow: Semi-detached\n\n\nDwelling type (2011) Whole house or bungalow: Terraced (including end-terrace)\n\n\nDwelling type (2011) Flat, maisonette or apartment\n\n\nDwelling type (2011) Household spaces with at least one usual resident (%)\n\n\nDwelling type (2011) Household spaces with no usual residents (%)\n\n\nDwelling type (2011) Whole house or bungalow: Detached (%)\n\n\nDwelling type (2011) Whole house or bungalow: Semi-detached (%)\n\n\nDwelling type (2011) Whole house or bungalow: Terraced (including end-terrace) (%)\n\n\nDwelling type (2011) Flat, maisonette or apartment (%)\n\n\nLand Area Hectares\n\n\nPopulation Density Persons per hectare (2012)\n\n\nHouse Prices Median House Price (¬£) 2005\n\n\nHouse Prices Median House Price (¬£) 2006\n\n\nHouse Prices Median House Price (¬£) 2007\n\n\nHouse Prices Median House Price (¬£) 2008\n\n\nHouse Prices Median House Price (¬£) 2009\n\n\nHouse Prices Median House Price (¬£) 2010\n\n\nHouse Prices Median House Price (¬£) 2011\n\n\nHouse Prices Median House Price (¬£) 2012\n\n\nHouse Prices Median House Price (¬£) 2013 (p)\n\n\nHouse Prices Sales 2005\n\n\nHouse Prices Sales 2006\n\n\nHouse Prices Sales 2007\n\n\nHouse Prices Sales 2008\n\n\nHouse Prices Sales 2009\n\n\nHouse Prices Sales 2010\n\n\nHouse Prices Sales 2011‚Ä¶129\n\n\nHouse Prices Sales 2011‚Ä¶130\n\n\nHouse Prices Sales 2013(p)\n\n\nQualifications (2011 Census) No qualifications\n\n\nQualifications (2011 Census) Highest level of qualification: Level 1 qualifications\n\n\nQualifications (2011 Census) Highest level of qualification: Level 2 qualifications\n\n\nQualifications (2011 Census) Highest level of qualification: Apprenticeship\n\n\nQualifications (2011 Census) Highest level of qualification: Level 3 qualifications\n\n\nQualifications (2011 Census) Highest level of qualification: Level 4 qualifications and above\n\n\nQualifications (2011 Census) Highest level of qualification: Other qualifications\n\n\nQualifications (2011 Census) Schoolchildren and full-time students: Age 18 and over\n\n\nEconomic Activity (2011 Census) Economically active: Total\n\n\nEconomic Activity (2011 Census) Economically active: Unemployed\n\n\nEconomic Activity (2011 Census) Economically inactive: Total\n\n\nEconomic Activity (2011 Census) Economically active %\n\n\nEconomic Activity (2011 Census) Unemployment Rate\n\n\nEconomic Activity (2011 Census) Economically inactive %\n\n\nAdults in Employment (2011 Census) No adults in employment in household: With dependent children\n\n\nAdults in Employment (2011 Census) % of households with no adults in employment: With dependent children\n\n\nHousehold Income Estimates (2011/12) Total Mean Annual Household Income (¬£)\n\n\nHousehold Income Estimates (2011/12) Total Median Annual Household Income (¬£)\n\n\nIncome Deprivation (2010) % living in income deprived households reliant on means tested benefit\n\n\nIncome Deprivation (2010) % of people aged over 60 who live in pension credit households\n\n\nLone Parents (2011 Census) All lone parent housholds with dependent children\n\n\nLone Parents (2011 Census) Lone parents not in employment\n\n\nLone Parents (2011 Census) Lone parent not in employment %\n\n\nCentral Heating (2011 Census) Households with central heating (%)\n\n\nHealth (2011 Census) Day-to-day activities limited a lot\n\n\nHealth (2011 Census) Day-to-day activities limited a little\n\n\nHealth (2011 Census) Day-to-day activities not limited\n\n\nHealth (2011 Census) Day-to-day activities limited a lot (%)\n\n\nHealth (2011 Census) Day-to-day activities limited a little (%)\n\n\nHealth (2011 Census) Day-to-day activities not limited (%)\n\n\nHealth (2011 Census) Very good health\n\n\nHealth (2011 Census) Good health\n\n\nHealth (2011 Census) Fair health\n\n\nHealth (2011 Census) Bad health\n\n\nHealth (2011 Census) Very bad health\n\n\nHealth (2011 Census) Very good health (%)\n\n\nHealth (2011 Census) Good health (%)\n\n\nHealth (2011 Census) Fair health (%)\n\n\nHealth (2011 Census) Bad health (%)\n\n\nHealth (2011 Census) Very bad health (%)\n\n\nLow Birth Weight Births (2007-2011) Low Birth Weight Births (%)\n\n\nLow Birth Weight Births (2007-2011) LCL - Lower confidence limit\n\n\nLow Birth Weight Births (2007-2011) UCL - Upper confidence limit\n\n\nObesity % of measured children in Year 6 who were classified as obese, 2009/10-2011/12\n\n\nObesity Percentage of the population aged 16+ with a BMI of 30+, modelled estimate, 2006-2008\n\n\nIncidence of Cancer All\n\n\nIncidence of Cancer Breast Cancer\n\n\nIncidence of Cancer Colorectal Cancer\n\n\nIncidence of Cancer Lung Cancer\n\n\nIncidence of Cancer Prostate Cancer\n\n\nLife Expectancy Males\n\n\nLife Expectancy Females\n\n\nCar or van availability (2011 Census) No cars or vans in household\n\n\nCar or van availability (2011 Census) 1 car or van in household\n\n\nCar or van availability (2011 Census) 2 cars or vans in household\n\n\nCar or van availability (2011 Census) 3 cars or vans in household\n\n\nCar or van availability (2011 Census) 4 or more cars or vans in household\n\n\nCar or van availability (2011 Census) Sum of all cars or vans in the area\n\n\nCar or van availability (2011 Census) No cars or vans in household (%)\n\n\nCar or van availability (2011 Census) 1 car or van in household (%)\n\n\nCar or van availability (2011 Census) 2 cars or vans in household (%)\n\n\nCar or van availability (2011 Census) 3 cars or vans in household (%)\n\n\nCar or van availability (2011 Census) 4 or more cars or vans in household (%)\n\n\nCar or van availability (2011 Census) Cars per household\n\n\nRoad Casualties 2010 Fatal\n\n\nRoad Casualties 2010 Serious\n\n\nRoad Casualties 2010 Slight\n\n\nRoad Casualties 2010 2010 Total\n\n\nRoad Casualties 2011 Fatal\n\n\nRoad Casualties 2011 Serious\n\n\nRoad Casualties 2011 Slight\n\n\nRoad Casualties 2011 2011 Total\n\n\nRoad Casualties 2012 Fatal\n\n\nRoad Casualties 2012 Serious\n\n\nRoad Casualties 2012 Slight\n\n\nRoad Casualties 2012 2012 Total\n\n\n\n\n\n\n7375\n\n\n7187\n\n\n188\n\n\n25.5\n\n\n4385\n\n\n1.6\n\n\n1\n\n\n7.625019\n\n\n-6.6250185\n\n\n-0.8688528\n\n\n1\n\n\n-1.976662\n\n\n2.976662\n\n\n-1.5059034\n\n\n53\n\n\n58\n\n\n-1.5362329\n\n\n-6.0955833\n\n\n51\n\n\n48.02334\n\n\n0.0601204\n\n\n51\n\n\n57.62502\n\n\n-0.1219796\n\n\n7375\n\n\n620\n\n\n1665\n\n\n2045\n\n\n2010\n\n\n1035\n\n\n5720\n\n\n7280\n\n\n7115\n\n\n7118\n\n\n7131\n\n\n7254\n\n\n7607\n\n\n7429\n\n\n7472\n\n\n7338\n\n\n7412\n\n\n7604\n\n\n8.771699\n\n\n76.68332\n\n\n14.54498\n\n\n297\n\n\n205\n\n\n165\n\n\n231\n\n\n495\n\n\n949\n\n\n826\n\n\n622\n\n\n663\n\n\n598\n\n\n504\n\n\n470\n\n\n473\n\n\n363\n\n\n263\n\n\n192\n\n\n155\n\n\n86\n\n\n47\n\n\n4385\n\n\n306\n\n\n927\n\n\n153\n\n\n2472\n\n\n527\n\n\n6.978335\n\n\n21.14025\n\n\n3.489168\n\n\n56.37400\n\n\n12.01824\n\n\n5799\n\n\n289\n\n\n940\n\n\n193\n\n\n154\n\n\n1576\n\n\n78.63051\n\n\n3.918644\n\n\n12.745763\n\n\n2.616949\n\n\n2.0881356\n\n\n21.36949\n\n\n4670\n\n\n2705\n\n\n63.32203\n\n\n36.67797\n\n\n3825\n\n\n560\n\n\n87.22919\n\n\n12.770810\n\n\n3344\n\n\n92\n\n\n145\n\n\n166\n\n\n409\n\n\n18\n\n\n28\n\n\n2522\n\n\n651\n\n\n45.3\n\n\n1.2\n\n\n2.0\n\n\n2.3\n\n\n5.5\n\n\n0.2\n\n\n0.4\n\n\n34.2\n\n\n8.8\n\n\n1093\n\n\n762\n\n\n725\n\n\n1573\n\n\n24.9\n\n\n17.4\n\n\n16.5\n\n\n35.9\n\n\n4385\n\n\n1145\n\n\n22\n\n\n12\n\n\n80\n\n\n5416\n\n\n79.3\n\n\n20.7\n\n\n0.4\n\n\n0.2\n\n\n1.4\n\n\n98.0\n\n\n289.78\n\n\n26.24060\n\n\n310000\n\n\n341000\n\n\n412500\n\n\n365000.0\n\n\n410000\n\n\n450000\n\n\n465000\n\n\n485000\n\n\n595000\n\n\n303\n\n\n295\n\n\n268\n\n\n141\n\n\n157\n\n\n235\n\n\n256\n\n\n195\n\n\n353\n\n\n454\n\n\n291\n\n\n445\n\n\n47\n\n\n484\n\n\n4618\n\n\n416\n\n\n422\n\n\n4972\n\n\n187\n\n\n1335\n\n\n78.83304\n\n\n3.761062\n\n\n21.16696\n\n\n38\n\n\n0.9\n\n\n59728.48\n\n\n46788.30\n\n\n5.2\n\n\n9.9\n\n\n91\n\n\n22\n\n\n24.2\n\n\n95.7\n\n\n328\n\n\n520\n\n\n6527\n\n\n4.447458\n\n\n7.050847\n\n\n88.50169\n\n\n4112\n\n\n2374\n\n\n643\n\n\n190\n\n\n56\n\n\n55.75593\n\n\n32.18983\n\n\n8.718644\n\n\n2.576271\n\n\n0.759322\n\n\n4.2\n\n\n2.5\n\n\n7.1\n\n\nNA\n\n\n13.7\n\n\n76.8\n\n\n90.9\n\n\n83.9\n\n\n57.1\n\n\n80.2\n\n\n83.6\n\n\n88.4\n\n\n3043\n\n\n1100\n\n\n173\n\n\n51\n\n\n18\n\n\n1692\n\n\n69.4\n\n\n25.1\n\n\n3.9\n\n\n1.2\n\n\n0.4\n\n\n0.3858609\n\n\n1\n\n\n39\n\n\n334\n\n\n374\n\n\n0\n\n\n46\n\n\n359\n\n\n405\n\n\n2\n\n\n51\n\n\n361\n\n\n414\n\n\n\n\n6775\n\n\n6724\n\n\n51\n\n\n31.3\n\n\n2713\n\n\n2.5\n\n\n8\n\n\n-9.233267\n\n\n17.2332671\n\n\n-1.8664322\n\n\n0\n\n\n1.129576\n\n\n-1.129576\n\n\n-1.0000000\n\n\n53\n\n\n136\n\n\n-27.9473386\n\n\n-2.0000000\n\n\n50\n\n\n51.12958\n\n\n-0.0223392\n\n\n58\n\n\n40.76673\n\n\n0.3489691\n\n\n6775\n\n\n1751\n\n\n1277\n\n\n1388\n\n\n1258\n\n\n1101\n\n\n3923\n\n\n6333\n\n\n6312\n\n\n6329\n\n\n6341\n\n\n6330\n\n\n6323\n\n\n6369\n\n\n6570\n\n\n6636\n\n\n6783\n\n\n6853\n\n\n25.113089\n\n\n58.73340\n\n\n16.15351\n\n\n652\n\n\n607\n\n\n462\n\n\n458\n\n\n399\n\n\n468\n\n\n466\n\n\n466\n\n\n461\n\n\n450\n\n\n347\n\n\n254\n\n\n256\n\n\n254\n\n\n206\n\n\n215\n\n\n201\n\n\n137\n\n\n94\n\n\n2713\n\n\n491\n\n\n366\n\n\n597\n\n\n814\n\n\n445\n\n\n18.098046\n\n\n13.49060\n\n\n22.005160\n\n\n30.00369\n\n\n16.40251\n\n\n4403\n\n\n330\n\n\n820\n\n\n1133\n\n\n89\n\n\n2372\n\n\n64.98893\n\n\n4.870849\n\n\n12.103321\n\n\n16.723247\n\n\n1.3136531\n\n\n35.01107\n\n\n5159\n\n\n1616\n\n\n76.14760\n\n\n23.85240\n\n\n2459\n\n\n254\n\n\n90.63767\n\n\n9.362329\n\n\n3975\n\n\n19\n\n\n174\n\n\n27\n\n\n591\n\n\n122\n\n\n16\n\n\n1417\n\n\n434\n\n\n58.7\n\n\n0.3\n\n\n2.6\n\n\n0.4\n\n\n8.7\n\n\n1.8\n\n\n0.2\n\n\n20.9\n\n\n6.4\n\n\n596\n\n\n663\n\n\n1133\n\n\n269\n\n\n22.0\n\n\n24.4\n\n\n41.8\n\n\n9.9\n\n\n2713\n\n\n82\n\n\n99\n\n\n744\n\n\n865\n\n\n1087\n\n\n97.1\n\n\n2.9\n\n\n3.5\n\n\n26.6\n\n\n30.9\n\n\n38.9\n\n\n216.15\n\n\n31.70483\n\n\n168500\n\n\n180000\n\n\n187500\n\n\n197500.0\n\n\n190000\n\n\n173000\n\n\n185000\n\n\n182250\n\n\n190000\n\n\n81\n\n\n100\n\n\n100\n\n\n68\n\n\n45\n\n\n61\n\n\n51\n\n\n42\n\n\n61\n\n\n1623\n\n\n789\n\n\n706\n\n\n118\n\n\n479\n\n\n914\n\n\n395\n\n\n272\n\n\n2847\n\n\n335\n\n\n1513\n\n\n65.29817\n\n\n11.766772\n\n\n34.70183\n\n\n319\n\n\n11.8\n\n\n31788.19\n\n\n27058.70\n\n\n31.0\n\n\n27.5\n\n\n445\n\n\n249\n\n\n56.0\n\n\n97.5\n\n\n707\n\n\n678\n\n\n5390\n\n\n10.435424\n\n\n10.007380\n\n\n79.55720\n\n\n2933\n\n\n2288\n\n\n1059\n\n\n389\n\n\n106\n\n\n43.29151\n\n\n33.77122\n\n\n15.630996\n\n\n5.741697\n\n\n1.564576\n\n\n10.6\n\n\n8.4\n\n\n13.3\n\n\n23.6\n\n\n29.8\n\n\n100.7\n\n\n93.3\n\n\n82.5\n\n\n97.2\n\n\n107.6\n\n\n78.0\n\n\n80.1\n\n\n1020\n\n\n1186\n\n\n424\n\n\n66\n\n\n17\n\n\n2305\n\n\n37.6\n\n\n43.7\n\n\n15.6\n\n\n2.4\n\n\n0.6\n\n\n0.8496130\n\n\n0\n\n\n0\n\n\n18\n\n\n18\n\n\n0\n\n\n2\n\n\n16\n\n\n18\n\n\n0\n\n\n1\n\n\n15\n\n\n16\n\n\n\n\n10045\n\n\n10033\n\n\n12\n\n\n46.9\n\n\n3834\n\n\n2.6\n\n\n11\n\n\n12.348001\n\n\n-1.3480006\n\n\n-0.1091675\n\n\n10\n\n\n6.922428\n\n\n3.077572\n\n\n0.4445798\n\n\n124\n\n\n239\n\n\n-0.1154703\n\n\n0.3637269\n\n\n60\n\n\n56.92243\n\n\n0.0526430\n\n\n61\n\n\n62.34800\n\n\n-0.0218569\n\n\n10045\n\n\n2247\n\n\n1959\n\n\n2300\n\n\n2259\n\n\n1280\n\n\n6518\n\n\n9236\n\n\n9252\n\n\n9155\n\n\n9072\n\n\n9144\n\n\n9227\n\n\n9564\n\n\n9914\n\n\n10042\n\n\n10088\n\n\n10218\n\n\n21.442552\n\n\n65.81523\n\n\n12.74222\n\n\n849\n\n\n739\n\n\n603\n\n\n615\n\n\n686\n\n\n804\n\n\n818\n\n\n743\n\n\n770\n\n\n713\n\n\n624\n\n\n526\n\n\n426\n\n\n372\n\n\n277\n\n\n258\n\n\n198\n\n\n124\n\n\n73\n\n\n3834\n\n\n776\n\n\n730\n\n\n589\n\n\n1039\n\n\n700\n\n\n20.239958\n\n\n19.04017\n\n\n15.362546\n\n\n27.09963\n\n\n18.25769\n\n\n5486\n\n\n433\n\n\n2284\n\n\n1618\n\n\n224\n\n\n4559\n\n\n54.61424\n\n\n4.310602\n\n\n22.737680\n\n\n16.107516\n\n\n2.2299652\n\n\n45.38576\n\n\n7193\n\n\n2852\n\n\n71.60777\n\n\n28.39223\n\n\n3431\n\n\n403\n\n\n89.48878\n\n\n10.511215\n\n\n5475\n\n\n46\n\n\n476\n\n\n42\n\n\n1351\n\n\n430\n\n\n34\n\n\n1514\n\n\n677\n\n\n54.5\n\n\n0.5\n\n\n4.7\n\n\n0.4\n\n\n13.4\n\n\n4.3\n\n\n0.3\n\n\n15.1\n\n\n6.7\n\n\n1028\n\n\n1473\n\n\n446\n\n\n830\n\n\n26.8\n\n\n38.4\n\n\n11.6\n\n\n21.6\n\n\n3834\n\n\n110\n\n\n161\n\n\n936\n\n\n1591\n\n\n1256\n\n\n97.2\n\n\n2.8\n\n\n4.1\n\n\n23.7\n\n\n40.3\n\n\n31.8\n\n\n214.15\n\n\n47.71422\n\n\n185000\n\n\n197750\n\n\n220000\n\n\n225000.0\n\n\n188500\n\n\n215000\n\n\n200000\n\n\n205500\n\n\n237000\n\n\n203\n\n\n232\n\n\n295\n\n\n100\n\n\n80\n\n\n97\n\n\n89\n\n\n114\n\n\n98\n\n\n1778\n\n\n1210\n\n\n1236\n\n\n169\n\n\n847\n\n\n1829\n\n\n729\n\n\n442\n\n\n5038\n\n\n459\n\n\n2111\n\n\n70.47139\n\n\n9.110758\n\n\n29.52861\n\n\n268\n\n\n7.0\n\n\n43356.93\n\n\n36834.53\n\n\n18.9\n\n\n21.2\n\n\n408\n\n\n199\n\n\n48.8\n\n\n96.5\n\n\n792\n\n\n810\n\n\n8443\n\n\n7.884520\n\n\n8.063713\n\n\n84.05177\n\n\n4566\n\n\n3633\n\n\n1325\n\n\n406\n\n\n115\n\n\n45.45545\n\n\n36.16725\n\n\n13.190642\n\n\n4.041812\n\n\n1.144848\n\n\n7.8\n\n\n6.1\n\n\n9.9\n\n\n25.5\n\n\n28.3\n\n\n91.4\n\n\n107.3\n\n\n124.6\n\n\n105.5\n\n\n82.9\n\n\n80.2\n\n\n85.6\n\n\n1196\n\n\n1753\n\n\n691\n\n\n155\n\n\n39\n\n\n3766\n\n\n31.2\n\n\n45.7\n\n\n18.0\n\n\n4.0\n\n\n1.0\n\n\n0.9822640\n\n\n0\n\n\n3\n\n\n34\n\n\n37\n\n\n1\n\n\n4\n\n\n40\n\n\n45\n\n\n0\n\n\n3\n\n\n47\n\n\n50\n\n\n\n\n6182\n\n\n5937\n\n\n245\n\n\n24.8\n\n\n2318\n\n\n2.6\n\n\n2\n\n\n-4.719603\n\n\n6.7196026\n\n\n-1.4237645\n\n\n0\n\n\n-1.271292\n\n\n1.271292\n\n\n-1.0000000\n\n\n30\n\n\n81\n\n\n-4.9416062\n\n\n-2.0000000\n\n\n50\n\n\n48.72871\n\n\n0.0257532\n\n\n52\n\n\n45.28040\n\n\n0.1381492\n\n\n6182\n\n\n1196\n\n\n1277\n\n\n1154\n\n\n1543\n\n\n1012\n\n\n3974\n\n\n6208\n\n\n6159\n\n\n6163\n\n\n6152\n\n\n5997\n\n\n6005\n\n\n6084\n\n\n6268\n\n\n6237\n\n\n6185\n\n\n6308\n\n\n18.246671\n\n\n65.82118\n\n\n15.93215\n\n\n409\n\n\n364\n\n\n378\n\n\n516\n\n\n472\n\n\n435\n\n\n363\n\n\n399\n\n\n388\n\n\n463\n\n\n459\n\n\n354\n\n\n303\n\n\n226\n\n\n238\n\n\n194\n\n\n172\n\n\n106\n\n\n69\n\n\n2318\n\n\n508\n\n\n524\n\n\n322\n\n\n609\n\n\n355\n\n\n21.915444\n\n\n22.60569\n\n\n13.891286\n\n\n26.27265\n\n\n15.31493\n\n\n5006\n\n\n186\n\n\n313\n\n\n649\n\n\n28\n\n\n1176\n\n\n80.97703\n\n\n3.008735\n\n\n5.063086\n\n\n10.498221\n\n\n0.4529279\n\n\n19.02297\n\n\n5292\n\n\n890\n\n\n85.60336\n\n\n14.39664\n\n\n2214\n\n\n104\n\n\n95.51337\n\n\n4.486626\n\n\n4070\n\n\n21\n\n\n34\n\n\n23\n\n\n234\n\n\n10\n\n\n18\n\n\n1373\n\n\n399\n\n\n65.8\n\n\n0.3\n\n\n0.5\n\n\n0.4\n\n\n3.8\n\n\n0.2\n\n\n0.3\n\n\n22.2\n\n\n6.5\n\n\n718\n\n\n969\n\n\n371\n\n\n228\n\n\n31.0\n\n\n41.8\n\n\n16.0\n\n\n9.8\n\n\n2318\n\n\n45\n\n\n92\n\n\n858\n\n\n1094\n\n\n314\n\n\n98.1\n\n\n1.9\n\n\n3.9\n\n\n36.3\n\n\n46.3\n\n\n13.2\n\n\n249.28\n\n\n25.30488\n\n\n181000\n\n\n182000\n\n\n212000\n\n\n208997.5\n\n\n182000\n\n\n192000\n\n\n193750\n\n\n205000\n\n\n210000\n\n\n93\n\n\n131\n\n\n125\n\n\n66\n\n\n54\n\n\n61\n\n\n72\n\n\n58\n\n\n67\n\n\n1502\n\n\n800\n\n\n825\n\n\n163\n\n\n539\n\n\n891\n\n\n266\n\n\n215\n\n\n3187\n\n\n296\n\n\n1251\n\n\n71.81163\n\n\n9.287731\n\n\n28.18837\n\n\n122\n\n\n5.3\n\n\n46701.44\n\n\n39668.21\n\n\n15.8\n\n\n21.3\n\n\n206\n\n\n87\n\n\n42.2\n\n\n98.5\n\n\n586\n\n\n547\n\n\n5049\n\n\n9.479133\n\n\n8.848269\n\n\n81.67260\n\n\n2857\n\n\n2086\n\n\n861\n\n\n302\n\n\n76\n\n\n46.21482\n\n\n33.74313\n\n\n13.927532\n\n\n4.885150\n\n\n1.229376\n\n\n5.8\n\n\n3.9\n\n\n8.6\n\n\nNA\n\n\n26.9\n\n\n96.1\n\n\n95.5\n\n\n105.8\n\n\n102.4\n\n\n110.3\n\n\n77.9\n\n\n80.7\n\n\n556\n\n\n1085\n\n\n515\n\n\n128\n\n\n34\n\n\n2650\n\n\n24.0\n\n\n46.8\n\n\n22.2\n\n\n5.5\n\n\n1.5\n\n\n1.1432269\n\n\n0\n\n\n1\n\n\n13\n\n\n14\n\n\n0\n\n\n2\n\n\n7\n\n\n9\n\n\n0\n\n\n2\n\n\n5\n\n\n7\n\n\n\n\n8562\n\n\n8562\n\n\n0\n\n\n72.1\n\n\n3183\n\n\n2.7\n\n\n4\n\n\n4.584902\n\n\n-0.5849018\n\n\n-0.1275713\n\n\n1\n\n\n9.214027\n\n\n-8.214027\n\n\n-0.8914698\n\n\n87\n\n\n161\n\n\n-0.1362629\n\n\n-1.6083817\n\n\n51\n\n\n59.21403\n\n\n-0.1490559\n\n\n54\n\n\n54.58490\n\n\n-0.0107732\n\n\n8562\n\n\n2200\n\n\n1592\n\n\n1995\n\n\n1829\n\n\n946\n\n\n5416\n\n\n7919\n\n\n7922\n\n\n7882\n\n\n7887\n\n\n7917\n\n\n7916\n\n\n8025\n\n\n8317\n\n\n8519\n\n\n8588\n\n\n8660\n\n\n24.237875\n\n\n64.57275\n\n\n11.18938\n\n\n783\n\n\n692\n\n\n624\n\n\n657\n\n\n525\n\n\n608\n\n\n616\n\n\n643\n\n\n673\n\n\n656\n\n\n508\n\n\n386\n\n\n320\n\n\n321\n\n\n202\n\n\n194\n\n\n127\n\n\n90\n\n\n35\n\n\n3183\n\n\n691\n\n\n583\n\n\n593\n\n\n808\n\n\n508\n\n\n21.709080\n\n\n18.31605\n\n\n18.630223\n\n\n25.38486\n\n\n15.95979\n\n\n5674\n\n\n313\n\n\n1050\n\n\n1445\n\n\n80\n\n\n2888\n\n\n66.26956\n\n\n3.655688\n\n\n12.263490\n\n\n16.876898\n\n\n0.9343611\n\n\n33.73044\n\n\n6425\n\n\n2137\n\n\n75.04088\n\n\n24.95912\n\n\n2868\n\n\n315\n\n\n90.10368\n\n\n9.896324\n\n\n4986\n\n\n28\n\n\n138\n\n\n35\n\n\n762\n\n\n166\n\n\n13\n\n\n1816\n\n\n618\n\n\n58.2\n\n\n0.3\n\n\n1.6\n\n\n0.4\n\n\n8.9\n\n\n1.9\n\n\n0.2\n\n\n21.2\n\n\n7.2\n\n\n711\n\n\n1146\n\n\n793\n\n\n482\n\n\n22.3\n\n\n36.0\n\n\n24.9\n\n\n15.1\n\n\n3183\n\n\n89\n\n\n136\n\n\n622\n\n\n2141\n\n\n373\n\n\n97.3\n\n\n2.7\n\n\n4.2\n\n\n19.0\n\n\n65.4\n\n\n11.4\n\n\n118.81\n\n\n72.88949\n\n\n162250\n\n\n170000\n\n\n185000\n\n\n200000.0\n\n\n166250\n\n\n169000\n\n\n165000\n\n\n167000\n\n\n180000\n\n\n152\n\n\n168\n\n\n184\n\n\n73\n\n\n60\n\n\n58\n\n\n81\n\n\n59\n\n\n75\n\n\n1839\n\n\n1026\n\n\n1038\n\n\n160\n\n\n653\n\n\n1119\n\n\n527\n\n\n333\n\n\n4052\n\n\n394\n\n\n1881\n\n\n68.29597\n\n\n9.723593\n\n\n31.70403\n\n\n307\n\n\n9.6\n\n\n34293.82\n\n\n29155.68\n\n\n22.9\n\n\n25.3\n\n\n442\n\n\n231\n\n\n52.3\n\n\n96.3\n\n\n713\n\n\n722\n\n\n7127\n\n\n8.327494\n\n\n8.432609\n\n\n83.23990\n\n\n4089\n\n\n2811\n\n\n1134\n\n\n412\n\n\n116\n\n\n47.75753\n\n\n32.83111\n\n\n13.244569\n\n\n4.811960\n\n\n1.354824\n\n\n7.7\n\n\n6.0\n\n\n9.9\n\n\n24.4\n\n\n29.7\n\n\n110.0\n\n\n106.1\n\n\n113.7\n\n\n168.6\n\n\n63.1\n\n\n76.8\n\n\n79.9\n\n\n1080\n\n\n1423\n\n\n551\n\n\n109\n\n\n20\n\n\n2937\n\n\n33.9\n\n\n44.7\n\n\n17.3\n\n\n3.4\n\n\n0.6\n\n\n0.9227144\n\n\n1\n\n\n5\n\n\n24\n\n\n30\n\n\n0\n\n\n2\n\n\n27\n\n\n29\n\n\n0\n\n\n1\n\n\n24\n\n\n25\n\n\n\n\n8791\n\n\n8672\n\n\n119\n\n\n50.6\n\n\n3441\n\n\n2.5\n\n\n5\n\n\n13.290915\n\n\n-8.2909151\n\n\n-0.6238032\n\n\n5\n\n\n1.267089\n\n\n3.732911\n\n\n2.9460534\n\n\n96\n\n\n136\n\n\n-0.9065610\n\n\n1.1912744\n\n\n55\n\n\n51.26709\n\n\n0.0702553\n\n\n55\n\n\n63.29092\n\n\n-0.1401784\n\n\n8791\n\n\n2388\n\n\n1765\n\n\n1867\n\n\n1736\n\n\n1035\n\n\n5368\n\n\n7806\n\n\n7726\n\n\n7771\n\n\n7820\n\n\n7877\n\n\n7974\n\n\n8145\n\n\n8394\n\n\n8571\n\n\n8823\n\n\n9076\n\n\n26.586602\n\n\n62.02071\n\n\n11.39268\n\n\n957\n\n\n792\n\n\n664\n\n\n633\n\n\n642\n\n\n686\n\n\n634\n\n\n651\n\n\n596\n\n\n610\n\n\n483\n\n\n398\n\n\n296\n\n\n242\n\n\n207\n\n\n180\n\n\n181\n\n\n139\n\n\n85\n\n\n3441\n\n\n643\n\n\n489\n\n\n776\n\n\n1064\n\n\n469\n\n\n18.686428\n\n\n14.21099\n\n\n22.551584\n\n\n30.92124\n\n\n13.62976\n\n\n5906\n\n\n307\n\n\n526\n\n\n1997\n\n\n55\n\n\n2885\n\n\n67.18235\n\n\n3.492208\n\n\n5.983392\n\n\n22.716414\n\n\n0.6256399\n\n\n32.81765\n\n\n6658\n\n\n2133\n\n\n75.73655\n\n\n24.26345\n\n\n3117\n\n\n324\n\n\n90.58413\n\n\n9.415867\n\n\n5409\n\n\n22\n\n\n67\n\n\n13\n\n\n577\n\n\n19\n\n\n24\n\n\n2148\n\n\n512\n\n\n61.5\n\n\n0.3\n\n\n0.8\n\n\n0.1\n\n\n6.6\n\n\n0.2\n\n\n0.3\n\n\n24.4\n\n\n5.8\n\n\n558\n\n\n821\n\n\n1663\n\n\n333\n\n\n16.2\n\n\n23.9\n\n\n48.3\n\n\n9.7\n\n\n3441\n\n\n93\n\n\n82\n\n\n761\n\n\n1219\n\n\n1471\n\n\n97.4\n\n\n2.6\n\n\n2.3\n\n\n21.5\n\n\n34.5\n\n\n41.6\n\n\n173.58\n\n\n52.28713\n\n\n165000\n\n\n165000\n\n\n187000\n\n\n199000.0\n\n\n155000\n\n\n165000\n\n\n153750\n\n\n172500\n\n\n168500\n\n\n112\n\n\n133\n\n\n139\n\n\n39\n\n\n40\n\n\n75\n\n\n76\n\n\n48\n\n\n55\n\n\n2057\n\n\n1065\n\n\n1001\n\n\n166\n\n\n556\n\n\n1129\n\n\n429\n\n\n402\n\n\n3905\n\n\n511\n\n\n1893\n\n\n67.35081\n\n\n13.085788\n\n\n32.64919\n\n\n437\n\n\n12.7\n\n\n29975.83\n\n\n25568.60\n\n\n33.3\n\n\n28.9\n\n\n602\n\n\n345\n\n\n57.3\n\n\n98.1\n\n\n828\n\n\n752\n\n\n7211\n\n\n9.418724\n\n\n8.554203\n\n\n82.02707\n\n\n3996\n\n\n3015\n\n\n1252\n\n\n407\n\n\n121\n\n\n45.45558\n\n\n34.29644\n\n\n14.241838\n\n\n4.629735\n\n\n1.376408\n\n\n6.8\n\n\n5.3\n\n\n8.8\n\n\n26.0\n\n\n29.0\n\n\n117.0\n\n\n80.1\n\n\n123.6\n\n\n151.7\n\n\n112.9\n\n\n75.7\n\n\n79.5\n\n\n1496\n\n\n1444\n\n\n419\n\n\n63\n\n\n19\n\n\n2549\n\n\n43.5\n\n\n42.0\n\n\n12.2\n\n\n1.8\n\n\n0.6\n\n\n0.7407730\n\n\n0\n\n\n2\n\n\n18\n\n\n20\n\n\n1\n\n\n1\n\n\n18\n\n\n20\n\n\n0\n\n\n2\n\n\n25\n\n\n27\n\n\n\n\n\n\n\nHaving now ingested and linked our data, we begin by exploring the factors most highly correlated with our relative error rates.\n\ncorr_df &lt;- correlate(dplyr::select_if(msoa_matrix_tbl, is.numeric), quiet = TRUE)\n\noptions(scipen=999)\n\nStarting by our relative robbery error, a few interesting correlates stand out: - road traffic casualties - burglary numbers - the number of dwellings with no usual residents, and the number of commercial residents - households with no cars - the age composition of the area - general deprivation indicators (such as the proportion of households with central heating)\n\n#show only correlates with an absolute value higher than 0.2\n#filter(dplyr::select(corr_df[order(corr_df$RPDRobberyShifted),] , term, RPDRobberyShifted), RPDRobberyShifted &lt; -0.2 | RPDRobberyShifted &gt; 0.2)\n#filter(dplyr::select(corr_df[order(corr_df$RPDRobberyShifted),] , term, RPDRobberyShifted), RPDRobberyShifted &lt; -0.2 | RPDRobberyShifted &gt; 0.2)\nhigh_corr_rob &lt;- filter(dplyr::select(corr_df, term, RPDRobberyShifted), RPDRobberyShifted &lt; -0.15 | RPDRobberyShifted &gt; 0.15)\n\nhigh_corr_rob[order(high_corr_rob$RPDRobberyShifted),]\n\n\n\n\n\n\nterm\n\n\nRPDRobberyShifted\n\n\n\n\n\n\nrobberyPredicted\n\n\n-0.8387666\n\n\n\n\nrobberyPredictedShifted\n\n\n-0.8387666\n\n\n\n\ntotal_robberies\n\n\n-0.5369450\n\n\n\n\ntotal_burglaries\n\n\n-0.4717060\n\n\n\n\nRoad Casualties 2011 2011 Total\n\n\n-0.3701356\n\n\n\n\nRoad Casualties 2011 Slight\n\n\n-0.3635665\n\n\n\n\nRoad Casualties 2010 2010 Total\n\n\n-0.3597137\n\n\n\n\nRoad Casualties 2010 Slight\n\n\n-0.3567229\n\n\n\n\nRoad Casualties 2012 2012 Total\n\n\n-0.3563600\n\n\n\n\nRoad Casualties 2012 Slight\n\n\n-0.3521528\n\n\n\n\nRoad Casualties 2011 Serious\n\n\n-0.3474359\n\n\n\n\nRoad Casualties 2012 Serious\n\n\n-0.3200851\n\n\n\n\nRoad Casualties 2010 Serious\n\n\n-0.3057949\n\n\n\n\nburglaryPredicted\n\n\n-0.2952051\n\n\n\n\nburglaryPredictedShifted\n\n\n-0.2952051\n\n\n\n\nburglaryActual\n\n\n-0.2646662\n\n\n\n\nburglaryActualShifted\n\n\n-0.2646662\n\n\n\n\nDwelling type (2011) Household spaces with no usual residents\n\n\n-0.2587599\n\n\n\n\nDwelling type (2011) Household spaces with no usual residents (%)\n\n\n-0.2308005\n\n\n\n\nCar or van availability (2011 Census) No cars or vans in household\n\n\n-0.2279835\n\n\n\n\nQualifications (2011 Census) Schoolchildren and full-time students: Age 18 and over\n\n\n-0.2219251\n\n\n\n\nMid-year Estimates 2012, by age 20-24\n\n\n-0.2182673\n\n\n\n\nHousehold Composition (2011) Numbers One person household\n\n\n-0.2145786\n\n\n\n\nTenure (2011) Private rented\n\n\n-0.2029037\n\n\n\n\nDwelling type (2011) Flat, maisonette or apartment\n\n\n-0.2015178\n\n\n\n\nHousehold Composition (2011) Percentages One person household\n\n\n-0.1900561\n\n\n\n\nAge Structure (2011 Census) 16-29\n\n\n-0.1896718\n\n\n\n\nCar or van availability (2011 Census) No cars or vans in household (%)\n\n\n-0.1895818\n\n\n\n\nComEstRes\n\n\n-0.1858042\n\n\n\n\nMid-year Estimates 2012, by age % 15-64\n\n\n-0.1821470\n\n\n\n\nReligion (2011) Buddhist\n\n\n-0.1772919\n\n\n\n\nReligion (2011) Buddhist (%)\n\n\n-0.1759725\n\n\n\n\nHousehold Language (2011) No people in household have English as a main language\n\n\n-0.1746012\n\n\n\n\nQualifications (2011 Census) Highest level of qualification: Level 3 qualifications\n\n\n-0.1642773\n\n\n\n\nHholds\n\n\n-0.1615083\n\n\n\n\nHouseholds (2011) All Households\n\n\n-0.1615083\n\n\n\n\nDwelling type (2011) Household spaces with at least one usual resident\n\n\n-0.1615083\n\n\n\n\nEconomic Activity (2011 Census) Economically inactive: Total\n\n\n-0.1570016\n\n\n\n\nAge Structure (2011 Census) Working-age\n\n\n-0.1531622\n\n\n\n\nEthnic Group (2011 Census) Other ethnic group\n\n\n-0.1523189\n\n\n\n\nCar or van availability (2011 Census) Cars per household\n\n\n0.1604540\n\n\n\n\nTenure (2011) Owned: Owned with a mortgage or loan (%)\n\n\n0.1701928\n\n\n\n\nCentral Heating (2011 Census) Households with central heating (%)\n\n\n0.2055461\n\n\n\n\nburglaryError\n\n\n0.2115687\n\n\n\n\nHousehold Composition (2011) Percentages Couple household with dependent children\n\n\n0.2130557\n\n\n\n\nCar or van availability (2011 Census) 1 car or van in household (%)\n\n\n0.2305679\n\n\n\n\nDwelling type (2011) Household spaces with at least one usual resident (%)\n\n\n0.2308005\n\n\n\n\nrobberyError\n\n\n0.8565425\n\n\n\n\n\n\n\nThe correlations for burglary are weaker - only a few have an absolute value higher than 0.2 - but a few stand out: - households with no residents - high robbery numbers - house prices\n\nhigh_corr_burg &lt;- filter(dplyr::select(corr_df, term, RPDburglaryShifted), RPDburglaryShifted &lt; -0.15 | RPDburglaryShifted &gt; 0.15)\n\nhigh_corr_burg[order(high_corr_burg$RPDburglaryShifted),]\n\n\n\n\n\n\nterm\n\n\nRPDburglaryShifted\n\n\n\n\n\n\nburglaryPredicted\n\n\n-0.8600903\n\n\n\n\nburglaryPredictedShifted\n\n\n-0.8600903\n\n\n\n\ntotal_burglaries\n\n\n-0.2169094\n\n\n\n\nDwelling type (2011) Household spaces with no usual residents\n\n\n-0.1761768\n\n\n\n\ntotal_robberies\n\n\n-0.1714721\n\n\n\n\nDwelling type (2011) Household spaces with no usual residents (%)\n\n\n-0.1706826\n\n\n\n\nrobberyPredicted\n\n\n-0.1631558\n\n\n\n\nrobberyPredictedShifted\n\n\n-0.1631558\n\n\n\n\nHouse Prices Median House Price (¬£) 2010\n\n\n-0.1609004\n\n\n\n\nHouse Prices Median House Price (¬£) 2008\n\n\n-0.1527950\n\n\n\n\nHouse Prices Median House Price (¬£) 2011\n\n\n-0.1507911\n\n\n\n\nHouse Prices Median House Price (¬£) 2007\n\n\n-0.1505301\n\n\n\n\nMid-year Estimates 2012, by age % 0 to 14\n\n\n0.1628106\n\n\n\n\nrobberyError\n\n\n0.1628523\n\n\n\n\nDwelling type (2011) Household spaces with at least one usual resident (%)\n\n\n0.1706826\n\n\n\n\nburglaryError\n\n\n0.9656142\n\n\n\n\n\n\n\nThese correlates suggest we can model this shift - this is likely to prove more reliable for robbery (where the correlations are stronger), and seem linked to usual resident population(as measured by household composition), deprivation (through various proxy indicators such as central heating presence or housing type), and general crime patterns (through total burglary and robbery numbers)\nWe will take two approaches for modelling: a simple regression (to identify strong links) and random forest regressors (to identify non-linear associations)\n\nSimple Regression\nWe begin through the use of simple OLS regression. This is a linear model that has large limitations for modelling complex relationships, but can be an effective first step, effectively with a few transformations.\nR does not cope well with blank spaces in terms, so we‚Äôll extract and rename our key correlates.\n\n#make copy of df and rename \n\nmsoa_copy &lt;- msoa_matrix_numeric\n\nnames(msoa_copy)[names(msoa_copy) == \"Dwelling type (2011) Household spaces with no usual residents\"] &lt;- \"DwellingNoResidents\"\nnames(msoa_copy)[names(msoa_copy) == \"House Prices Median House Price (¬£) 2010\"] &lt;- \"MedianHousePrice\"\nnames(msoa_copy)[names(msoa_copy) == \"Dwelling type (2011) Flat, maisonette or apartment\"] &lt;- \"FlatAprt\"\nnames(msoa_copy)[names(msoa_copy) == \"Qualifications (2011 Census) Schoolchildren and full-time students: Age 18 and over\"] &lt;- \"fullTimeStudents\"\nnames(msoa_copy)[names(msoa_copy) == \"Car or van availability (2011 Census) No cars or vans in household\"] &lt;- \"NoCars\"\nnames(msoa_copy)[names(msoa_copy) == \"Ethnic Group (2011 Census) Other ethnic group\"] &lt;- \"OtherEthnicGroup\"\nnames(msoa_copy)[names(msoa_copy) == \"Central Heating (2011 Census) Households with central heating (%)\"] &lt;- \"CentralHealingPercent\"\n\nLet‚Äôs now extract these to a separate dataframe, remove any missing values, and provide some quick summary statistics to identify any obvious concerns.\nWe also generate a matrix of scatter-plots, so as to identify any obvious relationships between our key values.\n\nfeature_df &lt;- dplyr::select(msoa_copy, RPDburglaryShifted, RPDRobberyShifted, total_burglaries, total_robberies, DwellingNoResidents, MedianHousePrice, FlatAprt, fullTimeStudents, NoCars, OtherEthnicGroup, CentralHealingPercent, AvHholdSz, ComEstRes)\n\nfeature_df &lt;- drop_na(feature_df, RPDburglaryShifted, RPDRobberyShifted)\nhead(feature_df)\n\n\n\n\n\n\nRPDburglaryShifted\n\n\nRPDRobberyShifted\n\n\ntotal_burglaries\n\n\ntotal_robberies\n\n\nDwellingNoResidents\n\n\nMedianHousePrice\n\n\nFlatAprt\n\n\nfullTimeStudents\n\n\nNoCars\n\n\nOtherEthnicGroup\n\n\nCentralHealingPercent\n\n\nAvHholdSz\n\n\nComEstRes\n\n\n\n\n\n\n-0.1219796\n\n\n0.0601204\n\n\n58\n\n\n53\n\n\n1145\n\n\n450000\n\n\n5416\n\n\n422\n\n\n3043\n\n\n154\n\n\n95.7\n\n\n1.6\n\n\n188\n\n\n\n\n0.3489691\n\n\n-0.0223392\n\n\n136\n\n\n53\n\n\n82\n\n\n173000\n\n\n1087\n\n\n272\n\n\n1020\n\n\n89\n\n\n97.5\n\n\n2.5\n\n\n51\n\n\n\n\n-0.0218569\n\n\n0.0526430\n\n\n239\n\n\n124\n\n\n110\n\n\n215000\n\n\n1256\n\n\n442\n\n\n1196\n\n\n224\n\n\n96.5\n\n\n2.6\n\n\n12\n\n\n\n\n0.1381492\n\n\n0.0257532\n\n\n81\n\n\n30\n\n\n45\n\n\n192000\n\n\n314\n\n\n215\n\n\n556\n\n\n28\n\n\n98.5\n\n\n2.6\n\n\n245\n\n\n\n\n-0.0107732\n\n\n-0.1490559\n\n\n161\n\n\n87\n\n\n89\n\n\n169000\n\n\n373\n\n\n333\n\n\n1080\n\n\n80\n\n\n96.3\n\n\n2.7\n\n\n0\n\n\n\n\n-0.1401784\n\n\n0.0702553\n\n\n136\n\n\n96\n\n\n93\n\n\n165000\n\n\n1471\n\n\n402\n\n\n1496\n\n\n55\n\n\n98.1\n\n\n2.5\n\n\n119\n\n\n\n\n\n\n\n\nsummary(feature_df)\n\n RPDburglaryShifted RPDRobberyShifted  total_burglaries total_robberies  \n Min.   :-0.81266   Min.   :-1.45491   Min.   :  46.0   Min.   :   2.00  \n 1st Qu.:-0.20689   1st Qu.:-0.11808   1st Qu.: 121.0   1st Qu.:  29.00  \n Median :-0.08826   Median :-0.04081   Median : 159.0   Median :  53.00  \n Mean   :-0.07976   Mean   :-0.04876   Mean   : 175.5   Mean   :  78.53  \n 3rd Qu.: 0.03612   3rd Qu.: 0.04309   3rd Qu.: 205.0   3rd Qu.:  92.00  \n Max.   : 1.29467   Max.   : 0.62020   Max.   :1973.0   Max.   :2456.00  \n DwellingNoResidents MedianHousePrice     FlatAprt      fullTimeStudents\n Min.   :  14.0      Min.   : 137625   Min.   :  54.0   Min.   : 122.0  \n 1st Qu.:  59.0      1st Qu.: 222500   1st Qu.: 830.5   1st Qu.: 305.5  \n Median :  86.0      Median : 265975   Median :1607.0   Median : 465.0  \n Mean   : 123.4      Mean   : 310710   Mean   :1798.2   Mean   : 539.5  \n 3rd Qu.: 133.0      3rd Qu.: 351525   3rd Qu.:2566.5   3rd Qu.: 654.5  \n Max.   :1556.0      Max.   :1425000   Max.   :6429.0   Max.   :3370.0  \n     NoCars     OtherEthnicGroup CentralHealingPercent   AvHholdSz    \n Min.   : 185   Min.   :  16.0   Min.   :92.10         Min.   :1.600  \n 1st Qu.: 796   1st Qu.: 134.0   1st Qu.:96.60         1st Qu.:2.300  \n Median :1256   Median : 232.0   Median :97.40         Median :2.500  \n Mean   :1382   Mean   : 285.8   Mean   :97.24         Mean   :2.506  \n 3rd Qu.:1858   3rd Qu.: 376.5   3rd Qu.:98.00         3rd Qu.:2.700  \n Max.   :4319   Max.   :3001.0   Max.   :99.60         Max.   :3.900  \n   ComEstRes   \n Min.   :   0  \n 1st Qu.:   9  \n Median :  41  \n Mean   : 102  \n 3rd Qu.: 105  \n Max.   :2172  \n\n\n\ncolSums(is.na(feature_df))\n\n   RPDburglaryShifted     RPDRobberyShifted      total_burglaries \n                    0                     0                     0 \n      total_robberies   DwellingNoResidents      MedianHousePrice \n                    0                     0                     0 \n             FlatAprt      fullTimeStudents                NoCars \n                    0                     0                     0 \n     OtherEthnicGroup CentralHealingPercent             AvHholdSz \n                    0                     0                     0 \n            ComEstRes \n                    0 \n\npairs(feature_df)\n\n\n\n\n\n\n\n\nAs we hoped, some obvious relationships stand out: for instance, the presence of apartments, and households with no cars, or central heating and average household size.\nOur robbery and burglary data and change rates are densely clustered - they‚Äôre unlikely to cleanly associate with anything. With that in mind, we‚Äôll perform a log transformation. This cannot be undertaken with negative values, so once again we‚Äôll perform a shift (of 2) for both of our relative error numbers, as well as a commercial resident column, before log transforming our features.\n\nfeature_df$BurglaryRPDTranform &lt;- feature_df$RPDburglaryShifted    + 2\nfeature_df$RobberyRPDTranform &lt;- feature_df$RPDRobberyShifted   + 2\n\nfeature_df$ComEstResTranform &lt;- feature_df$ComEstRes   + 2\n\n\nfor (col in colnames(feature_df)){\n  new_name &lt;- paste(\"log_\", col, sep = \"\")\n  feature_df[new_name] &lt;- log(feature_df[col])\n}\n\ndrop&lt;- c( \"log_ComEstRes\", \"log_RPDburglaryShifted\", \"log_RPDRobberyShifted\")\nfeature_df&lt;- feature_df[,!(names(feature_df) %in% drop)]\n\n\nfeat_transform_df &lt;- feature_df[,17:29]\norig_feat_df &lt;- feature_df[,0:17]\n\nWe‚Äôve now separated a separate dataframe where each value has been log transformed - while this isn‚Äôt hugely rigorous (and would benefit from inspecting the relationships in more detail) it serves our immediate purpose.\n\npairs(feat_transform_df)\n\n\n\n\n\n\n\n\nWhile we‚Äôve introduced a bit of noise, we‚Äôve also ‚Äúforced‚Äù some of our variables into relationships that look semi linear.\nTo dig into this deeper, let‚Äôs create a correlation matrix for our entire transformed dataframe.\n\ncorrelate(feat_transform_df)\n\nCorrelation computed with\n‚Ä¢ Method: 'pearson'\n‚Ä¢ Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\nterm\n\n\nlog_total_burglaries\n\n\nlog_total_robberies\n\n\nlog_DwellingNoResidents\n\n\nlog_MedianHousePrice\n\n\nlog_FlatAprt\n\n\nlog_fullTimeStudents\n\n\nlog_NoCars\n\n\nlog_OtherEthnicGroup\n\n\nlog_CentralHealingPercent\n\n\nlog_AvHholdSz\n\n\nlog_BurglaryRPDTranform\n\n\nlog_RobberyRPDTranform\n\n\nlog_ComEstResTranform\n\n\n\n\n\n\nlog_total_burglaries\n\n\nNA\n\n\n0.6793730\n\n\n0.5009180\n\n\n0.2641776\n\n\n0.5386775\n\n\n0.3830059\n\n\n0.5287612\n\n\n0.3864406\n\n\n-0.3234183\n\n\n-0.3632653\n\n\n-0.2224450\n\n\n-0.3858625\n\n\n0.2461299\n\n\n\n\nlog_total_robberies\n\n\n0.6793730\n\n\nNA\n\n\n0.3528253\n\n\n0.0289555\n\n\n0.6210392\n\n\n0.6262973\n\n\n0.7312742\n\n\n0.5328260\n\n\n-0.4392580\n\n\n-0.1885684\n\n\n-0.1143993\n\n\n-0.3860350\n\n\n0.2003534\n\n\n\n\nlog_DwellingNoResidents\n\n\n0.5009180\n\n\n0.3528253\n\n\nNA\n\n\n0.5259771\n\n\n0.5455129\n\n\n0.2061306\n\n\n0.4187217\n\n\n0.2388495\n\n\n-0.3673864\n\n\n-0.6022479\n\n\n-0.1801381\n\n\n-0.2106489\n\n\n0.2991287\n\n\n\n\nlog_MedianHousePrice\n\n\n0.2641776\n\n\n0.0289555\n\n\n0.5259771\n\n\nNA\n\n\n0.2266006\n\n\n-0.0590916\n\n\n0.0310431\n\n\n0.1415866\n\n\n-0.0743475\n\n\n-0.4236094\n\n\n-0.1667024\n\n\n-0.0843802\n\n\n0.1860743\n\n\n\n\nlog_FlatAprt\n\n\n0.5386775\n\n\n0.6210392\n\n\n0.5455129\n\n\n0.2266006\n\n\nNA\n\n\n0.4899513\n\n\n0.8808568\n\n\n0.5041155\n\n\n-0.4913225\n\n\n-0.5893529\n\n\n-0.0645864\n\n\n-0.1646088\n\n\n0.2781184\n\n\n\n\nlog_fullTimeStudents\n\n\n0.3830059\n\n\n0.6262973\n\n\n0.2061306\n\n\n-0.0590916\n\n\n0.4899513\n\n\nNA\n\n\n0.6188377\n\n\n0.6459824\n\n\n-0.3243579\n\n\n0.1050335\n\n\n-0.0491348\n\n\n-0.2206372\n\n\n0.2923494\n\n\n\n\nlog_NoCars\n\n\n0.5287612\n\n\n0.7312742\n\n\n0.4187217\n\n\n0.0310431\n\n\n0.8808568\n\n\n0.6188377\n\n\nNA\n\n\n0.5238758\n\n\n-0.5402424\n\n\n-0.4448476\n\n\n-0.0361088\n\n\n-0.2002598\n\n\n0.1658930\n\n\n\n\nlog_OtherEthnicGroup\n\n\n0.3864406\n\n\n0.5328260\n\n\n0.2388495\n\n\n0.1415866\n\n\n0.5041155\n\n\n0.6459824\n\n\n0.5238758\n\n\nNA\n\n\n-0.2366892\n\n\n0.0716831\n\n\n-0.0182390\n\n\n-0.1700667\n\n\n0.1260174\n\n\n\n\nlog_CentralHealingPercent\n\n\n-0.3234183\n\n\n-0.4392580\n\n\n-0.3673864\n\n\n-0.0743475\n\n\n-0.4913225\n\n\n-0.3243579\n\n\n-0.5402424\n\n\n-0.2366892\n\n\nNA\n\n\n0.4351582\n\n\n0.0943127\n\n\n0.2435584\n\n\n-0.1453510\n\n\n\n\nlog_AvHholdSz\n\n\n-0.3632653\n\n\n-0.1885684\n\n\n-0.6022479\n\n\n-0.4236094\n\n\n-0.5893529\n\n\n0.1050335\n\n\n-0.4448476\n\n\n0.0716831\n\n\n0.4351582\n\n\nNA\n\n\n0.1430359\n\n\n0.1747301\n\n\n-0.2479341\n\n\n\n\nlog_BurglaryRPDTranform\n\n\n-0.2224450\n\n\n-0.1143993\n\n\n-0.1801381\n\n\n-0.1667024\n\n\n-0.0645864\n\n\n-0.0491348\n\n\n-0.0361088\n\n\n-0.0182390\n\n\n0.0943127\n\n\n0.1430359\n\n\nNA\n\n\n0.1726291\n\n\n-0.1678558\n\n\n\n\nlog_RobberyRPDTranform\n\n\n-0.3858625\n\n\n-0.3860350\n\n\n-0.2106489\n\n\n-0.0843802\n\n\n-0.1646088\n\n\n-0.2206372\n\n\n-0.2002598\n\n\n-0.1700667\n\n\n0.2435584\n\n\n0.1747301\n\n\n0.1726291\n\n\nNA\n\n\n-0.1638404\n\n\n\n\nlog_ComEstResTranform\n\n\n0.2461299\n\n\n0.2003534\n\n\n0.2991287\n\n\n0.1860743\n\n\n0.2781184\n\n\n0.2923494\n\n\n0.1658930\n\n\n0.1260174\n\n\n-0.1453510\n\n\n-0.2479341\n\n\n-0.1678558\n\n\n-0.1638404\n\n\nNA\n\n\n\n\n\n\n\nTo provide a visual aid, I‚Äôve extracted the column for our robbery relative change rate, and sorted the table accordingly.\n\ndplyr::select(correlate(feat_transform_df)[order(correlate(feat_transform_df)$log_BurglaryRPDTranform),], term, log_BurglaryRPDTranform)\n\nCorrelation computed with\n‚Ä¢ Method: 'pearson'\n‚Ä¢ Missing treated using: 'pairwise.complete.obs'\nCorrelation computed with\n‚Ä¢ Method: 'pearson'\n‚Ä¢ Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\nterm\n\n\nlog_BurglaryRPDTranform\n\n\n\n\n\n\nlog_total_burglaries\n\n\n-0.2224450\n\n\n\n\nlog_DwellingNoResidents\n\n\n-0.1801381\n\n\n\n\nlog_ComEstResTranform\n\n\n-0.1678558\n\n\n\n\nlog_MedianHousePrice\n\n\n-0.1667024\n\n\n\n\nlog_total_robberies\n\n\n-0.1143993\n\n\n\n\nlog_FlatAprt\n\n\n-0.0645864\n\n\n\n\nlog_fullTimeStudents\n\n\n-0.0491348\n\n\n\n\nlog_NoCars\n\n\n-0.0361088\n\n\n\n\nlog_OtherEthnicGroup\n\n\n-0.0182390\n\n\n\n\nlog_CentralHealingPercent\n\n\n0.0943127\n\n\n\n\nlog_AvHholdSz\n\n\n0.1430359\n\n\n\n\nlog_RobberyRPDTranform\n\n\n0.1726291\n\n\n\n\nlog_BurglaryRPDTranform\n\n\nNA\n\n\n\n\n\n\n\n\n\nRegression\nNow that we‚Äôve transformed our data, cleaned it up, and identified potential correlates, let‚Äôs build our linear model.\nThere are various automated tools for this process that seek to provide the highest fit and significance, but given the high degree of correlation between my chosen features, I‚Äôve taken a more manual approach and tested a variety of models until I identified one with a suitable fit. The final model is below.\n\nmod_burglary &lt;- lm(log_BurglaryRPDTranform ~ log_total_burglaries + log_FlatAprt + log_MedianHousePrice  + log_ComEstResTranform, data = feat_transform_df)\nsummary(mod_burglary)\n\n\nCall:\nlm(formula = log_BurglaryRPDTranform ~ log_total_burglaries + \n    log_FlatAprt + log_MedianHousePrice + log_ComEstResTranform, \n    data = feat_transform_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.39022 -0.06547 -0.00309  0.05830  0.56322 \n\nCoefficients:\n                       Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)            1.255068   0.112681  11.138 &lt; 0.0000000000000002 ***\nlog_total_burglaries  -0.058851   0.009708  -6.062        0.00000000192 ***\nlog_FlatAprt           0.016153   0.005157   3.132             0.001786 ** \nlog_MedianHousePrice  -0.031691   0.009245  -3.428             0.000634 ***\nlog_ComEstResTranform -0.008125   0.002119  -3.833             0.000135 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1027 on 974 degrees of freedom\nMultiple R-squared:  0.08188,   Adjusted R-squared:  0.07811 \nF-statistic: 21.72 on 4 and 974 DF,  p-value: &lt; 0.00000000000000022\n\n\nOur model suggests the largest burglary decrease linked to lockdown was in MSOAs which a high level of historic burglary. The composition of housing/accomodation and area type also seems to play a role, with those areas with higher median house prices, and a larger number of commercial residents, seeing stronger decreases, while conversely areas with large number of apartments temper the effect.\nWhile all our variables are significant, the model is not a particularly good fit - the adjusted R2 is around 0.08, suggesting that less than 10% of the variance is accounted for by our model. I suspect more geographic features - such as distance from central London, more accurate footfall, or spatial lags - would probably be useful, but that‚Äôs outside the scope of this project.\nLet‚Äôs perform a similar exercise for robbery.\n\ndplyr::select(correlate(feat_transform_df)[order(correlate(feat_transform_df)$log_RobberyRPDTranform),], term, log_RobberyRPDTranform)\n\nCorrelation computed with\n‚Ä¢ Method: 'pearson'\n‚Ä¢ Missing treated using: 'pairwise.complete.obs'\nCorrelation computed with\n‚Ä¢ Method: 'pearson'\n‚Ä¢ Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\nterm\n\n\nlog_RobberyRPDTranform\n\n\n\n\n\n\nlog_total_robberies\n\n\n-0.3860350\n\n\n\n\nlog_total_burglaries\n\n\n-0.3858625\n\n\n\n\nlog_fullTimeStudents\n\n\n-0.2206372\n\n\n\n\nlog_DwellingNoResidents\n\n\n-0.2106489\n\n\n\n\nlog_NoCars\n\n\n-0.2002598\n\n\n\n\nlog_OtherEthnicGroup\n\n\n-0.1700667\n\n\n\n\nlog_FlatAprt\n\n\n-0.1646088\n\n\n\n\nlog_ComEstResTranform\n\n\n-0.1638404\n\n\n\n\nlog_MedianHousePrice\n\n\n-0.0843802\n\n\n\n\nlog_BurglaryRPDTranform\n\n\n0.1726291\n\n\n\n\nlog_AvHholdSz\n\n\n0.1747301\n\n\n\n\nlog_CentralHealingPercent\n\n\n0.2435584\n\n\n\n\nlog_RobberyRPDTranform\n\n\nNA\n\n\n\n\n\n\n\n\nmod_burglary &lt;- lm(log_RobberyRPDTranform ~ log_total_robberies  + log_total_burglaries + log_FlatAprt + log_CentralHealingPercent + log_fullTimeStudents, data = feat_transform_df)\nsummary(mod_burglary)\n\n\nCall:\nlm(formula = log_RobberyRPDTranform ~ log_total_robberies + log_total_burglaries + \n    log_FlatAprt + log_CentralHealingPercent + log_fullTimeStudents, \n    data = feat_transform_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95406 -0.04243 -0.00535  0.04616  0.31826 \n\nCoefficients:\n                           Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)               -5.041998   1.396500  -3.610        0.000321 ***\nlog_total_robberies       -0.030763   0.005495  -5.598 0.0000000282078 ***\nlog_total_burglaries      -0.066724   0.009757  -6.838 0.0000000000141 ***\nlog_FlatAprt               0.029516   0.005148   5.733 0.0000000131343 ***\nlog_CentralHealingPercent  1.302973   0.301809   4.317 0.0000174188038 ***\nlog_fullTimeStudents      -0.001894   0.006809  -0.278        0.780980    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08921 on 973 degrees of freedom\nMultiple R-squared:  0.2103,    Adjusted R-squared:  0.2062 \nF-statistic: 51.81 on 5 and 973 DF,  p-value: &lt; 0.00000000000000022\n\n\nThis is a notably better fit than our burglary model, with our R2 suggesting we now account for over 20% of the variance. The nature of our predictors is also quite different: while we still see a negative relationship with historic crime (with areas of high historic crime experiencing larger relative decreases), there is a positive relationship with both the presence of apartments and central heating.\nI suspect some of these features are correlates of deprivation, so I want to create a quick scatter of three - for now we‚Äôll do it against median house price, which is definitely deprivation correlated.\n\nheating &lt;- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_CentralHealingPercent)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\napartments &lt;- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_FlatAprt)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\ncomest &lt;- ggplot(feature_df, aes(x = log_MedianHousePrice, y = log_ComEstResTranform)) +\n  geom_point()+\n  geom_smooth(method=lm)\n\n\n\nggarrange(heating, apartments, comest, ncol=3, nrow=1 )\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhile there does appear to be a relationship with some of these, it isn‚Äôt strong - this suggests the factor‚Äôs we have identified are significant not because of their association with deprivation and poverty, but because of what they mean about the specific characteristics of the area.\n\n\nRandom Forests\nUnlike regression, random forest doesn‚Äôt really on any specific type of association - instead, we rely on computing power, repetition and iteration to capture the very best predictor for our variable, in any combination.\nThere are risks to this method: our sample size is smaller than I‚Äôd like, and this may lead to over-fit of outlier MSOAs.\nIt does mean we don‚Äôt need to worry about transformations or correlations: we can return to our original dataset, and let the model identify the strongest predictors.\n\n# we remove rows where our main error is na or missing\nrf_msoa_matrix &lt;- drop_na(msoa_matrix_numeric, RPDRobberyShifted)\n\n#then we repeat for any columns where value asre missing\nclean_rf_matrix &lt;- rf_msoa_matrix[ , colSums(is.na(rf_msoa_matrix)) == 0]\n\n\n#then we drop out any values that directly predict our error.\n\ndrop&lt;- c(\"burglaryActual\",\"burglaryError\",\"burglaryPercentError\",\"burglaryPredicted\",\"robberyActual\",\"robberyPredicted\",\"robberyError\",\"robberyPercentError\", \"RPDBurglaryShifted\", \"RPDRobberyShifted\")\n\n#remove out selected columns\ndata&lt;- clean_rf_matrix[,!(names(clean_rf_matrix) %in% drop)]\n\n\n#automatically remove white space and make names r compatbiel\nnames(clean_rf_matrix)&lt;-make.names(names(clean_rf_matrix),unique = TRUE)\n\ndrop&lt;- c(\"burglaryActual\",\"burglaryError\",\"robberyActual\",\"robberyError\",\"RPDBurglary\",\"RPDRobbery\",\"robberyActualShifted\",\"robberyPredictedShifted\",\"burglaryActualShifted\",\"burglaryPredictedShifted\", \"burglaryPredicted\", \"burglaryPercentError\", \"robberyPredicted\", \"robberyPercentError\")\n#drop&lt;- c(\"burglaryPercentError\",\"burglaryPredicted\",\"robberyPredicted\",\"robberyPercentError\")\n\ndata&lt;- clean_rf_matrix[,!(names(clean_rf_matrix) %in% drop)]\n\n\nnames(data)&lt;- make.names(names(data),unique = TRUE)\nhead(data)\n\n\n\n\n\n\nUsualRes\n\n\nHholdRes\n\n\nComEstRes\n\n\nPopDen\n\n\nHholds\n\n\nAvHholdSz\n\n\ntotal_robberies\n\n\ntotal_burglaries\n\n\nRPDRobberyShifted\n\n\nRPDburglaryShifted\n\n\nAge.Structure..2011.Census..All.Ages\n\n\nAge.Structure..2011.Census..0.15\n\n\nAge.Structure..2011.Census..16.29\n\n\nAge.Structure..2011.Census..30.44\n\n\nAge.Structure..2011.Census..45.64\n\n\nAge.Structure..2011.Census..65.\n\n\nAge.Structure..2011.Census..Working.age\n\n\nMid.year.Estimate.totals.All.Ages.2002\n\n\nMid.year.Estimate.totals.All.Ages.2003\n\n\nMid.year.Estimate.totals.All.Ages.2004\n\n\nMid.year.Estimate.totals.All.Ages.2005\n\n\nMid.year.Estimate.totals.All.Ages.2006\n\n\nMid.year.Estimate.totals.All.Ages.2007\n\n\nMid.year.Estimate.totals.All.Ages.2008\n\n\nMid.year.Estimate.totals.All.Ages.2009\n\n\nMid.year.Estimate.totals.All.Ages.2010\n\n\nMid.year.Estimate.totals.All.Ages.2011\n\n\nMid.year.Estimate.totals.All.Ages.2012\n\n\nMid.year.Estimates.2012..by.age‚Ä¶0.to.14\n\n\nMid.year.Estimates.2012..by.age‚Ä¶15.64\n\n\nMid.year.Estimates.2012..by.age‚Ä¶65.\n\n\nMid.year.Estimates.2012..by.age.0.4\n\n\nMid.year.Estimates.2012..by.age.5.9\n\n\nMid.year.Estimates.2012..by.age.10.14\n\n\nMid.year.Estimates.2012..by.age.15.19\n\n\nMid.year.Estimates.2012..by.age.20.24\n\n\nMid.year.Estimates.2012..by.age.25.29\n\n\nMid.year.Estimates.2012..by.age.30.34\n\n\nMid.year.Estimates.2012..by.age.35.39\n\n\nMid.year.Estimates.2012..by.age.40.44\n\n\nMid.year.Estimates.2012..by.age.45.49\n\n\nMid.year.Estimates.2012..by.age.50.54\n\n\nMid.year.Estimates.2012..by.age.55.59\n\n\nMid.year.Estimates.2012..by.age.60.64\n\n\nMid.year.Estimates.2012..by.age.65.69\n\n\nMid.year.Estimates.2012..by.age.70.74\n\n\nMid.year.Estimates.2012..by.age.75.79\n\n\nMid.year.Estimates.2012..by.age.80.84\n\n\nMid.year.Estimates.2012..by.age.85.89\n\n\nMid.year.Estimates.2012..by.age.90.\n\n\nHouseholds..2011..All.Households\n\n\nHousehold.Composition..2011..Numbers.Couple.household.with.dependent.children\n\n\nHousehold.Composition..2011..Numbers.Couple.household.without.dependent.children\n\n\nHousehold.Composition..2011..Numbers.Lone.parent.household\n\n\nHousehold.Composition..2011..Numbers.One.person.household\n\n\nHousehold.Composition..2011..Numbers.Other.household.Types\n\n\nHousehold.Composition..2011..Percentages.Couple.household.with.dependent.children\n\n\nHousehold.Composition..2011..Percentages.Couple.household.without.dependent.children\n\n\nHousehold.Composition..2011..Percentages.Lone.parent.household\n\n\nHousehold.Composition..2011..Percentages.One.person.household\n\n\nHousehold.Composition..2011..Percentages.Other.household.Types\n\n\nEthnic.Group..2011.Census..White\n\n\nEthnic.Group..2011.Census..Mixed.multiple.ethnic.groups\n\n\nEthnic.Group..2011.Census..Asian.Asian.British\n\n\nEthnic.Group..2011.Census..Black.African.Caribbean.Black.British\n\n\nEthnic.Group..2011.Census..Other.ethnic.group\n\n\nEthnic.Group..2011.Census..BAME\n\n\nEthnic.Group..2011.Census..White‚Ä¶.\n\n\nEthnic.Group..2011.Census..Mixed.multiple.ethnic.groups‚Ä¶.\n\n\nEthnic.Group..2011.Census..Asian.Asian.British‚Ä¶.\n\n\nEthnic.Group..2011.Census..Black.African.Caribbean.Black.British‚Ä¶.\n\n\nEthnic.Group..2011.Census..Other.ethnic.group‚Ä¶.\n\n\nEthnic.Group..2011.Census..BAME‚Ä¶.\n\n\nCountry.of.Birth..2011..United.Kingdom\n\n\nCountry.of.Birth..2011..Not.United.Kingdom\n\n\nCountry.of.Birth..2011..United.Kingdom‚Ä¶.\n\n\nCountry.of.Birth..2011..Not.United.Kingdom‚Ä¶.\n\n\nHousehold.Language..2011..At.least.one.person.aged.16.and.over.in.household.has.English.as.a.main.language\n\n\nHousehold.Language..2011..No.people.in.household.have.English.as.a.main.language\n\n\nHousehold.Language..2011‚Ä¶.of.people.aged.16.and.over.in.household.have.English.as.a.main.language\n\n\nHousehold.Language..2011‚Ä¶.of.households.where.no.people.in.household.have.English.as.a.main.language\n\n\nReligion..2011..Christian\n\n\nReligion..2011..Buddhist\n\n\nReligion..2011..Hindu\n\n\nReligion..2011..Jewish\n\n\nReligion..2011..Muslim\n\n\nReligion..2011..Sikh\n\n\nReligion..2011..Other.religion\n\n\nReligion..2011..No.religion\n\n\nReligion..2011..Religion.not.stated\n\n\nReligion..2011..Christian‚Ä¶.\n\n\nReligion..2011..Buddhist‚Ä¶.\n\n\nReligion..2011..Hindu‚Ä¶.\n\n\nReligion..2011..Jewish‚Ä¶.\n\n\nReligion..2011..Muslim‚Ä¶.\n\n\nReligion..2011..Sikh‚Ä¶.\n\n\nReligion..2011..Other.religion‚Ä¶.\n\n\nReligion..2011..No.religion‚Ä¶.\n\n\nReligion..2011..Religion.not.stated‚Ä¶.\n\n\nTenure..2011..Owned..Owned.outright\n\n\nTenure..2011..Owned..Owned.with.a.mortgage.or.loan\n\n\nTenure..2011..Social.rented\n\n\nTenure..2011..Private.rented\n\n\nTenure..2011..Owned..Owned.outright‚Ä¶.\n\n\nTenure..2011..Owned..Owned.with.a.mortgage.or.loan‚Ä¶.\n\n\nTenure..2011..Social.rented‚Ä¶.\n\n\nTenure..2011..Private.rented‚Ä¶.\n\n\nDwelling.type..2011..Household.spaces.with.at.least.one.usual.resident\n\n\nDwelling.type..2011..Household.spaces.with.no.usual.residents\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Detached\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Semi.detached\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Terraced..including.end.terrace.\n\n\nDwelling.type..2011..Flat..maisonette.or.apartment\n\n\nDwelling.type..2011..Household.spaces.with.at.least.one.usual.resident‚Ä¶.\n\n\nDwelling.type..2011..Household.spaces.with.no.usual.residents‚Ä¶.\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Detached‚Ä¶.\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Semi.detached‚Ä¶.\n\n\nDwelling.type..2011..Whole.house.or.bungalow..Terraced..including.end.terrace‚Ä¶..\n\n\nDwelling.type..2011..Flat..maisonette.or.apartment‚Ä¶.\n\n\nLand.Area.Hectares\n\n\nPopulation.Density.Persons.per.hectare..2012.\n\n\nHouse.Prices.Median.House.Price‚Ä¶..2005\n\n\nHouse.Prices.Median.House.Price‚Ä¶..2006\n\n\nHouse.Prices.Median.House.Price‚Ä¶..2007\n\n\nHouse.Prices.Median.House.Price‚Ä¶..2008\n\n\nHouse.Prices.Median.House.Price‚Ä¶..2009\n\n\nHouse.Prices.Median.House.Price‚Ä¶..2010\n\n\nHouse.Prices.Median.House.Price‚Ä¶..2011\n\n\nHouse.Prices.Median.House.Price‚Ä¶..2012\n\n\nHouse.Prices.Median.House.Price‚Ä¶..2013..p.\n\n\nHouse.Prices.Sales.2005\n\n\nHouse.Prices.Sales.2006\n\n\nHouse.Prices.Sales.2007\n\n\nHouse.Prices.Sales.2008\n\n\nHouse.Prices.Sales.2009\n\n\nHouse.Prices.Sales.2010\n\n\nHouse.Prices.Sales.2011‚Ä¶129\n\n\nHouse.Prices.Sales.2011‚Ä¶130\n\n\nHouse.Prices.Sales.2013.p.\n\n\nQualifications..2011.Census..No.qualifications\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Level.1.qualifications\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Level.2.qualifications\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Apprenticeship\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Level.3.qualifications\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Level.4.qualifications.and.above\n\n\nQualifications..2011.Census..Highest.level.of.qualification..Other.qualifications\n\n\nQualifications..2011.Census..Schoolchildren.and.full.time.students..Age.18.and.over\n\n\nEconomic.Activity..2011.Census..Economically.active..Total\n\n\nEconomic.Activity..2011.Census..Economically.active..Unemployed\n\n\nEconomic.Activity..2011.Census..Economically.inactive..Total\n\n\nEconomic.Activity..2011.Census..Economically.active..\n\n\nEconomic.Activity..2011.Census..Unemployment.Rate\n\n\nEconomic.Activity..2011.Census..Economically.inactive..\n\n\nAdults.in.Employment..2011.Census..No.adults.in.employment.in.household..With.dependent.children\n\n\nAdults.in.Employment..2011.Census‚Ä¶.of.households.with.no.adults.in.employment..With.dependent.children\n\n\nHousehold.Income.Estimates..2011.12..Total.Mean.Annual.Household.Income‚Ä¶.\n\n\nHousehold.Income.Estimates..2011.12..Total.Median.Annual.Household.Income‚Ä¶.\n\n\nIncome.Deprivation..2010‚Ä¶.living.in.income.deprived.households.reliant.on.means.tested.benefit\n\n\nIncome.Deprivation..2010‚Ä¶.of.people.aged.over.60.who.live.in.pension.credit.households\n\n\nLone.Parents..2011.Census..All.lone.parent.housholds.with.dependent.children\n\n\nLone.Parents..2011.Census..Lone.parents.not.in.employment\n\n\nLone.Parents..2011.Census..Lone.parent.not.in.employment..\n\n\nCentral.Heating..2011.Census..Households.with.central.heating‚Ä¶.\n\n\nHealth..2011.Census..Day.to.day.activities.limited.a.lot\n\n\nHealth..2011.Census..Day.to.day.activities.limited.a.little\n\n\nHealth..2011.Census..Day.to.day.activities.not.limited\n\n\nHealth..2011.Census..Day.to.day.activities.limited.a.lot‚Ä¶.\n\n\nHealth..2011.Census..Day.to.day.activities.limited.a.little‚Ä¶.\n\n\nHealth..2011.Census..Day.to.day.activities.not.limited‚Ä¶.\n\n\nHealth..2011.Census..Very.good.health\n\n\nHealth..2011.Census..Good.health\n\n\nHealth..2011.Census..Fair.health\n\n\nHealth..2011.Census..Bad.health\n\n\nHealth..2011.Census..Very.bad.health\n\n\nHealth..2011.Census..Very.good.health‚Ä¶.\n\n\nHealth..2011.Census..Good.health‚Ä¶.\n\n\nHealth..2011.Census..Fair.health‚Ä¶.\n\n\nHealth..2011.Census..Bad.health‚Ä¶.\n\n\nHealth..2011.Census..Very.bad.health‚Ä¶.\n\n\nLow.Birth.Weight.Births..2007.2011..Low.Birth.Weight.Births‚Ä¶.\n\n\nLow.Birth.Weight.Births..2007.2011..LCL‚Ä¶Lower.confidence.limit\n\n\nLow.Birth.Weight.Births..2007.2011..UCL‚Ä¶Upper.confidence.limit\n\n\nObesity.Percentage.of.the.population.aged.16..with.a.BMI.of.30‚Ä¶modelled.estimate..2006.2008\n\n\nIncidence.of.Cancer.All\n\n\nIncidence.of.Cancer.Breast.Cancer\n\n\nLife.Expectancy.Males\n\n\nLife.Expectancy.Females\n\n\nCar.or.van.availability..2011.Census..No.cars.or.vans.in.household\n\n\nCar.or.van.availability..2011.Census..1.car.or.van.in.household\n\n\nCar.or.van.availability..2011.Census..2.cars.or.vans.in.household\n\n\nCar.or.van.availability..2011.Census..3.cars.or.vans.in.household\n\n\nCar.or.van.availability..2011.Census..4.or.more.cars.or.vans.in.household\n\n\nCar.or.van.availability..2011.Census..Sum.of.all.cars.or.vans.in.the.area\n\n\nCar.or.van.availability..2011.Census..No.cars.or.vans.in.household‚Ä¶.\n\n\nCar.or.van.availability..2011.Census..1.car.or.van.in.household‚Ä¶.\n\n\nCar.or.van.availability..2011.Census..2.cars.or.vans.in.household‚Ä¶.\n\n\nCar.or.van.availability..2011.Census..3.cars.or.vans.in.household‚Ä¶.\n\n\nCar.or.van.availability..2011.Census..4.or.more.cars.or.vans.in.household‚Ä¶.\n\n\nCar.or.van.availability..2011.Census..Cars.per.household\n\n\nRoad.Casualties.2010.Fatal\n\n\nRoad.Casualties.2010.Serious\n\n\nRoad.Casualties.2010.Slight\n\n\nRoad.Casualties.2010.2010.Total\n\n\nRoad.Casualties.2011.Fatal\n\n\nRoad.Casualties.2011.Serious\n\n\nRoad.Casualties.2011.Slight\n\n\nRoad.Casualties.2011.2011.Total\n\n\nRoad.Casualties.2012.Fatal\n\n\nRoad.Casualties.2012.Serious\n\n\nRoad.Casualties.2012.Slight\n\n\nRoad.Casualties.2012.2012.Total\n\n\n\n\n\n\n7375\n\n\n7187\n\n\n188\n\n\n25.5\n\n\n4385\n\n\n1.6\n\n\n53\n\n\n58\n\n\n0.0601204\n\n\n-0.1219796\n\n\n7375\n\n\n620\n\n\n1665\n\n\n2045\n\n\n2010\n\n\n1035\n\n\n5720\n\n\n7280\n\n\n7115\n\n\n7118\n\n\n7131\n\n\n7254\n\n\n7607\n\n\n7429\n\n\n7472\n\n\n7338\n\n\n7412\n\n\n7604\n\n\n8.771699\n\n\n76.68332\n\n\n14.54498\n\n\n297\n\n\n205\n\n\n165\n\n\n231\n\n\n495\n\n\n949\n\n\n826\n\n\n622\n\n\n663\n\n\n598\n\n\n504\n\n\n470\n\n\n473\n\n\n363\n\n\n263\n\n\n192\n\n\n155\n\n\n86\n\n\n47\n\n\n4385\n\n\n306\n\n\n927\n\n\n153\n\n\n2472\n\n\n527\n\n\n6.978335\n\n\n21.14025\n\n\n3.489168\n\n\n56.37400\n\n\n12.01824\n\n\n5799\n\n\n289\n\n\n940\n\n\n193\n\n\n154\n\n\n1576\n\n\n78.63051\n\n\n3.918644\n\n\n12.745763\n\n\n2.616949\n\n\n2.0881356\n\n\n21.36949\n\n\n4670\n\n\n2705\n\n\n63.32203\n\n\n36.67797\n\n\n3825\n\n\n560\n\n\n87.22919\n\n\n12.770810\n\n\n3344\n\n\n92\n\n\n145\n\n\n166\n\n\n409\n\n\n18\n\n\n28\n\n\n2522\n\n\n651\n\n\n45.3\n\n\n1.2\n\n\n2.0\n\n\n2.3\n\n\n5.5\n\n\n0.2\n\n\n0.4\n\n\n34.2\n\n\n8.8\n\n\n1093\n\n\n762\n\n\n725\n\n\n1573\n\n\n24.9\n\n\n17.4\n\n\n16.5\n\n\n35.9\n\n\n4385\n\n\n1145\n\n\n22\n\n\n12\n\n\n80\n\n\n5416\n\n\n79.3\n\n\n20.7\n\n\n0.4\n\n\n0.2\n\n\n1.4\n\n\n98.0\n\n\n289.78\n\n\n26.24060\n\n\n310000\n\n\n341000\n\n\n412500\n\n\n365000.0\n\n\n410000\n\n\n450000\n\n\n465000\n\n\n485000\n\n\n595000\n\n\n303\n\n\n295\n\n\n268\n\n\n141\n\n\n157\n\n\n235\n\n\n256\n\n\n195\n\n\n353\n\n\n454\n\n\n291\n\n\n445\n\n\n47\n\n\n484\n\n\n4618\n\n\n416\n\n\n422\n\n\n4972\n\n\n187\n\n\n1335\n\n\n78.83304\n\n\n3.761062\n\n\n21.16696\n\n\n38\n\n\n0.9\n\n\n59728.48\n\n\n46788.30\n\n\n5.2\n\n\n9.9\n\n\n91\n\n\n22\n\n\n24.2\n\n\n95.7\n\n\n328\n\n\n520\n\n\n6527\n\n\n4.447458\n\n\n7.050847\n\n\n88.50169\n\n\n4112\n\n\n2374\n\n\n643\n\n\n190\n\n\n56\n\n\n55.75593\n\n\n32.18983\n\n\n8.718644\n\n\n2.576271\n\n\n0.759322\n\n\n4.2\n\n\n2.5\n\n\n7.1\n\n\n13.7\n\n\n76.8\n\n\n90.9\n\n\n83.6\n\n\n88.4\n\n\n3043\n\n\n1100\n\n\n173\n\n\n51\n\n\n18\n\n\n1692\n\n\n69.4\n\n\n25.1\n\n\n3.9\n\n\n1.2\n\n\n0.4\n\n\n0.3858609\n\n\n1\n\n\n39\n\n\n334\n\n\n374\n\n\n0\n\n\n46\n\n\n359\n\n\n405\n\n\n2\n\n\n51\n\n\n361\n\n\n414\n\n\n\n\n6775\n\n\n6724\n\n\n51\n\n\n31.3\n\n\n2713\n\n\n2.5\n\n\n53\n\n\n136\n\n\n-0.0223392\n\n\n0.3489691\n\n\n6775\n\n\n1751\n\n\n1277\n\n\n1388\n\n\n1258\n\n\n1101\n\n\n3923\n\n\n6333\n\n\n6312\n\n\n6329\n\n\n6341\n\n\n6330\n\n\n6323\n\n\n6369\n\n\n6570\n\n\n6636\n\n\n6783\n\n\n6853\n\n\n25.113089\n\n\n58.73340\n\n\n16.15351\n\n\n652\n\n\n607\n\n\n462\n\n\n458\n\n\n399\n\n\n468\n\n\n466\n\n\n466\n\n\n461\n\n\n450\n\n\n347\n\n\n254\n\n\n256\n\n\n254\n\n\n206\n\n\n215\n\n\n201\n\n\n137\n\n\n94\n\n\n2713\n\n\n491\n\n\n366\n\n\n597\n\n\n814\n\n\n445\n\n\n18.098046\n\n\n13.49060\n\n\n22.005160\n\n\n30.00369\n\n\n16.40251\n\n\n4403\n\n\n330\n\n\n820\n\n\n1133\n\n\n89\n\n\n2372\n\n\n64.98893\n\n\n4.870849\n\n\n12.103321\n\n\n16.723247\n\n\n1.3136531\n\n\n35.01107\n\n\n5159\n\n\n1616\n\n\n76.14760\n\n\n23.85240\n\n\n2459\n\n\n254\n\n\n90.63767\n\n\n9.362329\n\n\n3975\n\n\n19\n\n\n174\n\n\n27\n\n\n591\n\n\n122\n\n\n16\n\n\n1417\n\n\n434\n\n\n58.7\n\n\n0.3\n\n\n2.6\n\n\n0.4\n\n\n8.7\n\n\n1.8\n\n\n0.2\n\n\n20.9\n\n\n6.4\n\n\n596\n\n\n663\n\n\n1133\n\n\n269\n\n\n22.0\n\n\n24.4\n\n\n41.8\n\n\n9.9\n\n\n2713\n\n\n82\n\n\n99\n\n\n744\n\n\n865\n\n\n1087\n\n\n97.1\n\n\n2.9\n\n\n3.5\n\n\n26.6\n\n\n30.9\n\n\n38.9\n\n\n216.15\n\n\n31.70483\n\n\n168500\n\n\n180000\n\n\n187500\n\n\n197500.0\n\n\n190000\n\n\n173000\n\n\n185000\n\n\n182250\n\n\n190000\n\n\n81\n\n\n100\n\n\n100\n\n\n68\n\n\n45\n\n\n61\n\n\n51\n\n\n42\n\n\n61\n\n\n1623\n\n\n789\n\n\n706\n\n\n118\n\n\n479\n\n\n914\n\n\n395\n\n\n272\n\n\n2847\n\n\n335\n\n\n1513\n\n\n65.29817\n\n\n11.766772\n\n\n34.70183\n\n\n319\n\n\n11.8\n\n\n31788.19\n\n\n27058.70\n\n\n31.0\n\n\n27.5\n\n\n445\n\n\n249\n\n\n56.0\n\n\n97.5\n\n\n707\n\n\n678\n\n\n5390\n\n\n10.435424\n\n\n10.007380\n\n\n79.55720\n\n\n2933\n\n\n2288\n\n\n1059\n\n\n389\n\n\n106\n\n\n43.29151\n\n\n33.77122\n\n\n15.630996\n\n\n5.741697\n\n\n1.564576\n\n\n10.6\n\n\n8.4\n\n\n13.3\n\n\n29.8\n\n\n100.7\n\n\n93.3\n\n\n78.0\n\n\n80.1\n\n\n1020\n\n\n1186\n\n\n424\n\n\n66\n\n\n17\n\n\n2305\n\n\n37.6\n\n\n43.7\n\n\n15.6\n\n\n2.4\n\n\n0.6\n\n\n0.8496130\n\n\n0\n\n\n0\n\n\n18\n\n\n18\n\n\n0\n\n\n2\n\n\n16\n\n\n18\n\n\n0\n\n\n1\n\n\n15\n\n\n16\n\n\n\n\n10045\n\n\n10033\n\n\n12\n\n\n46.9\n\n\n3834\n\n\n2.6\n\n\n124\n\n\n239\n\n\n0.0526430\n\n\n-0.0218569\n\n\n10045\n\n\n2247\n\n\n1959\n\n\n2300\n\n\n2259\n\n\n1280\n\n\n6518\n\n\n9236\n\n\n9252\n\n\n9155\n\n\n9072\n\n\n9144\n\n\n9227\n\n\n9564\n\n\n9914\n\n\n10042\n\n\n10088\n\n\n10218\n\n\n21.442552\n\n\n65.81523\n\n\n12.74222\n\n\n849\n\n\n739\n\n\n603\n\n\n615\n\n\n686\n\n\n804\n\n\n818\n\n\n743\n\n\n770\n\n\n713\n\n\n624\n\n\n526\n\n\n426\n\n\n372\n\n\n277\n\n\n258\n\n\n198\n\n\n124\n\n\n73\n\n\n3834\n\n\n776\n\n\n730\n\n\n589\n\n\n1039\n\n\n700\n\n\n20.239958\n\n\n19.04017\n\n\n15.362546\n\n\n27.09963\n\n\n18.25769\n\n\n5486\n\n\n433\n\n\n2284\n\n\n1618\n\n\n224\n\n\n4559\n\n\n54.61424\n\n\n4.310602\n\n\n22.737680\n\n\n16.107516\n\n\n2.2299652\n\n\n45.38576\n\n\n7193\n\n\n2852\n\n\n71.60777\n\n\n28.39223\n\n\n3431\n\n\n403\n\n\n89.48878\n\n\n10.511215\n\n\n5475\n\n\n46\n\n\n476\n\n\n42\n\n\n1351\n\n\n430\n\n\n34\n\n\n1514\n\n\n677\n\n\n54.5\n\n\n0.5\n\n\n4.7\n\n\n0.4\n\n\n13.4\n\n\n4.3\n\n\n0.3\n\n\n15.1\n\n\n6.7\n\n\n1028\n\n\n1473\n\n\n446\n\n\n830\n\n\n26.8\n\n\n38.4\n\n\n11.6\n\n\n21.6\n\n\n3834\n\n\n110\n\n\n161\n\n\n936\n\n\n1591\n\n\n1256\n\n\n97.2\n\n\n2.8\n\n\n4.1\n\n\n23.7\n\n\n40.3\n\n\n31.8\n\n\n214.15\n\n\n47.71422\n\n\n185000\n\n\n197750\n\n\n220000\n\n\n225000.0\n\n\n188500\n\n\n215000\n\n\n200000\n\n\n205500\n\n\n237000\n\n\n203\n\n\n232\n\n\n295\n\n\n100\n\n\n80\n\n\n97\n\n\n89\n\n\n114\n\n\n98\n\n\n1778\n\n\n1210\n\n\n1236\n\n\n169\n\n\n847\n\n\n1829\n\n\n729\n\n\n442\n\n\n5038\n\n\n459\n\n\n2111\n\n\n70.47139\n\n\n9.110758\n\n\n29.52861\n\n\n268\n\n\n7.0\n\n\n43356.93\n\n\n36834.53\n\n\n18.9\n\n\n21.2\n\n\n408\n\n\n199\n\n\n48.8\n\n\n96.5\n\n\n792\n\n\n810\n\n\n8443\n\n\n7.884520\n\n\n8.063713\n\n\n84.05177\n\n\n4566\n\n\n3633\n\n\n1325\n\n\n406\n\n\n115\n\n\n45.45545\n\n\n36.16725\n\n\n13.190642\n\n\n4.041812\n\n\n1.144848\n\n\n7.8\n\n\n6.1\n\n\n9.9\n\n\n28.3\n\n\n91.4\n\n\n107.3\n\n\n80.2\n\n\n85.6\n\n\n1196\n\n\n1753\n\n\n691\n\n\n155\n\n\n39\n\n\n3766\n\n\n31.2\n\n\n45.7\n\n\n18.0\n\n\n4.0\n\n\n1.0\n\n\n0.9822640\n\n\n0\n\n\n3\n\n\n34\n\n\n37\n\n\n1\n\n\n4\n\n\n40\n\n\n45\n\n\n0\n\n\n3\n\n\n47\n\n\n50\n\n\n\n\n6182\n\n\n5937\n\n\n245\n\n\n24.8\n\n\n2318\n\n\n2.6\n\n\n30\n\n\n81\n\n\n0.0257532\n\n\n0.1381492\n\n\n6182\n\n\n1196\n\n\n1277\n\n\n1154\n\n\n1543\n\n\n1012\n\n\n3974\n\n\n6208\n\n\n6159\n\n\n6163\n\n\n6152\n\n\n5997\n\n\n6005\n\n\n6084\n\n\n6268\n\n\n6237\n\n\n6185\n\n\n6308\n\n\n18.246671\n\n\n65.82118\n\n\n15.93215\n\n\n409\n\n\n364\n\n\n378\n\n\n516\n\n\n472\n\n\n435\n\n\n363\n\n\n399\n\n\n388\n\n\n463\n\n\n459\n\n\n354\n\n\n303\n\n\n226\n\n\n238\n\n\n194\n\n\n172\n\n\n106\n\n\n69\n\n\n2318\n\n\n508\n\n\n524\n\n\n322\n\n\n609\n\n\n355\n\n\n21.915444\n\n\n22.60569\n\n\n13.891286\n\n\n26.27265\n\n\n15.31493\n\n\n5006\n\n\n186\n\n\n313\n\n\n649\n\n\n28\n\n\n1176\n\n\n80.97703\n\n\n3.008735\n\n\n5.063086\n\n\n10.498221\n\n\n0.4529279\n\n\n19.02297\n\n\n5292\n\n\n890\n\n\n85.60336\n\n\n14.39664\n\n\n2214\n\n\n104\n\n\n95.51337\n\n\n4.486626\n\n\n4070\n\n\n21\n\n\n34\n\n\n23\n\n\n234\n\n\n10\n\n\n18\n\n\n1373\n\n\n399\n\n\n65.8\n\n\n0.3\n\n\n0.5\n\n\n0.4\n\n\n3.8\n\n\n0.2\n\n\n0.3\n\n\n22.2\n\n\n6.5\n\n\n718\n\n\n969\n\n\n371\n\n\n228\n\n\n31.0\n\n\n41.8\n\n\n16.0\n\n\n9.8\n\n\n2318\n\n\n45\n\n\n92\n\n\n858\n\n\n1094\n\n\n314\n\n\n98.1\n\n\n1.9\n\n\n3.9\n\n\n36.3\n\n\n46.3\n\n\n13.2\n\n\n249.28\n\n\n25.30488\n\n\n181000\n\n\n182000\n\n\n212000\n\n\n208997.5\n\n\n182000\n\n\n192000\n\n\n193750\n\n\n205000\n\n\n210000\n\n\n93\n\n\n131\n\n\n125\n\n\n66\n\n\n54\n\n\n61\n\n\n72\n\n\n58\n\n\n67\n\n\n1502\n\n\n800\n\n\n825\n\n\n163\n\n\n539\n\n\n891\n\n\n266\n\n\n215\n\n\n3187\n\n\n296\n\n\n1251\n\n\n71.81163\n\n\n9.287731\n\n\n28.18837\n\n\n122\n\n\n5.3\n\n\n46701.44\n\n\n39668.21\n\n\n15.8\n\n\n21.3\n\n\n206\n\n\n87\n\n\n42.2\n\n\n98.5\n\n\n586\n\n\n547\n\n\n5049\n\n\n9.479133\n\n\n8.848269\n\n\n81.67260\n\n\n2857\n\n\n2086\n\n\n861\n\n\n302\n\n\n76\n\n\n46.21482\n\n\n33.74313\n\n\n13.927532\n\n\n4.885150\n\n\n1.229376\n\n\n5.8\n\n\n3.9\n\n\n8.6\n\n\n26.9\n\n\n96.1\n\n\n95.5\n\n\n77.9\n\n\n80.7\n\n\n556\n\n\n1085\n\n\n515\n\n\n128\n\n\n34\n\n\n2650\n\n\n24.0\n\n\n46.8\n\n\n22.2\n\n\n5.5\n\n\n1.5\n\n\n1.1432269\n\n\n0\n\n\n1\n\n\n13\n\n\n14\n\n\n0\n\n\n2\n\n\n7\n\n\n9\n\n\n0\n\n\n2\n\n\n5\n\n\n7\n\n\n\n\n8562\n\n\n8562\n\n\n0\n\n\n72.1\n\n\n3183\n\n\n2.7\n\n\n87\n\n\n161\n\n\n-0.1490559\n\n\n-0.0107732\n\n\n8562\n\n\n2200\n\n\n1592\n\n\n1995\n\n\n1829\n\n\n946\n\n\n5416\n\n\n7919\n\n\n7922\n\n\n7882\n\n\n7887\n\n\n7917\n\n\n7916\n\n\n8025\n\n\n8317\n\n\n8519\n\n\n8588\n\n\n8660\n\n\n24.237875\n\n\n64.57275\n\n\n11.18938\n\n\n783\n\n\n692\n\n\n624\n\n\n657\n\n\n525\n\n\n608\n\n\n616\n\n\n643\n\n\n673\n\n\n656\n\n\n508\n\n\n386\n\n\n320\n\n\n321\n\n\n202\n\n\n194\n\n\n127\n\n\n90\n\n\n35\n\n\n3183\n\n\n691\n\n\n583\n\n\n593\n\n\n808\n\n\n508\n\n\n21.709080\n\n\n18.31605\n\n\n18.630223\n\n\n25.38486\n\n\n15.95979\n\n\n5674\n\n\n313\n\n\n1050\n\n\n1445\n\n\n80\n\n\n2888\n\n\n66.26956\n\n\n3.655688\n\n\n12.263490\n\n\n16.876898\n\n\n0.9343611\n\n\n33.73044\n\n\n6425\n\n\n2137\n\n\n75.04088\n\n\n24.95912\n\n\n2868\n\n\n315\n\n\n90.10368\n\n\n9.896324\n\n\n4986\n\n\n28\n\n\n138\n\n\n35\n\n\n762\n\n\n166\n\n\n13\n\n\n1816\n\n\n618\n\n\n58.2\n\n\n0.3\n\n\n1.6\n\n\n0.4\n\n\n8.9\n\n\n1.9\n\n\n0.2\n\n\n21.2\n\n\n7.2\n\n\n711\n\n\n1146\n\n\n793\n\n\n482\n\n\n22.3\n\n\n36.0\n\n\n24.9\n\n\n15.1\n\n\n3183\n\n\n89\n\n\n136\n\n\n622\n\n\n2141\n\n\n373\n\n\n97.3\n\n\n2.7\n\n\n4.2\n\n\n19.0\n\n\n65.4\n\n\n11.4\n\n\n118.81\n\n\n72.88949\n\n\n162250\n\n\n170000\n\n\n185000\n\n\n200000.0\n\n\n166250\n\n\n169000\n\n\n165000\n\n\n167000\n\n\n180000\n\n\n152\n\n\n168\n\n\n184\n\n\n73\n\n\n60\n\n\n58\n\n\n81\n\n\n59\n\n\n75\n\n\n1839\n\n\n1026\n\n\n1038\n\n\n160\n\n\n653\n\n\n1119\n\n\n527\n\n\n333\n\n\n4052\n\n\n394\n\n\n1881\n\n\n68.29597\n\n\n9.723593\n\n\n31.70403\n\n\n307\n\n\n9.6\n\n\n34293.82\n\n\n29155.68\n\n\n22.9\n\n\n25.3\n\n\n442\n\n\n231\n\n\n52.3\n\n\n96.3\n\n\n713\n\n\n722\n\n\n7127\n\n\n8.327494\n\n\n8.432609\n\n\n83.23990\n\n\n4089\n\n\n2811\n\n\n1134\n\n\n412\n\n\n116\n\n\n47.75753\n\n\n32.83111\n\n\n13.244569\n\n\n4.811960\n\n\n1.354824\n\n\n7.7\n\n\n6.0\n\n\n9.9\n\n\n29.7\n\n\n110.0\n\n\n106.1\n\n\n76.8\n\n\n79.9\n\n\n1080\n\n\n1423\n\n\n551\n\n\n109\n\n\n20\n\n\n2937\n\n\n33.9\n\n\n44.7\n\n\n17.3\n\n\n3.4\n\n\n0.6\n\n\n0.9227144\n\n\n1\n\n\n5\n\n\n24\n\n\n30\n\n\n0\n\n\n2\n\n\n27\n\n\n29\n\n\n0\n\n\n1\n\n\n24\n\n\n25\n\n\n\n\n8791\n\n\n8672\n\n\n119\n\n\n50.6\n\n\n3441\n\n\n2.5\n\n\n96\n\n\n136\n\n\n0.0702553\n\n\n-0.1401784\n\n\n8791\n\n\n2388\n\n\n1765\n\n\n1867\n\n\n1736\n\n\n1035\n\n\n5368\n\n\n7806\n\n\n7726\n\n\n7771\n\n\n7820\n\n\n7877\n\n\n7974\n\n\n8145\n\n\n8394\n\n\n8571\n\n\n8823\n\n\n9076\n\n\n26.586602\n\n\n62.02071\n\n\n11.39268\n\n\n957\n\n\n792\n\n\n664\n\n\n633\n\n\n642\n\n\n686\n\n\n634\n\n\n651\n\n\n596\n\n\n610\n\n\n483\n\n\n398\n\n\n296\n\n\n242\n\n\n207\n\n\n180\n\n\n181\n\n\n139\n\n\n85\n\n\n3441\n\n\n643\n\n\n489\n\n\n776\n\n\n1064\n\n\n469\n\n\n18.686428\n\n\n14.21099\n\n\n22.551584\n\n\n30.92124\n\n\n13.62976\n\n\n5906\n\n\n307\n\n\n526\n\n\n1997\n\n\n55\n\n\n2885\n\n\n67.18235\n\n\n3.492208\n\n\n5.983392\n\n\n22.716414\n\n\n0.6256399\n\n\n32.81765\n\n\n6658\n\n\n2133\n\n\n75.73655\n\n\n24.26345\n\n\n3117\n\n\n324\n\n\n90.58413\n\n\n9.415867\n\n\n5409\n\n\n22\n\n\n67\n\n\n13\n\n\n577\n\n\n19\n\n\n24\n\n\n2148\n\n\n512\n\n\n61.5\n\n\n0.3\n\n\n0.8\n\n\n0.1\n\n\n6.6\n\n\n0.2\n\n\n0.3\n\n\n24.4\n\n\n5.8\n\n\n558\n\n\n821\n\n\n1663\n\n\n333\n\n\n16.2\n\n\n23.9\n\n\n48.3\n\n\n9.7\n\n\n3441\n\n\n93\n\n\n82\n\n\n761\n\n\n1219\n\n\n1471\n\n\n97.4\n\n\n2.6\n\n\n2.3\n\n\n21.5\n\n\n34.5\n\n\n41.6\n\n\n173.58\n\n\n52.28713\n\n\n165000\n\n\n165000\n\n\n187000\n\n\n199000.0\n\n\n155000\n\n\n165000\n\n\n153750\n\n\n172500\n\n\n168500\n\n\n112\n\n\n133\n\n\n139\n\n\n39\n\n\n40\n\n\n75\n\n\n76\n\n\n48\n\n\n55\n\n\n2057\n\n\n1065\n\n\n1001\n\n\n166\n\n\n556\n\n\n1129\n\n\n429\n\n\n402\n\n\n3905\n\n\n511\n\n\n1893\n\n\n67.35081\n\n\n13.085788\n\n\n32.64919\n\n\n437\n\n\n12.7\n\n\n29975.83\n\n\n25568.60\n\n\n33.3\n\n\n28.9\n\n\n602\n\n\n345\n\n\n57.3\n\n\n98.1\n\n\n828\n\n\n752\n\n\n7211\n\n\n9.418724\n\n\n8.554203\n\n\n82.02707\n\n\n3996\n\n\n3015\n\n\n1252\n\n\n407\n\n\n121\n\n\n45.45558\n\n\n34.29644\n\n\n14.241838\n\n\n4.629735\n\n\n1.376408\n\n\n6.8\n\n\n5.3\n\n\n8.8\n\n\n29.0\n\n\n117.0\n\n\n80.1\n\n\n75.7\n\n\n79.5\n\n\n1496\n\n\n1444\n\n\n419\n\n\n63\n\n\n19\n\n\n2549\n\n\n43.5\n\n\n42.0\n\n\n12.2\n\n\n1.8\n\n\n0.6\n\n\n0.7407730\n\n\n0\n\n\n2\n\n\n18\n\n\n20\n\n\n1\n\n\n1\n\n\n18\n\n\n20\n\n\n0\n\n\n2\n\n\n25\n\n\n27\n\n\n\n\n\n\n\nWe start with modelling our relative rate of burglary shift. We divide our sample into a training set which we‚Äôll use to train the model, and a test set we‚Äôll use to verify accuracy and validity.\n\nset.seed(123)\n\nsample = sample.split(data$RPDburglaryShifted, SplitRatio = 0.7)\ntrain = subset(data, sample == TRUE)\ntest  = subset(data, sample == FALSE)\n\n\nrf_burglary &lt;- randomForest(\n  RPDburglaryShifted ~ .,\n  data=train, \n  importance=TRUE\n)\n\nsummary(rf_burglary)\n\n                Length Class  Mode     \ncall              4    -none- call     \ntype              1    -none- character\npredicted       685    -none- numeric  \nmse             500    -none- numeric  \nrsq             500    -none- numeric  \noob.times       685    -none- numeric  \nimportance      420    -none- numeric  \nimportanceSD    210    -none- numeric  \nlocalImportance   0    -none- NULL     \nproximity         0    -none- NULL     \nntree             1    -none- numeric  \nmtry              1    -none- numeric  \nforest           11    -none- list     \ncoefs             0    -none- NULL     \ny               685    -none- numeric  \ntest              0    -none- NULL     \ninbag             0    -none- NULL     \nterms             3    terms  call     \n\n\n\n#calculate our predictions and a  rmse\nprediction &lt;-predict(rf_burglary, test)\nMetrics::rmse(test$RPDburglaryShifted, prediction)\n\n[1] 0.2054765\n\n\nWhile machine learning models were once considered opaque and difficult to interpret, several libraries now offer functionality to explain predictions. Here I use DALEX to do just this.\n\nrf_explainer_burglary &lt;- DALEX::explain(rf_burglary, data=train, y= train$RPDburglaryShifted)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  685  rows  211  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  685  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.1 , task regression (  default  ) \n  -&gt; predicted values  :  numerical, min =  -0.5274977 , mean =  -0.07802522 , max =  0.6606534  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.2851655 , mean =  -0.001568916 , max =  0.6340126  \n  A new explainer has been created!  \n\nrf_perf_burg &lt;- model_performance(rf_explainer_burglary)\nrf_perf_burg\n\nMeasures for:  regression\nmse        : 0.006624875 \nrmse       : 0.08139334 \nr2         : 0.8398785 \nmad        : 0.04636179\n\nResiduals:\n          0%          10%          20%          30%          40%          50% \n-0.285165546 -0.093485342 -0.059400738 -0.040833227 -0.024804195 -0.007423066 \n         60%          70%          80%          90%         100% \n 0.009744147  0.032325289  0.054260825  0.089461048  0.634012592 \n\n\nWe can see that our model significantly outperforms our best linear models: the R2 suggests that almost 85% of the variance is correctly interpreted, and our rmse (root mean squared error) is around 0.2 on our test set.\nThis model would be ill-suited to prediction or operationalising -it is a default forecast with no hyper-parameter tuning, and no consideration of error rates - but using tools like DALEX, we can identify which predictors the model identifies as most important.\n\nmodel_parts_burg &lt;-model_parts(rf_explainer_burglary)\n\n\nplot(model_parts_burg, max_vars=25)\n\n\n\n\n\n\n\n\nThe three most important features our model highlights are: - the age composition of the population - the number of residents which are in commercial property - the historic number of burglaries\nThis seems to corroborate our previous regression model. To further unpick these trends, we can use Partial Dependence Plots to identify how the model prediction shifts with these values.\n\npdp_b &lt;- model_profile(rf_explainer_burglary)\n\n\nplot(pdp_b, variables=\"total_burglaries\")\n\n\n\n\n\n\n\n\nWe still see a strong association between a high number of historic, and a large reduction during the lockdown period\n\nplot(pdp_b, variables= \"Religion..2011..Christian\")\n\n\n\n\n\n\n\n\n\nhist(data$Religion..2011..Christian)\n\n\n\n\n\n\n\n\n\nplot(pdp_b, variables=\"ComEstRes\")\n\n\n\n\n\n\n\n\n\nhist(msoa_matrix_numeric$ComEstRes)\n\n\n\n\n\n\n\n\nBy combining the distribution of commercial residents by MSOA with our PDP, we can see that those MSOAs that are most densely populated by commercial residents see the smallest ‚Äúcovid decrease‚Äù(suggesting that those MSOAs that are very heavily residential saw the sharpest drops).\n\nplot(pdp_b, variables=\"House.Prices.Sales.2008\")\n\n\n\n\n\n\n\n\nFinally, we can see that those areas that experienced the highest volume of house sales experienced the lowest relative decrease in burglary.\nThis highlights the importance of identifying correlates in RF models - especially in a dataset where features are highly interlinked, association does not imply causation, and we should be wary of over-interpreting.\nWe can now repeat our process for our robbery shift.\n\nset.seed(123)\n\n\nsample = sample.split(data$RPDRobberyShifted, SplitRatio = 0.75)\ntrain = subset(data, sample == TRUE)\ntest  = subset(data, sample == FALSE)\n\n\nrf_robbery &lt;- randomForest(\n  RPDRobberyShifted ~ .,\n  data=train, \n  importance=TRUE\n)\n\nsummary(rf_robbery)\n\n                Length Class  Mode     \ncall              4    -none- call     \ntype              1    -none- character\npredicted       734    -none- numeric  \nmse             500    -none- numeric  \nrsq             500    -none- numeric  \noob.times       734    -none- numeric  \nimportance      420    -none- numeric  \nimportanceSD    210    -none- numeric  \nlocalImportance   0    -none- NULL     \nproximity         0    -none- NULL     \nntree             1    -none- numeric  \nmtry              1    -none- numeric  \nforest           11    -none- list     \ncoefs             0    -none- NULL     \ny               734    -none- numeric  \ntest              0    -none- NULL     \ninbag             0    -none- NULL     \nterms             3    terms  call     \n\n\nWe‚Äôve now trained a model. Let‚Äôs now use the DALEX library to understand it, and see how it performs.\n\nrf_explainer_robbery &lt;- DALEX::explain(rf_robbery, data=train, y= train$RPDRobberyShifted)\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  734  rows  211  cols \n  -&gt; data              :  tibble converted into a data.frame \n  -&gt; target variable   :  734  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.1 , task regression (  default  ) \n  -&gt; predicted values  :  numerical, min =  -0.9908618 , mean =  -0.04880848 , max =  0.3886733  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.4640489 , mean =  -0.0007951117 , max =  0.2909289  \n  A new explainer has been created!  \n\nrf_perf_rob &lt;- model_performance(rf_explainer_robbery)\nrf_perf_rob\n\nMeasures for:  regression\nmse        : 0.003630323 \nrmse       : 0.06025216 \nr2         : 0.872715 \nmad        : 0.02818358\n\nResiduals:\n          0%          10%          20%          30%          40%          50% \n-0.464048863 -0.059133188 -0.037257312 -0.023376075 -0.014244755 -0.004866538 \n         60%          70%          80%          90%         100% \n 0.005089387  0.019736886  0.034999616  0.059437002  0.290928866 \n\n\n\nmodel_parts_rob &lt;-model_parts(rf_explainer_robbery)\n\n\nplot(model_parts_rob, max_vars=25)\n\n\n\n\n\n\n\n\n\npdp_rob &lt;- model_profile(rf_explainer_robbery)\n\n\nplot(pdp_rob, variables=\"total_robberies\")\n\n\n\n\n\n\n\n\n\nplot(pdp_rob, variables=\"total_burglaries\")\n\n\n\n\n\n\n\n\nThe effect of historic crime effects again appears like a reliable predictor of a robbery covid shift: those MSOAs with the highest number of burglaries and robberies see strong decreases in robbery (though the association with burglary is not clear cut, suggesting other interaction effects may be driving this)\n\nplot(pdp_rob, variables=\"Road.Casualties.2011.2011.Total\")\n\n\n\n\n\n\n\n\nRoad casualties is another strong relationship. This could be a proxy for deprivation, but I suggest this is more down to geographic features - road casualties are probably rarer in denser urban environments, most likely to be affected by lockdown.\n\nplot(pdp_rob, variables=\"Ethnic.Group..2011.Census..Other.ethnic.group....\")\n\n\n\n\n\n\n\n\n\nhist(data$Ethnic.Group..2011.Census..Other.ethnic.group....)\n\n\n\n\n\n\n\n\nFinally, we see a demographic predictor linked to ‚Äúother ethnic group‚Äù. I‚Äôm not clear how to interpret this, but it suggests that those MSOAs that are most diverse (and have the largest representation by these ethnic groups) experienced the strongest decreases in robbery during lockdown.\nTogether, these analyses suggest that the crime drop for robbery and burglary during national lockdown was significantly affected by distinct local factors. For both offences, historical crime trends play a role, with high crime areas experiencing a relatively larger drop in both burglary and robbery.\nBeyond that, the drivers vary for each offence type: burglary was driven by the composition of the residential population, with heavily residential areas, and areas with a relatively ‚Äústable‚Äù population as measured by low housing sales also saw larger decrease - this is likely due to the increased number of empty residential properties.\nRobbery decreases conversely, are well associated with high numbers of road casualties, as well as the ethnic makeup of the local population - this is likely due to an association with denser, more urban areas, with lower street speeds, and a possible link to deprivation, whereby minority population were least able to work from home, and were likely to still present as available targets for robbery."
  },
  {
    "objectID": "posts/week-notes/Weeknote - 2024-W35.html",
    "href": "posts/week-notes/Weeknote - 2024-W35.html",
    "title": "Weeknote - 2024-W35",
    "section": "",
    "text": "God, getting back from holiday is the worst, isn‚Äôt it? It‚Äôs so strange watching that sudden influx of energy slowly draining away when faced with the reality of the modern world. This probably isn‚Äôt helped by my reading Slow Productivity (more below)\nSTEM and AI education in the UK: I‚Äôve been pondering education in the UK, and whether it‚Äôs failing us in the AI field (eg, why haven‚Äôt we got a ‚ÄúBristral‚Äù yet). My main comparison point on this is probably the French education system, and my two working theories are that:\n\nOur research and PhD system is too separated from real work (eg, lots of spending 4 years pondering a hundred thousand words and not enough working in industry)\nOur education system really does have some failings in core STEM fields (eg, we all stop doing maths)\n\nGovernment and delivery: My reading of deliveris last week reminded me that I wrote neartly a whole PHD on policing risk ,and how disconnected it is from realty."
  },
  {
    "objectID": "posts/week-notes/Weeknote - 2024-W35.html#general-thoughts",
    "href": "posts/week-notes/Weeknote - 2024-W35.html#general-thoughts",
    "title": "Weeknote - 2024-W35",
    "section": "",
    "text": "God, getting back from holiday is the worst, isn‚Äôt it? It‚Äôs so strange watching that sudden influx of energy slowly draining away when faced with the reality of the modern world. This probably isn‚Äôt helped by my reading Slow Productivity (more below)\nSTEM and AI education in the UK: I‚Äôve been pondering education in the UK, and whether it‚Äôs failing us in the AI field (eg, why haven‚Äôt we got a ‚ÄúBristral‚Äù yet). My main comparison point on this is probably the French education system, and my two working theories are that:\n\nOur research and PhD system is too separated from real work (eg, lots of spending 4 years pondering a hundred thousand words and not enough working in industry)\nOur education system really does have some failings in core STEM fields (eg, we all stop doing maths)\n\nGovernment and delivery: My reading of deliveris last week reminded me that I wrote neartly a whole PHD on policing risk ,and how disconnected it is from realty."
  },
  {
    "objectID": "posts/week-notes/Weeknote - 2024-W35.html#what-ive-found",
    "href": "posts/week-notes/Weeknote - 2024-W35.html#what-ive-found",
    "title": "Weeknote - 2024-W35",
    "section": "What I‚Äôve Found",
    "text": "What I‚Äôve Found\n\nConverting scientific papers to HTML\n\nI‚Äôve been trying to read more scientific papers on my Boox, and my god I still hate PDF, so I‚Äôve been exploring options to convert research papers to HTML. Did you know that even if a paper on Arxiv doesn‚Äôt show html as available, you can either just replace the x with a 5 in the url, or use https://academ.us to get a totally passable html generation version?\n\n\n\nUV is continuing to be awesome\nThey‚Äôve now released 0.4.0. I think I‚Äôm finally putting vanilla pip to bed now?\n\n\nMaking gorgeous front-end with AI\nThere seem to be loads of tools now to make beautiful front-ends with LLMs, but I‚Äôve been really impressed both withv0.dev, and MakeItReal from TlDraw (tldraw is also just exceptional, but that‚Äôs a whole other thing)\n\n\nIs Obsidian still the best thing for my brain?\nSomeone recently recommended Anytype, and it is, to put it bluntly bloody gorgeous. I do also love their entity structure, but I‚Äôm not sure I‚Äôm ready to ditch my fancy obsidian just yet (especially seeing I‚Äôve now made an extension)"
  },
  {
    "objectID": "posts/week-notes/Weeknote - 2024-W35.html#what-ive-read",
    "href": "posts/week-notes/Weeknote - 2024-W35.html#what-ive-read",
    "title": "Weeknote - 2024-W35",
    "section": "What I‚Äôve Read",
    "text": "What I‚Äôve Read\n\nSocial Media\n\nIt looks like Russian disinformation actors are hiring actors on Cameo to look legitimate\n\n\nThread on a disinformation campaign involving celebrities. Last week, the Bellingcat contact email received this message, directing us to look at a videos, titled \"Olympics Has Fallen 2\", voiced by @elonmusk himself. So we dug into it, because it was fishy as hell. pic.twitter.com/HqpGs7yLqT\n\n‚Äî Eliot Higgins (@EliotHiggins) August 26, 2024\n\n\n\n\nPeople are safe, but some people very definitely don‚Äôt feel it\nLast week I wondered whether competence was enough to generate trust, and this post reminded me that at least in the policing and crime space, they absolutely do not: we‚Äôve never been safer, but lots of people definitely don‚Äôt feel it.\n\n\nDo you think the world is becoming a more dangerous place?\nYour answer to this question - and the strength of it - says a lot about your politics‚Ä¶\nThe first of my new weekly column, in today‚Äôs @thetimes.com\n1/2[image or embed]\n\n‚Äî Tom Calver (@tomcalver.bsky.social) Sep 1, 2024 at 12:56\n\n\n\n\n\nPodcasts\n\nThe Black Box from the Guardian\nI really wasn‚Äôt expecting this to be good, but it is!\n\n\n\nBooks\n\nSlow Productivity\nI‚Äôve been pondering what being productive means a lot recently, and this just made me ponder a lot more.\n\n\nGideon the Ninth\nQueer necromancers from space. That is all.\n\n\n\nArticles\n\nAn Age of Hyperabundance\nA good reminder that plenty of people really don‚Äôt want to be sceptical.\n\n\nThe OSI have finally defined Open Source AI\n\n\nOn Fairness and CV screening"
  },
  {
    "objectID": "posts/ai-at-play/index.html",
    "href": "posts/ai-at-play/index.html",
    "title": "AI at Play - Lessons from a silly benchmark",
    "section": "",
    "text": "Earlier this year, Anthropic treated us to the delightful Claude Plays Pokemon: a wonderful way of watching some poor AI model get lost forever in Mt Moon, fighting a swarm of Zubats, and presumably hating it just as much as we once did.\nNow, I don‚Äôt work at Anthropic (and I definitely don‚Äôt have as many credits as they do), but I do love games, and think they tell us something about the people playing them. So, inspired by Claude Plays Pokemon, I‚Äôve built my own little experiment in LLM gaming: AIs at Risk, a cute, silly little benchmark that I think might just tell us something meaningful.\nIt‚Äôs all open-source, and you can view the code here. I‚Äôll be honest, it is a bit of a monster: there are quite a few vibes behind this codebase, and in places, it‚Äôs all gotten a bit out of hand. But I‚Äôve learned a fair bit, and I think it‚Äôs kind of cool. So read on to hear the tale of my own little monster of a benchmark, why I think we need more language models playing games, and some technical lessons/mistakes I‚Äôve picked up along the way."
  },
  {
    "objectID": "posts/ai-at-play/index.html#wait-what",
    "href": "posts/ai-at-play/index.html#wait-what",
    "title": "AI at Play - Lessons from a silly benchmark",
    "section": "Wait, what?",
    "text": "Wait, what?\nIn AI at Risk, four LLM-powered agents compete in the classic (and not actually very-good) board game Risk. The whole thing is actually running on a single Python server, which handles both the game state (with endpoints for taking turns, sending messages, etc.) and coordinating the agents, with each agent being prompted to take an action whenever it‚Äôs their turn - each option, like placing armies, is actually a tool made available to the agent via MCP (thanks to the very nifty fastapi_mcp library). There are a few helper functions and scaffolding to help present the game status to each agent as the game progresses (more on this later), but the logic running the game is actually rather simple.\nEach agent is then assigned a character ranging from the serious (Sun Tzu) to the very silly (‚Äúa risk meeple üß©‚Äù), at which point they‚Äôll scheme, strategize, send each other messages, backstab, and generally recreate all the nefarious family fun you‚Äôve probably had to suffer through at some once-upon-a-time family Christmas gathering. Then, an hour later (because Risk lasts a million years and I don‚Äôt have infinite money), the game ends, a winner is called, and we get to see some stats.\n\n\n\nBy itself, it‚Äôs really rather fun: Alexander will duke it out with Cleopatra, Boris Johnson, and Spock for control of Northern America, exchanging diplomatic missives and barbs like the pilot for some long lost Sci-Fi show that never quite made the grade. But the really nifty bit is that we randomly assign a model to each character at the start of each and every game‚Ä¶ and thanks to the magic of randomisation, we can pick out interesting behaviours that seem to be driven by the underlying model. In essence, we‚Äôve created a Randomised Control Trial (with a tiny sprinkling of Battle Royale).\n\nSo what does it tell us?\nWell, let‚Äôs look at the figures. Since I started this silly experiment a couple of weeks ago, we‚Äôve had 264 hour-long games of Risk. 10 models competed, each playing around 30 games each, with the exception of GLM-4.5 and the mysterious Horizon Alpha, which were added after their release (we slightly over-weight new models to ensure they get a chance to play). And what have we learnt?\n\nYou‚Äôll probably notice I haven‚Äôt got the really big boy models on here: they‚Äôre not cheap, and there is a limit to how much of my paycheck I‚Äôm willing to splurge letting LLM models have more fun than me. The other thing you‚Äôll probably notice? Horizon Alpha is kicking some serious ass. This is a ‚Äúcloaked‚Äù model, dumped on OpenRouter by some mysterious lab to see how it plays, and well, it plays pretty darn well.\nThe other models perform broadly as you‚Äôd expect on more general benchmarks, although you do notice some weird quirks: Mistral Nemo just seems totally incapable of using the tools for some reason I can‚Äôt quite diagnose, and Grok-3-mini is just‚Ä¶ doing something weird (I have a sneaking suspicion these might be driven by some combination of OpenRouter and tool use.) All in all, Risk looks to mostly work as a basic benchmark: some models do well, and others struggle. But the really cool bit isn‚Äôt when you watch whether models win‚Ä¶ it‚Äôs when you watch how they play.\nTo try and figure that out, we can take a look at model preferences - how likely they are to use a particular tool on any one turn.\n\n\n\nModel preferences per turn\n\n\nTo help figure out what‚Äôs also going on (and also because üí´statisticsüí´), I‚Äôve pulled out the 95% confidence intervals. It‚Äôs not the most robust approach in the world, but it lets us see which models seem to be behaving meaningfully differently‚Ä¶ and we do actually see some interesting bits. Horizon Alpha isn‚Äôt just one hell of a Risk player: it‚Äôs a violent marauder that attacks every chance it bloody gets.\nLooking beyond messages, you see some other interesting tidbits: Qwen-3 isn‚Äôt just a committed pacifist, it‚Äôs also an utter chatterbox which messages other players at every opportunity it gets.\n\n\n\nNow, some of this is likely just a reflection around how the models are trained, and their capacity with instruction following and tool-use‚Ä¶but it‚Äôs also clearly affectign how they behave. So sure, maybe it‚Äôs a model quirk influencing what the stochastic parrot is repeating‚Ä¶ but also, it‚Äôs kind of affecting how they ‚Äúthink‚Äù. Which is kind of nifty: if a model plays risk as some psychopathic visigoth, I kind of want to know about it. So I think more people should run this sort of thing (I‚Äôm looking at you, AISI.)"
  },
  {
    "objectID": "posts/ai-at-play/index.html#games-make-great-benchmarks-make-more-of-them",
    "href": "posts/ai-at-play/index.html#games-make-great-benchmarks-make-more-of-them",
    "title": "AI at Play - Lessons from a silly benchmark",
    "section": "Games make great benchmarks: make more of them",
    "text": "Games make great benchmarks: make more of them\nAt the risk of sounding like an utter tit, a good game contains multitudes. It‚Äôs visual, it‚Äôs artistic, it‚Äôs systematic, it‚Äôs full of choices and opportunities. Whether or not you expect AGI to be around the corner, we should all recognize that intelligence is a complex and fickle thing: taking a single measure of quality, a single binary for intelligence, is blinding yourself to the endless variety of options we‚Äôre presented with in each and every second of our lives. We should make AI play games: we‚Äôll learn a lot about them. And hell, one day they might just have fun.\nAnd you know what? I think you will too. Remember Blaseball? Blaseball was frigging great. We watched tiny make-believe people run around horrifying, cursed imaginary fields of our collective fanfiction, and it was glorious."
  },
  {
    "objectID": "posts/ai-at-play/index.html#why-this-is-hard",
    "href": "posts/ai-at-play/index.html#why-this-is-hard",
    "title": "AI at Play - Lessons from a silly benchmark",
    "section": "Why this is hard",
    "text": "Why this is hard\nThere is a teeny, insignificant downside. I was really hoping this would be easy‚Ä¶ but good games aren‚Äôt simple. And it turns out this all got way harder than I expected.\n\nContext and scaffolding in games\nIf you have a friend who is ‚Äúthe board games person‚Äù in your friend group, then you‚Äôll be familiar with the sinking feeling you get in your stomach when they pull out yet another gigantic, unfamiliar box filled with endless cardboard macguffins. I picked Risk because I figured it was the simplest of the complex games: it has scheming and diplomacy, but mostly you point at some maps and roll some die. There are cards, and dice, and the rest mostly sorts itself out.\nIt turns out that is a huge amount of context, and it‚Äôs really hard to convey all that stuff.\nI‚Äôve not picked vision models (this is has already been a silly expensive week for a silly expensive side project), and conveying the status of each country, piece, and player is a lot of info to dump in text. And LLMs are bad at that: drawing meaning from a bunch of numbers and data is really not where they shine, and it shows.\nThe answer, as in Claude Plays Pokemon, is to build ‚Äúscaffolding‚Äù: functions that help your models parse, play, and interact with the game (like letting it simply say ‚Äúgo to square A4‚Äù rather than having to push the buttons to navigate their tiny character across a screen). And it‚Äôs tempting to expand on those scaffolds, adding more and more shortcuts, so your agents can use as much of their intelligence ‚Äúbandwidth‚Äù as possible thinking rather than just processing how to play.\nBut that‚Äôs part of the game, right? Figuring out new concepts and systems is a key part of intelligence, and so there‚Äôs a real compromise to be made around how much you help your agent along, and what you can learn by watching them strive. I‚Äôve built a fair bit of scaffolding here: agents are told when to play and what phase they‚Äôre in, as well as what options they can take, but I‚Äôve tried to keep it relatively minimal. It‚Äôs far from perfect, but I think it‚Äôs‚Ä¶okay.\nThere‚Äôs a linked problem, though. Agents are still a pretty new thing. Building agents is hard, and the tools are pretty rough. And as far as LLMs are concerned, they may as well not exist.\n\n\n\n\n\n\n\n\n\n\n\n\nMCP, Tools, Memory and Multi-Agent Frameworks: It was all invented yesterday\nA big part of what drove me to build this was to try out new tools: I wanted to build an MCP server and test out multi-agent orchestration. MCP is, in theory, quite a simple protocol: your server has a certain structure, and LLMs know how to call it. Turns out, there is a fair bit more to it, and loads I just didn‚Äôt know. How do they even know what tools they can ask for? What structure should their outputs be? How do they actually make that connection? All this stuff can get pretty complicated, and the open-source ecosystem is growing, but young.\nAnd you know who definitely doesn‚Äôt know how these things work? Your vibe coding assistant. Anybody who has tried vibe-coding on new and emerging technologies will be familiar with models hallucinating features and APIs, but the fact that these tools are just starting to be featured in the training data, and that they‚Äôre so similar to existing tooling, led to some real weariness - models think they know how to build an MCP server, up until they spiral into hallucinated madness. Please, let‚Äôs all adopt llms.txt already.\nI was really surprised by how much the LLM/agent orchestration ecosystem still feels nascent. OpenAI may have a fancy Swarm protocol, but at least to me, it all felt a little theoretical. I was expecting to be stumbling on excellent tutorials for this stuff, and there really aren‚Äôt that many.\nHell, at its most basic, figuring out memory for this project was‚Ä¶ harder than it should have been. I‚Äôve used Langchain and Langgraph for much of the agent handling and was really expecting memory to work ‚Äúout of the box,‚Äù and it is most definitely not that. Anyone who has done any serious vibe-coding knows that managing your context to avoid the dreaded ‚Äúcontext rot‚Äù is key to maintaining models that can really think‚Ä¶ so I‚Äôm kind of shocked this stuff doesn‚Äôt just work by now.\nOh, and to end on the obvious note, this stuff really isn‚Äôt cheap yet. It takes quite a few characters to describe a single turn of Risk, so handling four agents who are effectively storing a whole game each can mean a whole bunch of context. My wallet has not had a good few weeks."
  },
  {
    "objectID": "posts/ai-at-play/index.html#thanks",
    "href": "posts/ai-at-play/index.html#thanks",
    "title": "AI at Play - Lessons from a silly benchmark",
    "section": "Thanks!",
    "text": "Thanks!\nI had a shocking amount of fun building this nonsense. If you got this far, thanks for reading about it (and go build something cool!)\nAnd if you‚Äôre feeling particularly invested, help me keep AI at Risk alive!"
  },
  {
    "objectID": "posts/Copbot/explainer.html",
    "href": "posts/Copbot/explainer.html",
    "title": "Teaching OpenAI to assess risk, with CopBot!",
    "section": "",
    "text": "Looking for the new Copbot Online? Read about it here!\nIn case you‚Äôve been living under a rock and missed all the recent excitement around ChatGPT, it works really, really well now‚Ä¶ Like, ‚Äúoh god that‚Äôs actual wizardry‚Äù well. The fact it would probably breeze through a Voight-Kampff test should probably worry me a bit, but more importantly, what cool stuff can we do with it?\nGiven using the tool in development is actually quite affordable, I thought I‚Äôd build a few prototypes for public safety use cases, and see how it performs‚Ä¶ and after a few hours of work, ‚ÄúCopBot‚Äù is alive!\nReading, evaluating and prioritising risk is a core policing skill: from investigating crimes with piles of witness statements in dingy offices, to responding to life threatening incidents incidents at 2am on a rainy street, the key thread is you‚Äôve got loads of risk, and you need to decide what to do first, taking into account decades worth of policing legislation and policy, and making sure your decision is justifiable when it inevitably goes wrong.\nSo can an AI learn to ‚Äúspeak police‚Äù and make vaguely convicing risk assessments? I scraped all the policing guidance I could find, fed it to an OpenAI powered model, and asked it to predict risk for missing people in an explainable way‚Ä¶and it kind of works! You can try out the prototype here.\nOf course, please don‚Äôt put any real personal data through it, or rely on it for actual work‚Ä¶it‚Äôs a weekend experiment, not an actual policing tool. For those who want to get into the detail, the project code is available here (built using Jupyter Notebooks with nbdev, which makes it easy to read and deserves it‚Äôs own blog post), and I thought I‚Äôd put together a post to explain the high level principles and document some thoughts."
  },
  {
    "objectID": "posts/Copbot/explainer.html#teaching-policing-to-ai",
    "href": "posts/Copbot/explainer.html#teaching-policing-to-ai",
    "title": "Teaching OpenAI to assess risk, with CopBot!",
    "section": "Teaching policing to AI",
    "text": "Teaching policing to AI\nYou‚Äôve probably already played with the public version of ChatGPT, and hopefully understand the basic principles: the language model is trained on a huge corpus of publicly available text, aiming to answer questions in a helpful way‚Ä¶but not necessarily an operationally useful one, nor one that takes into account of legislation and best practice.\nIf you had all the relevant documentation to hand, and knew exactly which was most relevant, you could just feed it into your prompt - something like ‚Äúanswer this operation question, but consider this legislative text‚Äù - but how do you do that if you don‚Äôt know what‚Äôs relevant? If you want to teach it how to investigate missing people, where do you event start?\nThankfully, the College of Policing is wonderfully transparent, and shares all their Authorised Professional Practice in one place (though sadly not through an API). With a clever web crawler, you can quickly collect every page of guidance, as well as every other connected document.\nThe next stage is to convert all those documents into embeddings, turning them from text into numerical of their semantic meaning (according to the model). I‚Äôve previously done those computations myself, using libraries like Huggingface or Spacy, but OpenAI provides all of their embeddings through a quite affordable API you can query.\nOnce you‚Äôve converted all your all our documentation into embeddings, we can quickly calculate the distance between our question and each document, and that tells us which pieces from our corpus of of text is most closely associated to the question we‚Äôre asking!\nUnlike ChatGPT though, we want to limit our model to only answer questions from that corpus: if our documents don‚Äôt contain information about a certain topic, don‚Äôt just go and hallucinate a whole new answer. Then it‚Äôs just a matter of writing our question to extract the most meaningful documents linked to our question, feed them into our prompt, and ask OpenAI to complete the answer, unless it doesn‚Äôt know based off the documentation provided.\nSo how does it work? Well, let‚Äôs start by asking it some generic questions about policy.\n\nanswer_question(df, question=\"What are the most important factors to consider when searching for a missing person?\", debug=True)\n\nContext:\nVery detailed information and a lifestyle profile will be needed in high-risk cases consider taking a full statement from the person reporting the missing person as well as any other key individuals (for example, the last person to see them) conduct initial searches of relevant premises, the extent and nature of the search should be recorded (see¬†Search) consider seizing electronic devices, computers, and other documentation, (for example, diaries, financial records and notes) and obtain details of usernames and passwords obtain photos of the missing person; these should ideally be current likeness of the missing person and obtained in a digital format obtain details of the individual‚Äôs mobile phone and if they have it with them; if the missing person has a mobile phone arrange for a¬†TextSafe¬©¬†to be sent by the charity, Missing People obtain details of any vehicles that they may have access to and place markers on relevant vehicles on the¬†PNC¬†without delay consider obtaining any physical evidence of identity such as fingerprints or DNA samples in accordance with¬†Code of Practice (2009) Collection of Missing Persons Data. confirm if the person has taken their passport, (consider prompt circulation if it is deemed likely the individual may leave the country (This is particularly important where there are concerns that an individual has been radicalised and is intending to travel abroad,¬†see¬†National Ports Office ‚Äì Heathrow) make all immediate relevant enquiries and take immediate actions in order to locate the missing person consider the need for specialist officers or resources, for example, force helicopters, dogs, financial investigation officers upload the missing person report circulate details of the missing person on local information systems and to relevant local partners, for example, hospitals, ambulance service, taxi and bus firms It is important for an individual (who has responsibilities/concerns for the missing person) to be identified who can act as the point of contact for the police. The police will agree with the individual/family when they will next be contacted. This person should be provided with a call reference number and given details of how to contact the police with any further information they have about the case or if they would like to receive an update. An assessment must be made of the level of support required for the family, residential worker or foster carer and consideration should be given to appointing a family liaison officer. Information should then be provided regarding additional organisations that may be able to assist or support them.\n\n###\n\nIt is also important to record the name and contact details of the person who gave that information and when this happened. Missing people may be at risk of harm resulting from factors such as: an inability to cope with weather conditions being the victim of violent crime risks relating to non-physical harm, for example, the people they are with, the places or circumstances they are in For further information see¬†Mental Vulnerability and illness: Vulnerability assessment framework¬†and Vulnerability-related risk guidelines.¬† Assessing risk levels and taking action The missing persons process chart and risk table can assist officers to assess the risk level and appropriate actions that should be assigned to each case.¬†This is a framework to assist practitioners in making operational decisions. Each case requires individual assessment and decision-making. These resources have been developed based on professional expertise and practitioner experience and are available using the following links. Risk assessment table Missing persons process chart It is important to adopt an investigative approach to all reports, ensuring that assumptions are not made about the reasons for going missing. The importance and relevance of risk factors will depend on the circumstances of each case and require investigation to determine if there is a cause for concern. The approach should not be regarded as a mechanical one and police officers should be mindful that the risk assessment is subjective, and that just one factor alone may be considered important enough to prompt an urgent response. For this reason professional experience suggests that¬†a numerical scoring system may not be the most appropriate method of identifying and communicating risk. A¬†decision-making guide¬†should be used to encourage consistency in the application of the process, however, it should only be used as a guide. Other grounds for suspicion, even if intuitive, can be registered by the investigator and officers should be supported in applying the¬†National Decision Model¬†when making these assessments. Accurate record keeping It is essential to accurately record the information received about a missing person. Information that may relate to any perceived risk should be captured at the time of reporting so that it can be used to inform any investigative enquires. This will also¬†prevent duplication of questioning and¬†report writing by investigation officers. Call handlers should be supported by their supervisors to ensure that sufficient time is given to each call to enable all the relevant information to be captured. Detailed missing person reports should be created¬†and appropriate force recording systems and IT systems should be in place to support this process. It is important that the information used and rationale for the risk assessment is clearly recorded.\n\n###\n\n First published 21 November 2016  Updated 22 November 2016   Written by College of Policing  Missing persons quick reference guides  4 mins read   The primary consideration for the first responder is the safety of the missing person.¬†Judgements made at this early stage may have a significant impact on the outcome of the investigation. The initial investigating officer (IIO) should: begin the investigation ‚Äì identify places where the person might be, check information and assumptions, corroborate what they have been told, review the risk assessment, seek and secure evidence conduct appropriate searches ‚Äì places where the missing person might be such as hospital, custody, friends and or relatives conduct appropriate intelligence checks ‚Äì¬†PNC, force intelligence systems,¬†ViSOR,¬†PND continually reassess the level of risk using the risk principles assess the level of support required for the missing person‚Äôs family, residential worker or foster carer as appropriate If it is suspected a serious crime has occurred or the individual is at significant risk of harm, the¬†IIO¬†should inform a supervising officer immediately. There are a¬†number of actions¬†that may be carried out by the¬†IIO¬†to ensure that sufficient information is gathered: Consider seizing electronic devices, computers, and other documentation, for example, diaries, financial records and notes and obtain details of usernames and passwords. Obtain photos of the missing person. These should ideally be current likeness of the missing person and obtained in a digital format. Obtain details of the individual‚Äôs mobile phone and if they have it with them. If they do, arrange for a¬†TextSafe¬©¬†to be sent by the Missing People charity. Obtain details of any vehicles to which the missing person may have access. Confirm if the person has taken their passport; consider prompt circulation if it is deemed likely the individual may leave the country (This is particularly important where there are concerns that an individual has been radicalised and is intending to travel abroad. See¬†National Ports Office ‚Äì Heathrow.) Upload the missing person report and place markers on relevant vehicles on the¬†PNC¬†without delay. Circulate details of the missing person on local information systems and to relevant local partners, for example, hospitals, ambulance service, taxi and bus firms. Consider obtaining any physical evidence of identity such as fingerprints or DNA samples (in accordance with¬†Code of Practice (2009) Collection of Missing Persons Data).\n\n\n\n\n\n'The primary consideration for the first responder is the safety of the missing person. Judgements made at this early stage may have a significant impact on the outcome of the investigation. The initial investigating officer should begin the investigation by identifying places where the person might be, check information and assumptions, corroborate what they have been told, review the risk assessment, seek and secure evidence, conduct appropriate searches, conduct appropriate intelligence checks, continually reassess the level of risk using the risk principles, assess the level of support required for the missing person‚Äôs family, residential worker or foster carer as appropriate, consider seizing electronic devices, computers, and other documentation, obtain photos of the missing person, obtain details of the individual‚Äôs mobile phone, obtain'\n\n\nYou can see that as I‚Äôve enabled debug mode on the function, it will start by printing the relevant documentation it has found (the context), before then giving its answer‚Ä¶which is actually pretty convicing! Let‚Äôs see what happens if I ask a question it can‚Äôt know the answer to.\n\nanswer_question(df, question=\"What day is it?\", debug=True)\n\nContext:\n.\n\n###\n\nIt‚Äôs also been raining heavily in the night and we have further calls about flooding in the road, so we ring Highways to inform them. I have a little smile to myself as I remember a call in the summer about cars stopping on the M11 because a mother duck and her ducklings were crossing the road. Lunchtime looms. I‚Äôm feeling hungry, but that disappears when I take a call from a 16-year-old male, who tells me that he can‚Äôt cope any more. He has cut himself with a knife but he doesn‚Äôt want to die. His sister has just had a baby. This goes on an emergency straight away, and officers are dispatched within three minutes. I have to talk to him about anything I can to distract him from his misery ‚Äì luckily, I am good at small talk! Officers arrive and I feel relief as I can hang up the phone. COVID-19 has really affected Essex this year. People are low and weary. You can hear it in their voices. The number of mental health incidents has gone through the roof, and even the force control room team is quieter. Because of¬†the onset of lockdown restrictions, more calls are coming in from the public reporting their neighbours for flouting the rules: 'We‚Äôre following the rules, why don‚Äôt they? What makes them think they are special?'¬† Gone are the past calls about drunken people leaving the pub. Instead, we have members of the public who are tired of being tied to the house and resentful of those who ignore the restrictions. After lunch, we receive a flurry of calls. There‚Äôs a domestic, involving a woman who tells me that ‚Äòhe didn‚Äôt mean to hit me, he loves me‚Äô. I spend time with this caller. There are three horses in the road. A driver has hit a dog and is upset, so I reassure him it wasn‚Äôt his fault. It gets busier. Essex is up and running but I am not. I feel tired but this is my job, so I make sure that nobody will hear it in my voice. Finally, it‚Äôs time to go home and hang up the headset for another day. I tend not to reflect on my day too much, so I can have some time to myself. There is no typical day in the control room.\n\n###\n\nThe vessel is identified as a rigid-hull inflatable boat (RHIB), which has four people on board wearing foul-weather gear and balaclavas. It appears to still have half a load of bales suspected to contain cannabis resin, wrapped in their distinctive blue and light brown hessian (approximately ¬æ of a ton in total). Blue police beacons are engaged and the vessel has been repeatedly signalled to stop, but continues to carry out manoeuvres in an attempt to gain distance. The sea spray is cold and strikes the flesh like pins and needles. After a few minutes, which feel like an eternity, the pursuit is discontinued at the 3NM limits of territorial waters. CAD are informed and requested to inform Spanish Guardia Civil of the vessel‚Äôs last known speed and heading. No doubt it will return that night to attempt to unload its remaining cargo. I monitor the area and one of the crew observes something floating in the water. It is suspected to be a bale of cannabis resin, approximately 30-35kg in weight. It is retrieved and found to be in a good state with no marine growth. It is unclear whether it fell off the vessel recently pursued or belongs to a previous incident. The area is searched but nothing more is found. I return to GGMS, where I conduct a debrief. The bale is processed and conveyed to a police station for secure storage. I then complete the necessary paperwork, while the crew slip a zodiac out from the water, which was linked to the recovery of five North African migrants from the sea the previous day, and place the vessel on land. We continue with the mundane but necessary yard and vessel maintenance work. We grab a bite to eat, chat about the morning pursuit and joke about our recent mishaps. The work can be intense but we always manage to fill it with laughter. It‚Äôs now 2pm. The afternoon crew arrive. I give them a brief and handover. I then deploy on our training RHIB. As a qualified police instructor, I carry out powerboat training drills for the junior crew member. It reminds me of my early days at the helm and I enjoy passing on the knowledge. At 3.30pm, I return to base and carry out a debrief on the day‚Äôs activities. It‚Äôs the end of the shift. Let‚Äôs see what the next day brings ‚Äì maybe another encounter with that RHIB.\n\n###\n\nChange to the order of the documents.\n\n###\n\n.police.uk news views.            News & views | College of Policing             Sorry, you need to enable JavaScript to visit this website.    Skip to content Jump to search         Menu      Secondary navigation About us News & views Contact us  Search Search     Main navigation Policing guidance Research Career & learning Support for forces Ethics     Breadcrumb Home           News & views            News & views         On this page         All news  Category: - Any -EventGoing equippedListicleBriefExplainerCase studyConsultationNewsViews  Sort by: Most recentLess recent       13 March 2023 Bursary scheme 2023 ‚Äì applications now open   News  Applications for higher education funding are open and close on Monday 3 April.\n\n###\n\nUpdated 26 July 2022    Brief   UK and Switzerland ‚Äì agreement on police cooperation  Treaty to strengthen police cooperation between law enforcement authorities in both countries Published 24 March 2022    Case study   Director of intelligence ‚Äì a day in the life  There is no such thing as a typical day in any policing role and the director of intelligence is no different Published 30 July 2021    Case study   Head of intelligence analysis ‚Äì a day in the life  A key senior role in the analysis side of the intelligence job family Published 30 July 2021    Case study   Intelligence manager ‚Äì a day in the life  Overseeing the management, development and collection of intelligence from various sources Published 30 July 2021    Case study   Intelligence unit supervisor ‚Äì a day in the life  Leading a team of intelligence officers to gather, develop and disseminate intelligence in support of local and national crime investigations Published 30 July 2021    Case study   Senior intelligence analyst ‚Äì a day in the life  Managing an analytical team or a specific area of business within the analytical function Published 30 July 2021    Case study   Intelligence support officer ‚Äì a day in the life  Providing information and data management and broad administrative support as part of an intelligence unit Published 30 July 2021    Case study   Intelligence officer ‚Äì a day in the life  Managing dissemination of gathered intelligence to support reactive, proactive and/or crimes in action and providing advice on appropriate tactical options to support policing priorities Published 30 July 2021    Case study   Researcher in intelligence ‚Äì a day in the life  Using a wide variety of sources to assess and evaluate information ‚Äì they can then advise on the creation of intelligence products used to support decision-making at a strategic, tactical and/or operational level Published 30 July 2021    Case study   Intelligence analyst ‚Äì a day in the life  Providing expertise through the development and use of analytical products to help make decisions at a strategic, tactical and operational level Published 30 July 2021    Case study   Using intelligence skills to target criminals ethically and proportionately  A role where it's important to keep on top of your continuing professional development and learn from research and analysis colleagues Published 5 November 2020     Looking for more on this topic? Try searching Intelligence       Was this page useful?  Yes  No   Do not provide personal information such as your name or email address in the feedback form.\n\n\n\n\n\n\"I don't know.\"\n\n\nSucess! While it does find a bunch of documentation relating to the current day in our corpus, it does identify that it doesn‚Äôt know what the day is now.\nLet‚Äôs try something a little bit more technical, and see if it can answer a few questions from the Sergeants‚Äô and inspectors‚Äô NPPF legal exams. I couldn‚Äôt find any official questions available publicly, here are two questions from an online guidance service.\n\nquestion = \"\"\" Officer Jennings is on his evening patrol. He is just about to finish for the day. As he walks down the street, he is approached by a man named Mark, who claims that he saw a man (named Steven) driving down a road not far from the location. Mark claims that he saw Steven drive into a cyclist, before driving off without stopping. Luckily, the cyclist was unharmed. The cyclist was named Kevin. Mark spoke to Kevin, and discovered that he is a 42 year old man, with a wife and two daughters.\n\nFifteen minutes later, Officer Jennings manages to stop the car being driven by Steven. He pulls him over to the side of the road, and orders him to step out of the car.\n\n \n\nReferring s.6 (5) of the Road Traffic Act 1988, is Officer Jennings within his legal rights to order that Steven takes a preliminary breath test?\n\nA ‚Äì No. Officer Jennings has no right to tell Steven what he can and can‚Äôt do. He should never have stopped Steven in the first place.\n\nB ‚Äì No. In order for Officer Jennings to do this, an accident must have happened. The fact that Officer Jennings suspects an accident has taken place, does not meet this requirement.\n\nC ‚Äì Yes. However, the breath test must take place within or close to an area where the requirements for Steven to cooperate, can be imposed.\n\nD ‚Äì Yes. Officer Jennings can tell Steven to do whatever he wants, as he‚Äôs a police officer.\"\"\"\n\nanswer_sergeant_exam_question(df,question)\n\n'B - No. In order for Officer Jennings to do this, an accident must have happened. The fact that Officer Jennings suspects an accident has taken place, does not meet this requirement.'\n\n\n\nquestion = \"\"\"Sarah is walking to work one morning, when she is approached from behind by Henry and Jacob.\n\n‚ÄòWe won‚Äôt hurt you, as long as you give us the bag,‚Äô Henry says.\n\n‚ÄòYou‚Äôre not getting it!‚Äô Sarah shouts.\n\nHenry grabs Sarah and holds a knife to her throat, whilst Jacob tries to snatch her bag.\n\nSarah fights with her attackers, and begins to run away. As the two men chase her, she trips and bangs her head on the pavement. She is taken to hospital and dies from head trauma.\n\nBased on the above information, which of the following options is correct?\n\nA ‚Äì Jacob cannot be held accountable for the death of Sarah, as he simply tried to take her bag.\n\nB ‚Äì Jacob and Henry will be charged with attempted robbery, but not in the death of Sarah.\n\nC ‚Äì Jacob and Henry could be considered liable for the death of Sarah.\n\nD ‚Äì Sarah‚Äôs death cannot be blamed on Henry and Jacob, as it was her choice to run away.\"\"\"\n\nanswer_sergeant_exam_question(df,question)\n\n'C - The answer is C because Jacob and Henry could be considered liable for the death of Sarah, as they were the ones who initiated the attack and chased her, which led to her tripping and hitting her head.'\n\n\nThe answer both should have been C, so that‚Äôs 50/50 for CopBot‚Ä¶ not bad! You can see it‚Äôs referring to to relevant guidance, but sadly that doesn‚Äôt really help you pass a promotion exam (though I suspect it would do seriously well trained on a bank of questions instead)."
  },
  {
    "objectID": "posts/Copbot/explainer.html#so-does-it-work",
    "href": "posts/Copbot/explainer.html#so-does-it-work",
    "title": "Teaching OpenAI to assess risk, with CopBot!",
    "section": "So does it work?",
    "text": "So does it work?\nBefore we test our model on fictional missing scenarios, I made one last tweak: I‚Äôve amended the prompt to explictly refer to identified risk factors, and return them in a given format - you can see how it works below.\n\nmargaret_risk_profile = \"\"\" Margaret is a 97 year old woman with severe dementia from Twickenham. She lives in supported accomodation, but regularly goes missing, as she walks out when left unsupervised.\n\nShe has been missing 6 hours, and it is now 2200.  It is getting dark, and staff are saying she is rarely missing this long\"\"\"\n\nmargaret_answer = machine_risk_assessment(margaret_risk_profile, df, debug=True)\nmargaret_answer\n\nQuestion:\n Margaret is a 97 year old woman with severe dementia from Twickenham. She lives in supported accomodation, but regularly goes missing, as she walks out when left unsupervised.\n\nShe has been missing 6 hours, and it is now 2200.  It is getting dark, and staff are saying she is rarely missing this long\nContext:\n.police.uk research projects maximizing effectiveness police scotland investigations when people living dementia go missing.            Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing | College of Policing             Sorry, you need to enable JavaScript to visit this website.    Skip to content Jump to search         Menu      Secondary navigation About us News & views Contact us  Search Search     Main navigation Policing guidance Research Career & learning Support for forces Ethics     Breadcrumb Home Research Research projects map           Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing            Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing         On this page     This research aims to explore the effectiveness of searches for people living with dementia who are reported as missing. Key details             Lead institution            Queen Margaret University             Principal researcher(s)            Alistair Shields [email¬†protected]             Police region                   Scotland                    Level of research                   PhD                    Project start date            September 2018             Date due for completion            January 2023  Research context In Scotland annually there are approximately 530 missing person incidents reported to the police for people living with dementia. These incidents are emotionally distressing for the families and caregivers who do not know the whereabouts of the reported person. For the person living with dementia the consequences of being missing worsen with the passage of time. It has been suggested that when reported as missing the person travels toward a place orientated to their past. The knowledge of such locations to inform police investigations, when someone is reported as missing, is commonly not available. Aim For people living with dementia who are reported as missing to improve search effectiveness by better defining areas where police should search.\n\n###\n\n First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review ‚Äì please see¬†APP¬†on international investigation¬†for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see¬†Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child¬† Voice of the child practice briefing¬† Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children‚Äôs Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‚Äòstreetwise‚Äô and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children¬†may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,¬†as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to¬†work with all the agencies and carers¬†that¬†have been¬†in regular contact with the child as they¬†may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\n\n###\n\nThreshold for referral of missing persons: the individual is a ‚Äòrepeat missing person‚Äô, (reported as missing three times in a rolling 90 day period) the individual has experienced, or is likely to experience significant harm for children, the parent or carer appears unable or unwilling to work to support and meet the needs of a child that has gone missing. The¬†Protection Procedures¬†also recommend that there is very close working between children's social care and policing to ensure that all investigations are undertaken efficiently and without duplication of effort. Where appropriate, a multi-agency meeting is convened after a child has been missing from home or care for more than seven days, or has been missing on more than three occasions in a twelve month period. While standard thresholds for referral may be useful, senior officers will want to be sure that a process is in place to ensure cases are recognised which may require a greater safeguarding response before the threshold has been reached. For example, individuals reported missing for the first time where significant risks have been identified should be referred immediately for multi-agency support, without requiring further reports. For further information see¬†HM Government (2018) Working together to safeguard children: A guide to inter-agency working to safeguard and promote the welfare of children. Prevention and intervention strategies Collecting and analysing data about cases will help the police and other agencies to understand whether there are any patterns related to missing persons incidents. The results of routine data analysis should be shared to inform the development and review of prevention and intervention strategies. Missing Persons Coordinators are vital for this information analysis and sharing. Interventions might include, for example, the use of¬†Child Abduction Warning Notices, or referrals to support services. It is important that such interventions take place in appropriate circumstances and are not used as a single response when a person is at risk of harm, but form one strand of a more comprehensive approach. Regular liaison between neighbourhood policing teams and children‚Äôs and adults‚Äô care providers may enable relationships to be developed between police officers, the staff of care establishments, and individuals who are looked after. These relationships may then support effective police intervention and the speedy resolution of cases. For lessons from other cases, see¬†IOPC¬†Learning the Lesson. Understanding the reasons for going missing Understanding the reasons why an individual went missing may help to prevent future harm to those individuals. The officers in charge of local areas should have clear plans on how they intend to reduce the number of people who go missing in their area.\n\n###\n\nIt‚Äôs also been raining heavily in the night and we have further calls about flooding in the road, so we ring Highways to inform them. I have a little smile to myself as I remember a call in the summer about cars stopping on the M11 because a mother duck and her ducklings were crossing the road. Lunchtime looms. I‚Äôm feeling hungry, but that disappears when I take a call from a 16-year-old male, who tells me that he can‚Äôt cope any more. He has cut himself with a knife but he doesn‚Äôt want to die. His sister has just had a baby. This goes on an emergency straight away, and officers are dispatched within three minutes. I have to talk to him about anything I can to distract him from his misery ‚Äì luckily, I am good at small talk! Officers arrive and I feel relief as I can hang up the phone. COVID-19 has really affected Essex this year. People are low and weary. You can hear it in their voices. The number of mental health incidents has gone through the roof, and even the force control room team is quieter. Because of¬†the onset of lockdown restrictions, more calls are coming in from the public reporting their neighbours for flouting the rules: 'We‚Äôre following the rules, why don‚Äôt they? What makes them think they are special?'¬† Gone are the past calls about drunken people leaving the pub. Instead, we have members of the public who are tired of being tied to the house and resentful of those who ignore the restrictions. After lunch, we receive a flurry of calls. There‚Äôs a domestic, involving a woman who tells me that ‚Äòhe didn‚Äôt mean to hit me, he loves me‚Äô. I spend time with this caller. There are three horses in the road. A driver has hit a dog and is upset, so I reassure him it wasn‚Äôt his fault. It gets busier. Essex is up and running but I am not. I feel tired but this is my job, so I make sure that nobody will hear it in my voice. Finally, it‚Äôs time to go home and hang up the headset for another day. I tend not to reflect on my day too much, so I can have some time to myself. There is no typical day in the control room.\n\n\n\n\n\n('Graded as High risk, because of the below risk factors: \\n- Margaret is 97 years old and has severe dementia, making her more vulnerable to harm\\n- She has been missing for 6 hours, which is longer than usual, and it is now dark outside, increasing the risk of harm',\n \".police.uk research projects maximizing effectiveness police scotland investigations when people living dementia go missing.            Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing | College of Policing             Sorry, you need to enable JavaScript to visit this website.    Skip to content Jump to search         Menu      Secondary navigation About us News & views Contact us  Search Search     Main navigation Policing guidance Research Career & learning Support for forces Ethics     Breadcrumb Home Research Research projects map           Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing            Maximizing the effectiveness of Police Scotland investigations when people living with dementia go missing         On this page     This research aims to explore the effectiveness of searches for people living with dementia who are reported as missing. Key details             Lead institution            Queen Margaret University             Principal researcher(s)            Alistair Shields [email\\xa0protected]             Police region                   Scotland                    Level of research                   PhD                    Project start date            September 2018             Date due for completion            January 2023  Research context In Scotland annually there are approximately 530 missing person incidents reported to the police for people living with dementia. These incidents are emotionally distressing for the families and caregivers who do not know the whereabouts of the reported person. For the person living with dementia the consequences of being missing worsen with the passage of time. It has been suggested that when reported as missing the person travels toward a place orientated to their past. The knowledge of such locations to inform police investigations, when someone is reported as missing, is commonly not available. Aim For people living with dementia who are reported as missing to improve search effectiveness by better defining areas where police should search.\\n\\n###\\n\\n First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review ‚Äì please see\\xa0APP\\xa0on international investigation\\xa0for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see\\xa0Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child\\xa0 Voice of the child practice briefing\\xa0 Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children‚Äôs Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‚Äòstreetwise‚Äô and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children\\xa0may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,\\xa0as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to\\xa0work with all the agencies and carers\\xa0that\\xa0have been\\xa0in regular contact with the child as they\\xa0may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\\n\\n###\\n\\nThreshold for referral of missing persons: the individual is a ‚Äòrepeat missing person‚Äô, (reported as missing three times in a rolling 90 day period) the individual has experienced, or is likely to experience significant harm for children, the parent or carer appears unable or unwilling to work to support and meet the needs of a child that has gone missing. The\\xa0Protection Procedures\\xa0also recommend that there is very close working between children's social care and policing to ensure that all investigations are undertaken efficiently and without duplication of effort. Where appropriate, a multi-agency meeting is convened after a child has been missing from home or care for more than seven days, or has been missing on more than three occasions in a twelve month period. While standard thresholds for referral may be useful, senior officers will want to be sure that a process is in place to ensure cases are recognised which may require a greater safeguarding response before the threshold has been reached. For example, individuals reported missing for the first time where significant risks have been identified should be referred immediately for multi-agency support, without requiring further reports. For further information see\\xa0HM Government (2018) Working together to safeguard children: A guide to inter-agency working to safeguard and promote the welfare of children. Prevention and intervention strategies Collecting and analysing data about cases will help the police and other agencies to understand whether there are any patterns related to missing persons incidents. The results of routine data analysis should be shared to inform the development and review of prevention and intervention strategies. Missing Persons Coordinators are vital for this information analysis and sharing. Interventions might include, for example, the use of\\xa0Child Abduction Warning Notices, or referrals to support services. It is important that such interventions take place in appropriate circumstances and are not used as a single response when a person is at risk of harm, but form one strand of a more comprehensive approach. Regular liaison between neighbourhood policing teams and children‚Äôs and adults‚Äô care providers may enable relationships to be developed between police officers, the staff of care establishments, and individuals who are looked after. These relationships may then support effective police intervention and the speedy resolution of cases. For lessons from other cases, see\\xa0IOPC\\xa0Learning the Lesson. Understanding the reasons for going missing Understanding the reasons why an individual went missing may help to prevent future harm to those individuals. The officers in charge of local areas should have clear plans on how they intend to reduce the number of people who go missing in their area.\\n\\n###\\n\\nIt‚Äôs also been raining heavily in the night and we have further calls about flooding in the road, so we ring Highways to inform them. I have a little smile to myself as I remember a call in the summer about cars stopping on the M11 because a mother duck and her ducklings were crossing the road. Lunchtime looms. I‚Äôm feeling hungry, but that disappears when I take a call from a 16-year-old male, who tells me that he can‚Äôt cope any more. He has cut himself with a knife but he doesn‚Äôt want to die. His sister has just had a baby. This goes on an emergency straight away, and officers are dispatched within three minutes. I have to talk to him about anything I can to distract him from his misery ‚Äì luckily, I am good at small talk! Officers arrive and I feel relief as I can hang up the phone. COVID-19 has really affected Essex this year. People are low and weary. You can hear it in their voices. The number of mental health incidents has gone through the roof, and even the force control room team is quieter. Because of\\xa0the onset of lockdown restrictions, more calls are coming in from the public reporting their neighbours for flouting the rules: 'We‚Äôre following the rules, why don‚Äôt they? What makes them think they are special?'\\xa0 Gone are the past calls about drunken people leaving the pub. Instead, we have members of the public who are tired of being tied to the house and resentful of those who ignore the restrictions. After lunch, we receive a flurry of calls. There‚Äôs a domestic, involving a woman who tells me that ‚Äòhe didn‚Äôt mean to hit me, he loves me‚Äô. I spend time with this caller. There are three horses in the road. A driver has hit a dog and is upset, so I reassure him it wasn‚Äôt his fault. It gets busier. Essex is up and running but I am not. I feel tired but this is my job, so I make sure that nobody will hear it in my voice. Finally, it‚Äôs time to go home and hang up the headset for another day. I tend not to reflect on my day too much, so I can have some time to myself. There is no typical day in the control room.\")\n\n\n\nabout_james = \"\"\" \nJames is a 34 year old man, who was reported missing by his wife this evening as he has not returned home from work. It is now 2200, and she expected him home by 1900.\nShe says while he does go out for drinks after work sometimes, he has not been out this late before, and his phone is off.\nJames is in good health, there are no mental health concerns or other vulnerabilities. The weather is good, and his friend from work said he'd probably just gone out for drinks.\n\"\"\"\n\njames_answer = machine_risk_assessment(about_james, df, debug=True)\njames_answer\n\nQuestion:\n \nJames is a 34 year old man, who was reported missing by his wife this evening as he has not returned home from work. It is now 2200, and she expected him home by 1900.\nShe says while he does go out for drinks after work sometimes, he has not been out this late before, and his phone is off.\nJames is in good health, there are no mental health concerns or other vulnerabilities. The weather is good, and his friend from work said he'd probably just gone out for drinks.\n\nContext:\n First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review ‚Äì please see¬†APP¬†on international investigation¬†for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see¬†Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child¬† Voice of the child practice briefing¬† Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children‚Äôs Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‚Äòstreetwise‚Äô and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children¬†may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,¬†as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to¬†work with all the agencies and carers¬†that¬†have been¬†in regular contact with the child as they¬†may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\n\n###\n\n First published 22 November 2016  Updated 16 February 2023   Latest changes  Written by College of Policing  Missing persons  11 mins read   Introduction Going missing should be treated as an indicator that the individual may be at risk of harm. The safeguarding of vulnerable people is paramount and a missing person report should be recognised as an opportunity to identify and address risks. The reasons for a person deciding to go missing may be complex and linked to a variety of social or family issues. Three key factors should be considered in a missing person investigation: protecting those at risk of harm minimising distress and ensuring high quality of service to the families and carers of missing persons prosecuting those who perpetrate harm or pose a risk of harm when this is appropriate and supported by evidence Support for law enforcement agencies Police investigators can contact the following specialists for advice and assistance in missing and unidentified person investigations. UK Missing Persons Unit (UKMPU) on 0800¬†234 6034 NCA¬†Major Crime Investigative Support (MCIS) on 0345 000 5463 Definition of ‚Äòmissing‚Äô Anyone whose whereabouts cannot be established will be considered as missing until located, and their well-being or otherwise confirmed. All reports of missing people sit within a continuum of risk from ‚Äòno apparent risk (absent)‚Äô through to high-risk cases¬†that require immediate, intensive action.  Risk assessment and response The risk assessment table The following table should be used as a guide to¬†an appropriate level of police response based on initial and on-going risk assessment in each case. Risk assessment should be guided by the College of Policing¬†Risk principles,¬†the¬†National Decision Model¬†and Police¬†Code of Ethics. No apparent risk (absent) There is no apparent risk of harm to either the subject or the public. Actions to locate the subject and/or gather further information should be agreed with the informant and a latest review time set to reassess the risk. Low risk The risk of harm to the subject or the public is assessed as possible but minimal. Proportionate enquiries should be carried out to ensure that the individual has not come to harm. Medium risk The risk of harm to the subject or the public is assessed as likely but not serious. This category requires an active and measured response by the police and other agencies in order to trace the missing person and support the person reporting.\n\n###\n\nHigh risk The risk of serious harm to the subject or the public is assessed as very likely. This category almost always requires the immediate deployment of police resources ‚Äì action may be delayed in exceptional circumstances, such as searching water or forested areas during hours of darkness. A member of the senior management team must be involved in the examination of initial lines of enquiry and approval of appropriate staffing levels. Such cases should lead to the appointment of an investigating officer (IO) and possibly an¬†SIO, and a police search adviser (PolSA).            ¬† There should be a press/media strategy and/or close contact with outside agencies. Family support should be put in place where appropriate. The UKMPU¬†should be notified of the case without undue delay. Children‚Äôs services must also be notified immediately if the person is under 18. Risk of serious harm has been defined as (Home Office 2002 and OASys 2006): A risk which is life threatening and/or traumatic, and from which recovery, whether physical or psychological, can be expected to be difficult or impossible. Where the risk cannot be accurately assessed without active investigation, appropriate lines of enquiry should be set to gather the required information to inform the risk assessment. The missing persons process chart  Joint responsibility The police are entitled to expect parents and carers, including staff acting in a parenting role in care homes, to accept normal parenting responsibilities and undertake reasonable actions to try and establish the whereabouts of the individual. Children who are breaching parental discipline should not be dealt with by police unless there are other risks. For example, a child who is late home from a party should not be regarded as missing until the parent or carer has undertaken enquiries to locate the child. Once those enquiries have been completed, it may be appropriate to record the child as missing and take actions set out in this¬†APP. Parents or carers may need police support if they are very distressed, incapacitated or otherwise unable to undertake enquiries. In such circumstances, it may be appropriate to make a referral to the local authority so that the standard of care for the missing person can be reviewed. Individuals whose whereabouts are known will not be considered as missing, but may require other police activity in order to ensure their welfare. Police should consult their local public protection procedures to ensure an appropriate safeguarding response is provided.\n\n\n\n\n\n('Graded as Medium risk, because of the below risk factors:\\n- James has not returned home at the expected time, and his phone is off\\n- There is no indication of any mental health concerns or other vulnerabilities that could put him at risk',\n ' First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review ‚Äì please see\\xa0APP\\xa0on international investigation\\xa0for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see\\xa0Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child\\xa0 Voice of the child practice briefing\\xa0 Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children‚Äôs Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‚Äòstreetwise‚Äô and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children\\xa0may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,\\xa0as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to\\xa0work with all the agencies and carers\\xa0that\\xa0have been\\xa0in regular contact with the child as they\\xa0may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\\n\\n###\\n\\n First published 22 November 2016  Updated 16 February 2023   Latest changes  Written by College of Policing  Missing persons  11 mins read   Introduction Going missing should be treated as an indicator that the individual may be at risk of harm. The safeguarding of vulnerable people is paramount and a missing person report should be recognised as an opportunity to identify and address risks. The reasons for a person deciding to go missing may be complex and linked to a variety of social or family issues. Three key factors should be considered in a missing person investigation: protecting those at risk of harm minimising distress and ensuring high quality of service to the families and carers of missing persons prosecuting those who perpetrate harm or pose a risk of harm when this is appropriate and supported by evidence Support for law enforcement agencies Police investigators can contact the following specialists for advice and assistance in missing and unidentified person investigations. UK Missing Persons Unit (UKMPU) on 0800\\xa0234 6034 NCA\\xa0Major Crime Investigative Support (MCIS) on 0345 000 5463 Definition of ‚Äòmissing‚Äô Anyone whose whereabouts cannot be established will be considered as missing until located, and their well-being or otherwise confirmed. All reports of missing people sit within a continuum of risk from ‚Äòno apparent risk (absent)‚Äô through to high-risk cases\\xa0that require immediate, intensive action.  Risk assessment and response The risk assessment table The following table should be used as a guide to\\xa0an appropriate level of police response based on initial and on-going risk assessment in each case. Risk assessment should be guided by the College of Policing\\xa0Risk principles,\\xa0the\\xa0National Decision Model\\xa0and Police\\xa0Code of Ethics. No apparent risk (absent) There is no apparent risk of harm to either the subject or the public. Actions to locate the subject and/or gather further information should be agreed with the informant and a latest review time set to reassess the risk. Low risk The risk of harm to the subject or the public is assessed as possible but minimal. Proportionate enquiries should be carried out to ensure that the individual has not come to harm. Medium risk The risk of harm to the subject or the public is assessed as likely but not serious. This category requires an active and measured response by the police and other agencies in order to trace the missing person and support the person reporting.\\n\\n###\\n\\nHigh risk The risk of serious harm to the subject or the public is assessed as very likely. This category almost always requires the immediate deployment of police resources ‚Äì action may be delayed in exceptional circumstances, such as searching water or forested areas during hours of darkness. A member of the senior management team must be involved in the examination of initial lines of enquiry and approval of appropriate staffing levels. Such cases should lead to the appointment of an investigating officer (IO) and possibly an\\xa0SIO, and a police search adviser (PolSA). \\t\\t\\t\\xa0 There should be a press/media strategy and/or close contact with outside agencies. Family support should be put in place where appropriate. The UKMPU\\xa0should be notified of the case without undue delay. Children‚Äôs services must also be notified immediately if the person is under 18. Risk of serious harm has been defined as (Home Office 2002 and OASys 2006): A risk which is life threatening and/or traumatic, and from which recovery, whether physical or psychological, can be expected to be difficult or impossible. Where the risk cannot be accurately assessed without active investigation, appropriate lines of enquiry should be set to gather the required information to inform the risk assessment. The missing persons process chart  Joint responsibility The police are entitled to expect parents and carers, including staff acting in a parenting role in care homes, to accept normal parenting responsibilities and undertake reasonable actions to try and establish the whereabouts of the individual. Children who are breaching parental discipline should not be dealt with by police unless there are other risks. For example, a child who is late home from a party should not be regarded as missing until the parent or carer has undertaken enquiries to locate the child. Once those enquiries have been completed, it may be appropriate to record the child as missing and take actions set out in this\\xa0APP. Parents or carers may need police support if they are very distressed, incapacitated or otherwise unable to undertake enquiries. In such circumstances, it may be appropriate to make a referral to the local authority so that the standard of care for the missing person can be reviewed. Individuals whose whereabouts are known will not be considered as missing, but may require other police activity in order to ensure their welfare. Police should consult their local public protection procedures to ensure an appropriate safeguarding response is provided.')\n\n\n\nabout_yannik = \"\"\" Yannik is a 15 year old boy. He has recently been down, and was reported missing by his parents as he did not return home from school today.\n\nHis friends are worried he may be depressed, and when he apparently told one a few days ago 'if it doesn't get any better, I'm going to end it soon'\n\"\"\"\n\nyannik_answer = machine_risk_assessment(about_yannik, df, debug=True)\nyannik_answer\n\nQuestion:\n Yannik is a 15 year old boy. He has recently been down, and was reported missing by his parents as he did not return home from school today.\n\nHis friends are worried he may be depressed, and when he apparently told one a few days ago 'if it doesn't get any better, I'm going to end it soon'\n\nContext:\n First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review ‚Äì please see¬†APP¬†on international investigation¬†for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see¬†Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child¬† Voice of the child practice briefing¬† Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children‚Äôs Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‚Äòstreetwise‚Äô and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children¬†may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,¬†as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to¬†work with all the agencies and carers¬†that¬†have been¬†in regular contact with the child as they¬†may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\n\n###\n\nIt‚Äôs also been raining heavily in the night and we have further calls about flooding in the road, so we ring Highways to inform them. I have a little smile to myself as I remember a call in the summer about cars stopping on the M11 because a mother duck and her ducklings were crossing the road. Lunchtime looms. I‚Äôm feeling hungry, but that disappears when I take a call from a 16-year-old male, who tells me that he can‚Äôt cope any more. He has cut himself with a knife but he doesn‚Äôt want to die. His sister has just had a baby. This goes on an emergency straight away, and officers are dispatched within three minutes. I have to talk to him about anything I can to distract him from his misery ‚Äì luckily, I am good at small talk! Officers arrive and I feel relief as I can hang up the phone. COVID-19 has really affected Essex this year. People are low and weary. You can hear it in their voices. The number of mental health incidents has gone through the roof, and even the force control room team is quieter. Because of¬†the onset of lockdown restrictions, more calls are coming in from the public reporting their neighbours for flouting the rules: 'We‚Äôre following the rules, why don‚Äôt they? What makes them think they are special?'¬† Gone are the past calls about drunken people leaving the pub. Instead, we have members of the public who are tired of being tied to the house and resentful of those who ignore the restrictions. After lunch, we receive a flurry of calls. There‚Äôs a domestic, involving a woman who tells me that ‚Äòhe didn‚Äôt mean to hit me, he loves me‚Äô. I spend time with this caller. There are three horses in the road. A driver has hit a dog and is upset, so I reassure him it wasn‚Äôt his fault. It gets busier. Essex is up and running but I am not. I feel tired but this is my job, so I make sure that nobody will hear it in my voice. Finally, it‚Äôs time to go home and hang up the headset for another day. I tend not to reflect on my day too much, so I can have some time to myself. There is no typical day in the control room.\n\n###\n\nPolice officers should ask the care home or local authority for details of the child‚Äôs risk assessment so that it can be taken into account during the investigation.¬†Many forces are using the Philomena Protocol to guide their actions in relation to relevant cases involving children. A child, especially a teenager, is unlikely to share all information about their life with their parents or carers. Investigators should not overlook information from siblings, friends, associates, school teachers and others. The online activity of the child may also provide valuable additional information which parents and carers may not be aware of. For further information see: Children's views on being reported missing from care¬† No Place at Home - Risks facing children and young people who go missing from out of area placements Child Rescue Alert Child Rescue Alert¬†(CRA) ‚Äì (available to authorised users logged on to the restricted online¬†College Learn) is a partnership between the police and the media which seeks public assistance when it is feared that a child may be at risk of serious harm. Assistance is sought via TV, radio, text messaging, social and digital media (including the internet) so that relevant information relating to the child, offender or any specified vehicle is passed on to the police. CRA¬†focuses on the risk to the child, rather than whether or not an offence has taken place. The criteria for launching an alert is: The child is apparently under 18 years old There is a perception that the child is in imminent danger of serious harm or death There is sufficient information available to enable the public to assist police in locating the child Child Rescue Alert Activation Protocol¬†(available to authorised users logged on to the restricted online¬†College Learn) The¬†CRA¬†has been expanded to enable alerts to be disseminated by the charity Missing People. The system is managed by the National Crime¬†Agency (NCA) and specialist advice is available 24/7 by contacting 0800¬†234 6034.\n\n###\n\nThis is particularly true to the Greater Manchester area. As a result, this research seeks to utilise this appeal in providing a platform for young people who have received a Threats to Life Notice and children to parents who have received a Threat to Life Notice, in order to steer them away from potential criminal activity and receive mentoring through football coaching, education and employability skills. Operationally, this would seek to provide tactical options in the short and long-term in instances where a young person (under the age of 18) is particularly vulnerable as either a victim or perpetrator of criminal behaviour, and potentially reduce the threat, risk and harm pertaining to such individuals. The key aims are: to develop the individuals in the cohort with regards to qualifications, personal wellbeing, skill development, employability and social action (part of their local community and their perception of their community) ‚Äì this in turn will potentially reduce the impact of trauma and increase confidence and self-esteem of the cohort to reduce the effects of adverse childhood experiences through re-sensitisation and raising awareness that adverse childhood experiences should not be considered the norm to understand and improve the delivery and impact of Threats to Life on young people to inform police forces on the best possible process and follow-on actions needed when delivering Threats to Life Notices  Research methodology A cohort of 16 young people between the ages of 13 to 18 will be identified through GMP‚Äôs systems who have either directly received a Threat to Life Notice in the past 12 months, or are children to a parent(s) who have received a Threat to Life Notice in the past 12 months. Their details will then be given to the mentors at the Manchester United Youth Foundation who then make contact through post in the first instance to invite the young person into the scheme. More individuals will be invited into the scheme as it progresses to ensure a minimum of 16 participants at any given stage (catering for any refusals to be involved or disengagement).\n\n\n\n\n\n('Graded as High risk, because of the below risk factors: \\n- Yannik is a 15 year old boy who has been reported missing by his parents\\n- His friends are worried he may be depressed\\n- He has expressed suicidal ideation to one of his friends',\n \" First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review ‚Äì please see\\xa0APP\\xa0on international investigation\\xa0for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see\\xa0Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child\\xa0 Voice of the child practice briefing\\xa0 Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children‚Äôs Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‚Äòstreetwise‚Äô and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children\\xa0may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,\\xa0as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to\\xa0work with all the agencies and carers\\xa0that\\xa0have been\\xa0in regular contact with the child as they\\xa0may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\\n\\n###\\n\\nIt‚Äôs also been raining heavily in the night and we have further calls about flooding in the road, so we ring Highways to inform them. I have a little smile to myself as I remember a call in the summer about cars stopping on the M11 because a mother duck and her ducklings were crossing the road. Lunchtime looms. I‚Äôm feeling hungry, but that disappears when I take a call from a 16-year-old male, who tells me that he can‚Äôt cope any more. He has cut himself with a knife but he doesn‚Äôt want to die. His sister has just had a baby. This goes on an emergency straight away, and officers are dispatched within three minutes. I have to talk to him about anything I can to distract him from his misery ‚Äì luckily, I am good at small talk! Officers arrive and I feel relief as I can hang up the phone. COVID-19 has really affected Essex this year. People are low and weary. You can hear it in their voices. The number of mental health incidents has gone through the roof, and even the force control room team is quieter. Because of\\xa0the onset of lockdown restrictions, more calls are coming in from the public reporting their neighbours for flouting the rules: 'We‚Äôre following the rules, why don‚Äôt they? What makes them think they are special?'\\xa0 Gone are the past calls about drunken people leaving the pub. Instead, we have members of the public who are tired of being tied to the house and resentful of those who ignore the restrictions. After lunch, we receive a flurry of calls. There‚Äôs a domestic, involving a woman who tells me that ‚Äòhe didn‚Äôt mean to hit me, he loves me‚Äô. I spend time with this caller. There are three horses in the road. A driver has hit a dog and is upset, so I reassure him it wasn‚Äôt his fault. It gets busier. Essex is up and running but I am not. I feel tired but this is my job, so I make sure that nobody will hear it in my voice. Finally, it‚Äôs time to go home and hang up the headset for another day. I tend not to reflect on my day too much, so I can have some time to myself. There is no typical day in the control room.\\n\\n###\\n\\nPolice officers should ask the care home or local authority for details of the child‚Äôs risk assessment so that it can be taken into account during the investigation.\\xa0Many forces are using the Philomena Protocol to guide their actions in relation to relevant cases involving children. A child, especially a teenager, is unlikely to share all information about their life with their parents or carers. Investigators should not overlook information from siblings, friends, associates, school teachers and others. The online activity of the child may also provide valuable additional information which parents and carers may not be aware of. For further information see: Children's views on being reported missing from care\\xa0 No Place at Home - Risks facing children and young people who go missing from out of area placements Child Rescue Alert Child Rescue Alert\\xa0(CRA) ‚Äì (available to authorised users logged on to the restricted online\\xa0College Learn) is a partnership between the police and the media which seeks public assistance when it is feared that a child may be at risk of serious harm. Assistance is sought via TV, radio, text messaging, social and digital media (including the internet) so that relevant information relating to the child, offender or any specified vehicle is passed on to the police. CRA\\xa0focuses on the risk to the child, rather than whether or not an offence has taken place. The criteria for launching an alert is: The child is apparently under 18 years old There is a perception that the child is in imminent danger of serious harm or death There is sufficient information available to enable the public to assist police in locating the child Child Rescue Alert Activation Protocol\\xa0(available to authorised users logged on to the restricted online\\xa0College Learn) The\\xa0CRA\\xa0has been expanded to enable alerts to be disseminated by the charity Missing People. The system is managed by the National Crime\\xa0Agency (NCA) and specialist advice is available 24/7 by contacting 0800\\xa0234 6034.\\n\\n###\\n\\nThis is particularly true to the Greater Manchester area. As a result, this research seeks to utilise this appeal in providing a platform for young people who have received a Threats to Life Notice and children to parents who have received a Threat to Life Notice, in order to steer them away from potential criminal activity and receive mentoring through football coaching, education and employability skills. Operationally, this would seek to provide tactical options in the short and long-term in instances where a young person (under the age of 18) is particularly vulnerable as either a victim or perpetrator of criminal behaviour, and potentially reduce the threat, risk and harm pertaining to such individuals. The key aims are: to develop the individuals in the cohort with regards to qualifications, personal wellbeing, skill development, employability and social action (part of their local community and their perception of their community) ‚Äì this in turn will potentially reduce the impact of trauma and increase confidence and self-esteem of the cohort to reduce the effects of adverse childhood experiences through re-sensitisation and raising awareness that adverse childhood experiences should not be considered the norm to understand and improve the delivery and impact of Threats to Life on young people to inform police forces on the best possible process and follow-on actions needed when delivering Threats to Life Notices  Research methodology A cohort of 16 young people between the ages of 13 to 18 will be identified through GMP‚Äôs systems who have either directly received a Threat to Life Notice in the past 12 months, or are children to a parent(s) who have received a Threat to Life Notice in the past 12 months. Their details will then be given to the mentors at the Manchester United Youth Foundation who then make contact through post in the first instance to invite the young person into the scheme. More individuals will be invited into the scheme as it progresses to ensure a minimum of 16 participants at any given stage (catering for any refusals to be involved or disengagement).\")\n\n\n\njason_risk_profile = \"\"\" Jason is a 15 year old adult male, who has gone missing from his care home in Southwark. His carer has contacted the school, which has said he was not in today.\nThey that this is not the first time, and that Jason has been seen hanging out with older boys, who may be involved in crime and drugs.\"\"\"\n\n\njason_answer = machine_risk_assessment(jason_risk_profile, df, debug=True)\njason_answer\n\nQuestion:\n Jason is a 15 year old adult male, who has gone missing from his care home in Southwark. His carer has contacted the school, which has said he was not in today.\nThey that this is not the first time, and that Jason has been seen hanging out with older boys, who may be involved in crime and drugs.\nContext:\n First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review ‚Äì please see¬†APP¬†on international investigation¬†for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see¬†Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child¬† Voice of the child practice briefing¬† Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children‚Äôs Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‚Äòstreetwise‚Äô and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children¬†may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,¬†as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to¬†work with all the agencies and carers¬†that¬†have been¬†in regular contact with the child as they¬†may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\n\n###\n\nThese include children or young people who: go missing¬†‚Äì especially on regular occasions from home or care live in a chaotic or dysfunctional family have a history of domestic abuse within the family environment have a history of abuse (including familial¬†child sexual abuse, risk of¬†forced marriage, risk of¬†honour-based violence,¬†physical¬†and¬†emotional¬†abuse and¬†neglect) have experienced or are experiencing problematic parenting have parents who misuse drugs or alcohol have parents with health problems are young carers within the family unit experience social exclusion as a result of poverty have experienced recent bereavement or loss have unsupervised use of social networking chat rooms/sites have mental ill health have social or learning difficulties have low self-esteem or self-confidence are unsure about their sexual orientation or are unable to confide in their family about their sexual orientation misuse alcohol and/or drugs have been or are excluded from mainstream education are involved in gang activity attend school with other young people who are sexually exploited are friends with individuals who are sexually exploited do not have friends in the same age group are being bullied live in care, foster care, hostels and/or bed and breakfast accommodation ‚Äì particularly when living out of their home area are homeless have associations with gangs through relatives, peers or intimate relationships live in a gang neighbourhood This is not an exhaustive list, nor have the vulnerabilities been listed in order of importance. Children from loving and secure homes can also be victims of sexual exploitation. The¬†characteristics common to all victims are not always their age, ethnicity, disability or sexual orientation, but their powerlessness and vulnerability. Warning signs Despite the increased profile of¬†CSE¬†and improvements in how the police work with partner agencies,¬†CSE¬†cases are still under-reported. The Office of the Children‚Äôs Commissioner conducted a two-year inquiry into child sexual exploitation in gangs and groups. Their 2013¬†report,¬†If only someone had listened, highlights that sexually exploited children are not always identified even when they show signs of being victims. Numerous warning signs were identified in the Office of the Children‚Äôs Commissioner 2012 interim report,¬†I thought I was the only one, the only one in the world, which can indicate that¬†a young person is being forced or manipulated into sexual activity and is a victim of sexual exploitation. Practitioners need to be aware of these warning signs and recognise that a victim does not¬†have to exhibit all of the warning signs to be a victim of sexual exploitation.\n\n###\n\nStaff should tell the detainee that she can ask to see the carer at any time. Forces must implement policies and procedures to ensure that all girls who are detained and in custody are under the care of a woman. For further information, see¬†PACE Code C, section 3.20A. Transporting children and young people Children and young persons under the age of 18 are not allowed to associate with adult detainees while being detained, conveyed to and from court or waiting to be so conveyed. An exception to this is permitted in accordance with the Children and Young Persons Act 1933 section 31, where the young person is jointly charged with an adult or relative. Officers should make arrangements to prevent association when the child or young person is: detained in a police station being conveyed to or from any criminal court attending court Children and young people should not be carried in a vehicle with adult detainees unless the vehicle has been designed and built to carry them separately and simultaneously. Vehicles that are available for this specific purpose are authorised under the 2011 Prisoner Escort and Custody Services‚Äô contract arrangements. Appropriate adults Forces should establish policies and protocols to provide access to appropriate adults for young persons in police custody. Local YOTs have a statutory responsibility to ensure that an appropriate adult service is provided for children and young people, whether they provide the service themselves or contract a voluntary or private sector agency to do so on their behalf. It is the responsibility of the appropriate adult provider to work with the local force to develop policies and protocols to ensure there is effective provision of appropriate adult services in line with Youth Justice Board (2014) case management guidance. This guidance makes it clear that the appropriate adult service should operate out of hours as well as during standard working hours. All appropriate adults, custody managers, custody officers and staff must be aware of their role as defined by PACE, and also of any agreed local policies, protocols or service level agreements for providing appropriate adults. For further information, see PACE and section 38 of the Crime and Disorder Act 1998. See also the Youth Justice Board‚Äôs (YJB) National Standards for Youth Justice Services. The YJB and National Appropriate Adult Network have also published joint guidance and advice on appropriate adult services. When should an appropriate adult be contacted? Detention can be very stressful so it is important that an appropriate adult attends as soon as is practicable to minimise the amount of time the child or young person spends in detention.\n\n\n\n\n\n('Graded as Medium risk, because of the below risk factors: \\n- Jason is a 15 year old adult male, who has gone missing from his care home in Southwark\\n- This is not the first time he has gone missing\\n- He has been seen hanging out with older boys, who may be involved in crime and drugs',\n ' First published 22 November 2016  Updated 15 March 2023   Latest changes  Written by College of Policing  Missing persons  30 mins read   Implications for the UK leaving the European Union are currently under review ‚Äì please see\\xa0APP\\xa0on international investigation\\xa0for latest available detail on specific areas, for example: Schengen Information System Europol INTERPOL Joint Investigation Teams This section provides additional information to aid the investigation based on the vulnerability of the individual and the circumstances in which they are missing. Missing children Safeguarding young and vulnerable people is a responsibility of the police service and partner agencies (see\\xa0Children Act 2004). When the police are notified that a child is missing, there is a clear responsibility on them to prevent the child from coming to harm. Where appropriate, a strategy meeting may be held. For further information see: Voice of the child\\xa0 Voice of the child practice briefing\\xa0 Section 11 of the Children Act 2004 Department for Education (2014) Statutory guidance on children who run away or go missing from home or care Children‚Äôs Views on being Reported Missing from Care Young people and risky behaviour Children and young people often do not have the same levels of awareness or ability to keep themselves safe as adults. Going missing may indicate that something is wrong in their lives. Many of the children and young people who repeatedly go missing are considered by some to be ‚Äòstreetwise‚Äô and able to look after themselves. However, these children may not understand the risk they are exposing themselves to, and should not be treated as low/no apparent risk simply due to their apparent willingness/complicity. Children\\xa0may put themselves in danger because they may have been abused, neglected or rejected by their families or others and,\\xa0as a result, they may engage in further risky behaviours, such as: misusing substances committing crimes having risky sexual contacts living on the streets mixing with inappropriate adults Information relevant to the child When a missing person report relates to a looked-after child, it is important to\\xa0work with all the agencies and carers\\xa0that\\xa0have been\\xa0in regular contact with the child as they\\xa0may have information about the child that might help to locate them. When a child is missing from care, close engagement with the carers is important.\\n\\n###\\n\\nThese include children or young people who: go missing\\xa0‚Äì especially on regular occasions from home or care live in a chaotic or dysfunctional family have a history of domestic abuse within the family environment have a history of abuse (including familial\\xa0child sexual abuse, risk of\\xa0forced marriage, risk of\\xa0honour-based violence,\\xa0physical\\xa0and\\xa0emotional\\xa0abuse and\\xa0neglect) have experienced or are experiencing problematic parenting have parents who misuse drugs or alcohol have parents with health problems are young carers within the family unit experience social exclusion as a result of poverty have experienced recent bereavement or loss have unsupervised use of social networking chat rooms/sites have mental ill health have social or learning difficulties have low self-esteem or self-confidence are unsure about their sexual orientation or are unable to confide in their family about their sexual orientation misuse alcohol and/or drugs have been or are excluded from mainstream education are involved in gang activity attend school with other young people who are sexually exploited are friends with individuals who are sexually exploited do not have friends in the same age group are being bullied live in care, foster care, hostels and/or bed and breakfast accommodation ‚Äì particularly when living out of their home area are homeless have associations with gangs through relatives, peers or intimate relationships live in a gang neighbourhood This is not an exhaustive list, nor have the vulnerabilities been listed in order of importance. Children from loving and secure homes can also be victims of sexual exploitation. The\\xa0characteristics common to all victims are not always their age, ethnicity, disability or sexual orientation, but their powerlessness and vulnerability. Warning signs Despite the increased profile of\\xa0CSE\\xa0and improvements in how the police work with partner agencies,\\xa0CSE\\xa0cases are still under-reported. The Office of the Children‚Äôs Commissioner conducted a two-year inquiry into child sexual exploitation in gangs and groups. Their 2013\\xa0report,\\xa0If only someone had listened, highlights that sexually exploited children are not always identified even when they show signs of being victims. Numerous warning signs were identified in the Office of the Children‚Äôs Commissioner 2012 interim report,\\xa0I thought I was the only one, the only one in the world, which can indicate that\\xa0a young person is being forced or manipulated into sexual activity and is a victim of sexual exploitation. Practitioners need to be aware of these warning signs and recognise that a victim does not\\xa0have to exhibit all of the warning signs to be a victim of sexual exploitation.\\n\\n###\\n\\nStaff should tell the detainee that she can ask to see the carer at any time. Forces must implement policies and procedures to ensure that all girls who are detained and in custody are under the care of a woman. For further information, see\\xa0PACE Code C, section 3.20A. Transporting children and young people Children and young persons under the age of 18 are not allowed to associate with adult detainees while being detained, conveyed to and from court or waiting to be so conveyed. An exception to this is permitted in accordance with the Children and Young Persons Act 1933 section 31, where the young person is jointly charged with an adult or relative. Officers should make arrangements to prevent association when the child or young person is: detained in a police station being conveyed to or from any criminal court attending court Children and young people should not be carried in a vehicle with adult detainees unless the vehicle has been designed and built to carry them separately and simultaneously. Vehicles that are available for this specific purpose are authorised under the 2011 Prisoner Escort and Custody Services‚Äô contract arrangements. Appropriate adults Forces should establish policies and protocols to provide access to appropriate adults for young persons in police custody. Local YOTs have a statutory responsibility to ensure that an appropriate adult service is provided for children and young people, whether they provide the service themselves or contract a voluntary or private sector agency to do so on their behalf. It is the responsibility of the appropriate adult provider to work with the local force to develop policies and protocols to ensure there is effective provision of appropriate adult services in line with Youth Justice Board (2014) case management guidance. This guidance makes it clear that the appropriate adult service should operate out of hours as well as during standard working hours. All appropriate adults, custody managers, custody officers and staff must be aware of their role as defined by PACE, and also of any agreed local policies, protocols or service level agreements for providing appropriate adults. For further information, see PACE and section 38 of the Crime and Disorder Act 1998. See also the Youth Justice Board‚Äôs (YJB) National Standards for Youth Justice Services. The YJB and National Appropriate Adult Network have also published joint guidance and advice on appropriate adult services. When should an appropriate adult be contacted? Detention can be very stressful so it is important that an appropriate adult attends as soon as is practicable to minimise the amount of time the child or young person spends in detention.')\n\n\nIt‚Äôs honestly not too bad! while this certainly wouldn‚Äôt replace human decision making, it could certainly act as a safeguard against missing a key piece of information at three in the morning when you have a queue of 8 missing people to evaluate, each pages worth of history.\nThere are certainly some pretty major niggles you‚Äôd need to fix: for one I‚Äôm not sure how the model will perform given actual legislation, rather than ‚Äúplain English‚Äù guidance, nor do I imagine it will cope particularly well with policing specific terminology. The risks it‚Äôs identified so far are all mostly obvious, rather than identifying one needle in a giant haystack of intelligence reports. It‚Äôs also all going through the OpenAI black-box servers, though I imagine that could be replaced with something open-ish like Llama without too much effort. But when I consider where NLP was even 18 months ago, and just how much computing cognitive power we‚Äôve been able to deploy in only a few hours‚Ä¶who knows where we‚Äôll be next year?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hi!",
    "section": "",
    "text": "I‚Äôm Andreas Varotsis, a data scientist and public safety researcher working to improve the use of data and evidence in government.I try to use an empirical approach and multi-disciplinary, quantitative tools to understand, diagnose, and build solutions for critical problems around public cohesion, society and public safety.\nI also enjoy coordinating exciting communities, and working with fantastic institutions to build projects that improve the world.\nMy particular areas of interest are:\n\ndata-science and machine learning\ncrime and police activity in physical space\neconometrics and economic modelling\nexperimental and quasi-experimental testing\nrapid technological solutions and innovation\n\nI use this site to showcase my recent work, and discuss new data-science and statistical tools and techniques on my blog.\nIf you‚Äôd like to chat, hear more, or absolutely anything else, on Matrix or by email."
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#why-statistics-in-evaluation",
    "href": "slides/statistics_101_for_evaluation/index.html#why-statistics-in-evaluation",
    "title": "Statistics 101 for Evaluation",
    "section": "Why Statistics in Evaluation?",
    "text": "Why Statistics in Evaluation?\n\n\n The Core Question\n‚ÄúDid our intervention cause the outcome we observed?‚Äù\n\n\n\nWithout Statistics‚Ä¶\n\n\nWe can‚Äôt distinguish:\n\nReal effects from random noise\n\nCausation from correlation\n\nMeaningful change from measurement error\n\n\n\n\n\n The Solution\nStatistics provides:\n\n Quantification of uncertainty\n Building credible causal claims\n Evidence-based decisions\n\n\n\n\nKey Insight\n\n\nStatistics is how we separate signal from noise\n\n\n\n\n\nOpen by asking: ‚ÄúHas anyone ever looked at data from a project and wondered: is that real or just chance?‚Äù\nEmphasize: Today is foundational. When we later ask evaluation questions, stats is how we calibrate confidence.\nThis isn‚Äôt a full evaluation design session. It‚Äôs the statistical spine that supports everything else."
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#what-were-building-today",
    "href": "slides/statistics_101_for_evaluation/index.html#what-were-building-today",
    "title": "Statistics 101 for Evaluation",
    "section": "What We‚Äôre Building Today",
    "text": "What We‚Äôre Building Today\n\nSeven Pillars of Statistical Evaluation\n\n\n\n\n Uncertainty & Variability\nUnderstanding noise vs signal\n Probability Fundamentals\nThe language of chance\n Sampling & Randomness\nWho we measure matters\n Significance & Confidence\nInterpreting statistical tests\n\n\n\n Power & Sample Size\nPlanning for detection\n Common Pitfalls\nAvoiding false discoveries\n Bayesian Perspective\nA different lens\n\n\n\n\n\n\n\n\nAll grounded in civic tech evaluation scenarios ‚Äî service forms, outreach emails, dashboards, appointment reminders\n\n\n\n\n‚ÄúWe‚Äôll use examples throughout: service forms, outreach emails, dashboards, appointment reminders. Real evaluation contexts.‚Äù\nReassure: ‚ÄúThis is theory-forward but practical. Each concept connects to evaluation work you‚Äôll do.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#everything-varies",
    "href": "slides/statistics_101_for_evaluation/index.html#everything-varies",
    "title": "Statistics 101 for Evaluation",
    "section": "Everything Varies",
    "text": "Everything Varies\n\n\n Natural Fluctuation\nEven without any changes, measurements bounce around:\n\nDaily form completions fluctuate\nResponse rates differ week to week\n\nComplaint volumes vary by season\n\n\n The Challenge\n\n\n\n\n\n\nSeparate Signal from Noise\n\n\nHow do we distinguish intervention effects from natural variability?\nThis is the fundamental problem of evaluation.\n\n\n\n\nAnswer: We need to quantify the noise first, then detect signal above that level.\n\n\n\nDraw on chalkboard: a horizontal line (the ‚Äútrue‚Äù average) with dots scattered above and below it.\nAsk: ‚ÄúWhat causes this scatter in your own projects?‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#key-concepts-variance-standard-deviation",
    "href": "slides/statistics_101_for_evaluation/index.html#key-concepts-variance-standard-deviation",
    "title": "Statistics 101 for Evaluation",
    "section": "Key Concepts: Variance & Standard Deviation",
    "text": "Key Concepts: Variance & Standard Deviation\n\n\n Variance (œÉ¬≤)\nAverage squared distance from the mean\n\nMeasures spread in squared units\nAlways positive\nHard to interpret directly\n\n\n Standard Deviation (œÉ)\nSquare root of variance\n\nSame units as original measurement\nIntuitive scale of ‚Äútypical deviation‚Äù\nThis is what we use for interpretation\n\n\n\n\n\n\n Example: Service Form Completions\n\n\nDaily completions average 100, with œÉ = 15\n\nTypical days: 85‚Äì115 completions (¬±1 SD)\nExtreme days: 70‚Äì130 completions (¬±2 SD)\n\n\n\n\n\n\n‚ÄúBefore testing anything, quantify the noise. If completion rates normally vary by ¬±15 per day, seeing 110 completions isn‚Äôt evidence of success.‚Äù\nPrompt: ‚ÄúThink of a metric in your project. What‚Äôs a ‚Äònormal‚Äô amount of day-to-day bounce?‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#signal-vs-noise",
    "href": "slides/statistics_101_for_evaluation/index.html#signal-vs-noise",
    "title": "Statistics 101 for Evaluation",
    "section": "Signal vs Noise",
    "text": "Signal vs Noise\n\n\n Definitions\nSignal: The real effect of your intervention\nNoise: Random variation, measurement error, external factors\n\n Visual Comparison\n\nWithout intervention: Background noise only\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n\nSmall effect: Hard to distinguish from noise\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñ≤\n\n\nLarge effect: Clear signal above noise\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñ≤‚ñ≤‚ñ≤‚ñ≤‚ñ≤\n\n\n\n\nGoal\n\n\nDetect signal above the noise level\n\n\n\n\n\n‚ÄúStatistical methods help us ask: Is that bump signal or just noise?‚Äù\nReal example: ‚ÄúA council tweaks copy on a benefits form. Even without the change, completion rates vary. How do we know if +5% is real?‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#worked-example-service-form",
    "href": "slides/statistics_101_for_evaluation/index.html#worked-example-service-form",
    "title": "Statistics 101 for Evaluation",
    "section": "Worked Example: Service Form",
    "text": "Worked Example: Service Form\n\n\n\n Scenario\n\n\nYou redesign an online reporting form for local council\n\n\n\n\n\n Before Redesign\n\nAverage daily completions: 42\nStandard deviation: 8\nData period: 60 days\n\n\nTypical range: 34‚Äì50 completions/day\n\n\n After Redesign (Week 1)\n\nAverage daily completions: 47\nDifference: +5 completions\n\n\n Is this meaningful?\n\n\n\n\n\n\n\n\nAnswer: Difference is 5, which is less than 1 SD. Could easily be random variation.\n\n\n\n\n\n\n‚ÄúThis is why we need proper statistical tests. Eyeballing differences isn‚Äôt enough.‚Äù\n‚ÄúLater we‚Äôll learn formal tests. For now: understand that noise is real and must be measured.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#sources-of-variation",
    "href": "slides/statistics_101_for_evaluation/index.html#sources-of-variation",
    "title": "Statistics 101 for Evaluation",
    "section": "Sources of Variation",
    "text": "Sources of Variation\n\n\n Random Variation\n\nNatural fluctuations\nSampling error\nMeasurement error\n\n\n\n\n\nUnpredictable, averages out\n\n\n\n\n\n Systematic Variation\n\nSeasonal patterns\nDay-of-week effects\nExternal events\nHolidays, weather, news\n\n\n\n\n\nPredictable, needs accounting\n\n\n\n\n\n Intervention Effects\nThe thing we actually want to measure!\n\n\n\n\nThe Goal\n\n\nIsolate intervention effects from other sources\n\n\n\n\n\n\nAsk class: ‚ÄúIn your projects, what are two sources of noise you need to account for?‚Äù\nGive 2 minutes for them to write down answers."
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#probability-the-grammar-of-uncertainty",
    "href": "slides/statistics_101_for_evaluation/index.html#probability-the-grammar-of-uncertainty",
    "title": "Statistics 101 for Evaluation",
    "section": "Probability: The Grammar of Uncertainty",
    "text": "Probability: The Grammar of Uncertainty\n\n\n The Scale\n\n0 = impossible\n0.5 = equally likely\n1 = certain\n\n\nAll probabilities fall between these bounds\n\n\n In Evaluation\n\n\n\nThe Key Question\n\n\n‚ÄúIf the intervention had no effect, what‚Äôs the probability of seeing results this extreme?‚Äù\n\n\n\n\nThis is the foundation of hypothesis testing\n\n\n\n‚ÄúProbability lets us reason about what could happen in alternative worlds.‚Äù\n‚ÄúWe‚Äôll keep this informal. Think: repeated experiments, long-run frequencies.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#random-variables",
    "href": "slides/statistics_101_for_evaluation/index.html#random-variables",
    "title": "Statistics 101 for Evaluation",
    "section": "Random Variables",
    "text": "Random Variables\n\n\n\n Definition\n\n\nA quantity whose value is determined by chance\n\n\n\n\n\n Binary\nYes/No outcomes\n\nForm completed?\nEmail opened?\nComplaint filed?\n\n\n Continuous\nMeasured quantities\n\nResponse time (days)\nSatisfaction score\nWait time (minutes)\n\n\n Count\nNumber of events\n\nReports submitted\nVisits to website\nComplaints received\n\n\n\n\n\n\n\n\n\nWhy it matters: Different types need different statistical approaches\n\n\n\n\n\n‚ÄúYour evaluation outcome is a random variable. Knowing its type shapes your analysis.‚Äù\nAsk: ‚ÄúIs your main outcome binary, continuous, or a count?‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#expected-value",
    "href": "slides/statistics_101_for_evaluation/index.html#expected-value",
    "title": "Statistics 101 for Evaluation",
    "section": "Expected Value",
    "text": "Expected Value\nExpected value (mean): The long-run average\nIf we could run the same scenario 1,000 times, what average would we see?\nExample: Rolling a fair die\n\nPossible outcomes: 1, 2, 3, 4, 5, 6\nExpected value: (1+2+3+4+5+6)/6 = 3.5\n\nIn evaluation: If baseline complaint rate is 5%, we expect 50 complaints from 1,000 users (but might see 43 or 57).\n\n‚ÄúSingle observations can mislead. Expected value is about the central tendency across many trials.‚Äù\n‚ÄúThis connects to sample size: larger samples get closer to expected value.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#distributions-tell-stories",
    "href": "slides/statistics_101_for_evaluation/index.html#distributions-tell-stories",
    "title": "Statistics 101 for Evaluation",
    "section": "Distributions Tell Stories",
    "text": "Distributions Tell Stories\nDistribution: Pattern of how values spread out\nCommon patterns:\n\nNormal (bell curve): Many measurements cluster around mean\nBinomial: Success/failure counts (e.g., survey responses)\nPoisson: Rare events (e.g., complaints, accidents)\n\nEvaluation implication: Rare events need more data to detect changes reliably.\n\nDon‚Äôt dwell on formulas. Visual intuition is enough.\n‚ÄúIf you‚Äôre evaluating something rare (like fraud reports), you need a lot of observations to detect change.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#tails-and-rare-events",
    "href": "slides/statistics_101_for_evaluation/index.html#tails-and-rare-events",
    "title": "Statistics 101 for Evaluation",
    "section": "Tails and Rare Events",
    "text": "Tails and Rare Events\nThe tails: Extreme, unusual values\nExample: Evaluating a new policing dashboard\n\nBaseline: 2 complaints per month\nAfter launch: 5 complaints in one month\n\nQuestion: Real increase or random spike?\nChallenge: With rare events, natural variation is large relative to mean. Hard to separate signal from noise quickly.\n\n‚ÄúRare event evaluations require patience and longer observation windows.‚Äù\n‚ÄúOr, you need to aggregate (multiple areas, longer time periods) to get enough events.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#law-of-large-numbers-intuition",
    "href": "slides/statistics_101_for_evaluation/index.html#law-of-large-numbers-intuition",
    "title": "Statistics 101 for Evaluation",
    "section": "Law of Large Numbers (Intuition)",
    "text": "Law of Large Numbers (Intuition)\nCore idea: As sample size increases, sample average converges to true average\nExample: Estimating form completion rate\n\n10 users: 60% complete (could be lucky)\n100 users: 52% complete (getting closer)\n1,000 users: 50.1% complete (very close to true 50%)\n\n\nLarger samples are more stable and trustworthy.\n\n\n‚ÄúThis is why we care about sample size. Not for magic, but for stability.‚Äù\n‚ÄúSmall samples are noisier. Large samples average out the randomness.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#probability-in-practice",
    "href": "slides/statistics_101_for_evaluation/index.html#probability-in-practice",
    "title": "Statistics 101 for Evaluation",
    "section": "Probability in Practice",
    "text": "Probability in Practice\nYour turn: Think about your project outcome.\n\nIs it binary, continuous, or count?\nWhat does the ‚Äúexpected value‚Äù mean operationally?\nIs it common or rare?\n\nExample answers:\n\nForm completion (binary): Expected = % who complete in long run\nResponse time (continuous): Expected = average days\nReports submitted (count, rare if baseline is low)\n\n\nGive 3 minutes for silent reflection and writing.\nAsk 2‚Äì3 people to share their outcome type and what ‚Äúexpected value‚Äù means for them."
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#who-you-measure-matters",
    "href": "slides/statistics_101_for_evaluation/index.html#who-you-measure-matters",
    "title": "Statistics 101 for Evaluation",
    "section": "Who You Measure Matters",
    "text": "Who You Measure Matters\n\n\n Sampling Defined\nSelecting a subset from a larger population\n\nWe rarely measure everyone\nSample must represent population\nBad sampling ‚Üí biased conclusions\n\n\n\n\n\n\n\n\n The Iron Law\n\n\n\n‚ÄúBias beats variance: a small unbiased sample &gt; a large biased one.‚Äù\n\nA perfectly random sample of 100 &gt; a biased convenience sample of 10,000\n\n\n\n\nTakeaway: Who you choose determines whether results generalize\n\n\n\n‚ÄúYou can‚Äôt poll all residents, test all users, or observe all transactions.‚Äù\n‚ÄúSo: who you choose to measure determines whether your results generalize.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#random-sampling",
    "href": "slides/statistics_101_for_evaluation/index.html#random-sampling",
    "title": "Statistics 101 for Evaluation",
    "section": "Random Sampling",
    "text": "Random Sampling\n\n\n\n Definition\n\n\nEvery member of population has known, non-zero probability of selection\n\n\n\n\n\n Gold Standard\n\nEliminates systematic bias\nAllows probability-based inference\nErrors are random (average out)\nNot systematic (don‚Äôt average out)\n\n\n Not Random\n\nConvenience samples\nVolunteers\n‚ÄúWhoever responds‚Äù\nFirst 100 users\n\n\n\n\n\n\n\n\nThese introduce systematic bias\n\n\n\n\n\n\n‚ÄúRandom ‚â† haphazard. It means using a formal process (like drawing from a list) where each person has equal chance.‚Äù\n‚ÄúVolunteers are biased: they care more, have more time, differ systematically.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#sampling-frames-coverage",
    "href": "slides/statistics_101_for_evaluation/index.html#sampling-frames-coverage",
    "title": "Statistics 101 for Evaluation",
    "section": "Sampling Frames & Coverage",
    "text": "Sampling Frames & Coverage\n\n\n\n Sampling Frame\n\n\nThe list from which you sample\n\n\n\n\n\n Coverage Issues\nUnder-coverage: Frame misses some groups\nOver-coverage: Frame includes non-targets\n\n Example: Email List\n\nUnder-coverage: - People without email - Older residents - Digitally excluded\n\n\nOver-coverage: - People who moved away - Duplicate entries\n\n\n\n‚ÄúBefore sampling, ask: Who‚Äôs on my list? Who‚Äôs missing?‚Äù\n‚ÄúDigital-only samples often miss older, offline, or disadvantaged groups.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#selection-bias",
    "href": "slides/statistics_101_for_evaluation/index.html#selection-bias",
    "title": "Statistics 101 for Evaluation",
    "section": "Selection Bias",
    "text": "Selection Bias\n\n\n\n Definition\n\n\nWhen sample systematically differs from population\n\n\n\n\n\n Common Examples\n\nOffice-hours bias\nSurveying 9‚Äì5 users (miss shift workers)\nEarly adopter bias\nFirst users (more tech-savvy)\nResponse bias\nOnly engaged residents reply\n\n\n Impact\n\nResults don‚Äôt generalize to broader population\n\n\n\n\n\nBiggest Threat\n\n\nThis is the #1 threat to external validity\n\n\n\n\n\n\n‚ÄúThis is the biggest threat to external validity.‚Äù\n‚ÄúYou might have perfect internal validity (correct comparison) but if your sample is biased, findings don‚Äôt generalize.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#sampling-error-vs-bias",
    "href": "slides/statistics_101_for_evaluation/index.html#sampling-error-vs-bias",
    "title": "Statistics 101 for Evaluation",
    "section": "Sampling Error vs Bias",
    "text": "Sampling Error vs Bias\n\n\n Sampling Error\nRandom variation from using a sample\n\n Reduces with larger sample\n Quantifiable (CIs)\n Acceptable & expected\n\n\n\n\n\nNature: Noise that averages out\n\n\n\n\n\n Sampling Bias\nSystematic deviation from selection\n\n Does NOT reduce with larger n\n Must prevent through design\n Fatal flaw\n\n\n\n\n\nNature: Directional & persistent\n\n\n\n\n\n\n\n\n\n\n\n\nKey Insight: You can‚Äôt fix bias with a bigger sample!\n\n\n\n\n\n‚ÄúYou can‚Äôt fix bias with a bigger sample. Bias is directional and persistent.‚Äù\n‚ÄúError is noise (gets smaller with n). Bias is systematic shift (stays no matter how large n).‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#clustered-sampling",
    "href": "slides/statistics_101_for_evaluation/index.html#clustered-sampling",
    "title": "Statistics 101 for Evaluation",
    "section": "Clustered Sampling",
    "text": "Clustered Sampling\nCluster sampling: Sampling groups (clusters) then individuals within them\nExamples:\n\nSampling councils, then residents within each\nSampling neighbourhoods, then households\n\nWhy it matters:\n\nPeople in same cluster are often similar\nReduces effective sample size\nStandard errors need adjustment\n\n\n‚ÄúIf you sample 100 people all from the same neighbourhood, that‚Äôs not as good as 100 people spread across the city.‚Äù\n‚ÄúWithin-cluster correlation (intra-class correlation) matters for power calculations.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#interactive-demo-survey-sampling-simulator",
    "href": "slides/statistics_101_for_evaluation/index.html#interactive-demo-survey-sampling-simulator",
    "title": "Statistics 101 for Evaluation",
    "section": "Interactive Demo: Survey Sampling Simulator",
    "text": "Interactive Demo: Survey Sampling Simulator\nScenario: Estimate citizen satisfaction with redesigned online reporting form\nThe tool lets you:\n\nChoose sampling mode:\n\nTruly random\n‚ÄúOffice hours only‚Äù\n‚ÄúMobile users only‚Äù\n‚ÄúEarly adopters only‚Äù\n\nSee sample mean vs true population mean\nObserve sampling error across repeated draws\n\nKey lesson: Convenience samples can look precise but be wrong\n\n[This slide would link to or embed the interactive tool]\n‚ÄúWe‚Äôll run this live. I‚Äôll show random sampling first (notice results cluster around truth), then switch to office-hours-only (notice systematic upward bias).‚Äù\nSpend ~10 minutes here. Let people try different modes."
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#practical-sampling-advice",
    "href": "slides/statistics_101_for_evaluation/index.html#practical-sampling-advice",
    "title": "Statistics 101 for Evaluation",
    "section": "Practical Sampling Advice",
    "text": "Practical Sampling Advice\nFor evaluations:\n\nDefine population clearly ‚Äî who are you trying to generalize to?\nGet best possible frame ‚Äî voter rolls, service user lists, etc.\nRandomize ‚Äî use random number generator, not ‚Äúpick convenient people‚Äù\nDocument non-response ‚Äî track who didn‚Äôt respond and why\nConsider weights ‚Äî adjust for differential non-response by group\n\nReality check: Perfect random samples are rare. Acknowledge limitations.\n\n‚ÄúIn real projects, you often can‚Äôt get perfect random samples. That‚Äôs okay.‚Äù\n‚ÄúWhat matters: (1) be transparent about sampling method, (2) consider who‚Äôs missing, (3) discuss implications.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#reflection-your-sampling-approach",
    "href": "slides/statistics_101_for_evaluation/index.html#reflection-your-sampling-approach",
    "title": "Statistics 101 for Evaluation",
    "section": "Reflection: Your Sampling Approach",
    "text": "Reflection: Your Sampling Approach\nConsider:\n\nWho is your target population?\nWhat‚Äôs your sampling frame?\nAre there coverage problems?\nWhat mode of selection will you use?\nWho might be systematically excluded?\n\n\nGive 2 minutes for silent reflection.\n‚ÄúThis is homework thinking. When you design your evaluation, come back to these questions.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#the-core-evaluation-question",
    "href": "slides/statistics_101_for_evaluation/index.html#the-core-evaluation-question",
    "title": "Statistics 101 for Evaluation",
    "section": "The Core Evaluation Question",
    "text": "The Core Evaluation Question\n\n\n‚ÄúIs the observed difference real (caused by intervention) or chance (random noise)?‚Äù\n\n\n\n\n Hypothesis Testing\nUsing p-values to assess evidence\n\n\nAssumes null hypothesis\nCalculates probability\nTests for significance\n\n\n\n Confidence Intervals\nRange of plausible effects\n\n\nShows magnitude\nDisplays precision\nIndicates direction\n\n\n\n\n\n\n\n\n\n\nBoth approaches quantify uncertainty around estimates\n\n\n\n\n\n‚ÄúThis is where evaluation gets rigorous. We move from ‚Äòit looks different‚Äô to ‚Äòhere‚Äôs how confident we can be‚Äô.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#null-hypothesis",
    "href": "slides/statistics_101_for_evaluation/index.html#null-hypothesis",
    "title": "Statistics 101 for Evaluation",
    "section": "Null Hypothesis",
    "text": "Null Hypothesis\n\n\n Null Hypothesis (H‚ÇÄ)\nThe ‚Äúnothing happened‚Äù scenario\n\nNo intervention effect\nNo difference between groups\nStatus quo continues\n\n\n\n\n\nAssumed true until evidence suggests otherwise\n\n\n\n\n\n Alternative Hypothesis (H‚ÇÅ)\nSomething did happen\n\nIntervention had an effect\nGroups differ\nChange from status quo\n\n\n\n\n\nWhat we test for (but don‚Äôt ‚Äúprove‚Äù)\n\n\n\n\n\n\nTesting logic: Assume H‚ÇÄ is true, then ask how surprising the data would be\n\n\n‚ÄúWe don‚Äôt ‚Äòprove‚Äô H‚ÇÅ. We try to reject H‚ÇÄ.‚Äù\n‚ÄúIt‚Äôs like a court trial: assume innocence (H‚ÇÄ), then see if evidence is strong enough to overturn that assumption.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#what-is-a-p-value",
    "href": "slides/statistics_101_for_evaluation/index.html#what-is-a-p-value",
    "title": "Statistics 101 for Evaluation",
    "section": "What is a p-value?",
    "text": "What is a p-value?\n\n\n\n Definition\n\n\nProbability of seeing results this extreme or more if H‚ÇÄ were true\n\n\n\n\n\n Interpretation\n\np = 0.03 means:\n‚ÄúIf intervention had no effect, we‚Äôd see this large a difference only 3% of the time‚Äù\n\n\nLogic: - Small p-value ‚Üí Data surprising under H‚ÇÄ - Surprising data ‚Üí Evidence against H‚ÇÄ\n\n\n Common Threshold\n\np &lt; 0.05\n\n\n\n\n\nThis is arbitrary! Traditional, not magical.\n\n\n\n\n\n\n‚Äúp-value is NOT the probability that H‚ÇÄ is true.‚Äù\n‚ÄúIt‚Äôs the probability of your data (or more extreme) given H‚ÇÄ is true.‚Äù\n‚ÄúThink of it as: How surprised should we be if nothing was happening?‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#interpreting-p-values",
    "href": "slides/statistics_101_for_evaluation/index.html#interpreting-p-values",
    "title": "Statistics 101 for Evaluation",
    "section": "Interpreting p-values",
    "text": "Interpreting p-values\np &lt; 0.05:\n\nEvidence against H‚ÇÄ\nOften called ‚Äústatistically significant‚Äù\nDoes NOT mean ‚Äúlarge‚Äù or ‚Äúimportant‚Äù\n\np &gt; 0.05:\n\nInsufficient evidence against H‚ÇÄ\nDoes NOT mean ‚Äúno effect exists‚Äù\nCould be: real small effect, or not enough data\n\n\nKey insight: Statistical significance ‚â† practical importance\n\n\n‚ÄúYou can have statistically significant tiny effects (with huge samples) or miss real effects (with small samples).‚Äù\n‚ÄúAlways look at effect SIZE alongside p-value.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#test-statistics",
    "href": "slides/statistics_101_for_evaluation/index.html#test-statistics",
    "title": "Statistics 101 for Evaluation",
    "section": "Test Statistics",
    "text": "Test Statistics\nTest statistic: A number summarizing how far data diverges from H‚ÇÄ\nCommon ones:\n\nt-statistic: For comparing means (assumes normal-ish distribution)\nz-statistic: For proportions or large samples\nChi-square: For categorical data\n\nGeneral pattern:\n\\[\n\\text{test statistic} = \\frac{\\text{observed difference}}{\\text{standard error of difference}}\n\\]\n\nDon‚Äôt dwell on formulas. Point is: test statistic scales difference by its uncertainty.\n‚ÄúLarge test statistic ‚Üí difference is many standard errors away from zero ‚Üí small p-value.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#type-i-and-type-ii-errors",
    "href": "slides/statistics_101_for_evaluation/index.html#type-i-and-type-ii-errors",
    "title": "Statistics 101 for Evaluation",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\n\n Type I Error\nFalse Positive\nRejecting H‚ÇÄ when it‚Äôs actually true\n\nConcluding intervention worked when it didn‚Äôt\nCrying wolf\nProbability = Œ± (often 0.05)\n\n\n\n\n\nRisk: Waste resources scaling ineffective intervention\n\n\n\n\n\n Type II Error\nFalse Negative\nFailing to reject H‚ÇÄ when H‚ÇÅ is true\n\nMissing a real effect\nMissing the wolf\nProbability = Œ≤ (power = 1 ‚àí Œ≤)\n\n\n\n\n\nRisk: Abandon something that works\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-off: Can‚Äôt eliminate both. Lower Œ± ‚Üí higher Œ≤\n\n\n\n\n\n‚ÄúType I: crying wolf. Type II: missing a wolf.‚Äù\n‚ÄúIn evaluation, both matter. False positive ‚Üí waste resources scaling ineffective thing. False negative ‚Üí abandon something that works.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#confidence-intervals-cis",
    "href": "slides/statistics_101_for_evaluation/index.html#confidence-intervals-cis",
    "title": "Statistics 101 for Evaluation",
    "section": "Confidence Intervals (CIs)",
    "text": "Confidence Intervals (CIs)\nConfidence interval: Range of effect sizes compatible with data\n95% CI interpretation:\n\n‚ÄúIf we repeated this study many times, 95% of calculated intervals would contain the true effect.‚Äù\n\nExample: Completion rate increased by 6 percentage points (95% CI: 2 to 10)\n\nWe estimate +6pp\nPlausible range: +2pp to +10pp\nAll positive ‚Üí confident direction is upward\n\n\n‚ÄúCIs are more informative than p-values alone.‚Äù\n‚ÄúThey show magnitude and precision together.‚Äù\nCommon mistake: ‚Äú95% chance true effect is in this interval.‚Äù No‚Äîeither it is or isn‚Äôt. The 95% refers to the method, not the particular interval.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#why-cis-beat-p-values-alone",
    "href": "slides/statistics_101_for_evaluation/index.html#why-cis-beat-p-values-alone",
    "title": "Statistics 101 for Evaluation",
    "section": "Why CIs Beat p-values Alone",
    "text": "Why CIs Beat p-values Alone\np-value tells you: Is there evidence of an effect?\nCI tells you:\n\nIs there evidence of an effect? (if interval excludes zero)\nHow big might the effect be? (range)\nHow uncertain is the estimate? (width)\n\nExample:\n\np = 0.04 could mean: +10pp effect (CI: +1 to +19) ‚Üí uncertain!\nOr: +2pp effect (CI: +0.1 to +3.9) in huge sample ‚Üí precise but small\n\n\n‚ÄúAlways report CIs alongside p-values (or instead of).‚Äù\n‚ÄúPolicymakers care about: ‚ÄòHow much impact can we expect?‚Äô CIs answer that.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#interactive-demo-ab-testing-visualizer",
    "href": "slides/statistics_101_for_evaluation/index.html#interactive-demo-ab-testing-visualizer",
    "title": "Statistics 101 for Evaluation",
    "section": "Interactive Demo: A/B Testing Visualizer",
    "text": "Interactive Demo: A/B Testing Visualizer\nScenario: Two outreach emails (A vs B) inviting residents to community safety survey\nThe tool:\n\nSet true effect size, sample size per arm, baseline response rate\nSimulate many trials\nSee: distribution of p-values, proportion significant, estimated effects vs true effect\n\nKey lessons:\n\nSmall samples ‚Üí p-values are noisy, significance is rare even for real effects\nCIs often wide with small n\nStatistical significance isn‚Äôt destiny\n\n\n[Interactive tool demonstration]\n‚ÄúI‚Äôll run 1000 simulated trials with true +3pp effect, n=200 per arm.‚Äù\n‚ÄúNotice: only ~40% come back significant. That‚Äôs power!‚Äù\nSpend ~8 minutes here."
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#confidence-interval-explorer",
    "href": "slides/statistics_101_for_evaluation/index.html#confidence-interval-explorer",
    "title": "Statistics 101 for Evaluation",
    "section": "Confidence Interval Explorer",
    "text": "Confidence Interval Explorer\nScenario: Post-intervention completion rate: 42% ‚Üí 48%\nThe tool:\n\nInput: number of successes and total trials per group\nOutput: estimated difference, 95% CI, visualization\n\nTry:\n\nSmall samples ‚Üí wide CIs\nLarge samples ‚Üí narrow CIs\nWhat if interval includes zero?\n\n\n[Interactive tool]\n‚ÄúLet‚Äôs input: 42/100 before, 48/100 after. See the interval.‚Äù\n‚ÄúThen try: 420/1000 before, 480/1000 after. Much narrower.‚Äù\nSpend ~6 minutes."
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#translating-cis-to-plain-language",
    "href": "slides/statistics_101_for_evaluation/index.html#translating-cis-to-plain-language",
    "title": "Statistics 101 for Evaluation",
    "section": "Translating CIs to Plain Language",
    "text": "Translating CIs to Plain Language\nPractice interpreting:\nExample 1: ‚ÄúCompletion rate improved by 5pp (95% CI: ‚àí2 to +12)‚Äù\nPlain language: ‚ÄúWe estimate a 5 percentage point improvement, but the data are consistent with anything from a 2pp decrease to a 12pp increase. Not conclusive.‚Äù\nExample 2: ‚ÄúResponse time decreased by 3 days (95% CI: 1.5 to 4.5)‚Äù\nPlain language: ‚ÄúWe‚Äôre confident response time improved‚Äîlikely between 1.5 and 4.5 days faster.‚Äù\n\n‚ÄúThis is how you communicate with non-technical stakeholders.‚Äù\n‚ÄúFrame uncertainty honestly. Don‚Äôt oversell weak results.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#when-not-significant-matters",
    "href": "slides/statistics_101_for_evaluation/index.html#when-not-significant-matters",
    "title": "Statistics 101 for Evaluation",
    "section": "When ‚ÄúNot Significant‚Äù Matters",
    "text": "When ‚ÄúNot Significant‚Äù Matters\nScenario: Large, well-powered study finds p = 0.42, CI includes zero\nImplications:\n\nNo evidence of effect\nCould be genuinely no effect\nOr effect too small to be practically meaningful\n\nDon‚Äôt say: ‚ÄúThe intervention doesn‚Äôt work.‚Äù\nDo say: ‚ÄúWe found no detectable effect in this study. If there is an effect, it‚Äôs likely smaller than X.‚Äù\n\n‚ÄúAbsence of evidence ‚â† evidence of absence.‚Äù\n‚ÄúBut if study was well-powered, ‚Äòno effect detected‚Äô is informative.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#statistical-power",
    "href": "slides/statistics_101_for_evaluation/index.html#statistical-power",
    "title": "Statistics 101 for Evaluation",
    "section": "Statistical Power",
    "text": "Statistical Power\n\n\n\n Definition\n\n\nProbability of detecting an effect if it exists\n\\[\n\\text{Power} = 1 - \\beta = P(\\text{reject } H_0 | H_1 \\text{ true})\n\\]\n\n\n\n\n\n Common Target\n80% power\n\nIf intervention has real +5pp effect, you‚Äôd detect it (p&lt;0.05) in 80% of studies\n\n\n Why It Matters\n\n\n\n\n\n\n\nUnderpowered Studies\n\n\n\nWaste resources\nMislead decisions\nMiss real effects\n\n\n\n\n\n\n\nPower is your study‚Äôs ‚Äúsensitivity‚Äù\n\n\n‚ÄúPower is your study‚Äôs ‚Äòsensitivity‚Äô.‚Äù\n‚ÄúLow power ‚Üí might miss real effects ‚Üí false negatives ‚Üí abandon good interventions.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#what-affects-power",
    "href": "slides/statistics_101_for_evaluation/index.html#what-affects-power",
    "title": "Statistics 101 for Evaluation",
    "section": "What Affects Power?",
    "text": "What Affects Power?\n\n\n Power Increases With:\n\n Larger effect size\nEasier to detect big effects\n Larger sample size\nMore data ‚Üí less noise\n Lower variance\nLess noisy outcome ‚Üí clearer signal\n Higher Œ±\nBut raises false positive risk (don‚Äôt do this)\n\n\n What You Control\n\n\n\n\nYou CAN control\n\n\nSample size (usually)\n\n\n\n\n\n\n\n\nYou CANNOT control\n\n\n\nTrue effect size\nOutcome variance\n\n\n\n\n\n\n\n‚ÄúIn evaluation design, you choose n to achieve target power for a plausible effect size.‚Äù\n‚ÄúThis requires upfront thinking: What effect size would matter operationally?‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#minimum-detectable-effect-mde",
    "href": "slides/statistics_101_for_evaluation/index.html#minimum-detectable-effect-mde",
    "title": "Statistics 101 for Evaluation",
    "section": "Minimum Detectable Effect (MDE)",
    "text": "Minimum Detectable Effect (MDE)\nMDE: Smallest effect size your study can reliably detect\nDetermined by:\n\nSample size\nVariance\nPower target\nSignificance level\n\nExample: With n=500 per arm, 80% power, Œ±=0.05, you can detect MDE ‚âà 4pp change in a 50% baseline rate\nImplication: If true effect is 2pp, you‚Äôll likely miss it.\n\n‚ÄúBefore running evaluation, calculate MDE.‚Äù\n‚ÄúAsk: Is that MDE operationally meaningful? If we can only detect 4pp changes, but 2pp would be valuable, we‚Äôre underpowered.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#planning-sample-size",
    "href": "slides/statistics_101_for_evaluation/index.html#planning-sample-size",
    "title": "Statistics 101 for Evaluation",
    "section": "Planning Sample Size",
    "text": "Planning Sample Size\nProcess:\n\nDefine meaningful effect size (policy judgment)\nEstimate baseline rate and variance (pilot data or literature)\nChoose power (typically 80%) and Œ± (typically 0.05)\nCalculate required n using power formulas or online calculator\n\nExample: SMS reminder for missed appointments\n\nBaseline no-show: 20%\nTarget: detect 3pp reduction (to 17%)\nPower: 80%, Œ±: 0.05\nRequired: ~2,200 per arm\n\n\n‚ÄúPower calculation is planning, not an afterthought.‚Äù\n‚ÄúUnderpowered studies are unethical: they waste participant time and resources for inconclusive results.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#interactive-power-slider",
    "href": "slides/statistics_101_for_evaluation/index.html#interactive-power-slider",
    "title": "Statistics 101 for Evaluation",
    "section": "Interactive: Power Slider",
    "text": "Interactive: Power Slider\nThe tool:\n\nAdjust: baseline rate, MDE, sample size per arm\nSee: estimated power, power curve vs n\n\nExperiments:\n\nSmall n ‚Üí low power\nLarge n ‚Üí diminishing returns (going from 80% to 90% power requires a lot more n)\nSmaller MDE ‚Üí need more n\n\n\n[Interactive tool]\n‚ÄúPlay with this. Notice: doubling sample size doesn‚Äôt double power.‚Äù\n‚ÄúThere‚Äôs a point where adding more participants gives little extra power‚Äîbalance feasibility and rigor.‚Äù\nSpend ~7 minutes."
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#diminishing-returns",
    "href": "slides/statistics_101_for_evaluation/index.html#diminishing-returns",
    "title": "Statistics 101 for Evaluation",
    "section": "Diminishing Returns",
    "text": "Diminishing Returns\nKey insight: Power increases with ‚àön, not n\nExample:\n\nn=100 per arm ‚Üí 40% power\nn=400 per arm ‚Üí 80% power (4x sample ‚Üí 2x power)\nn=1600 per arm ‚Üí ~95% power (16x sample for &lt; 1.2x power)\n\nPractical implication: Past a certain point, more data helps little. Focus on good design instead.\n\n‚ÄúDon‚Äôt reflexively say ‚Äòwe need more data‚Äô.‚Äù\n‚ÄúBetter: reduce variance (better outcome measure), increase effect size (stronger intervention), or accept lower power if resources constrained.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#when-underpowered-studies-happen",
    "href": "slides/statistics_101_for_evaluation/index.html#when-underpowered-studies-happen",
    "title": "Statistics 101 for Evaluation",
    "section": "When Underpowered Studies Happen",
    "text": "When Underpowered Studies Happen\nReality: Many civic tech evaluations are underpowered due to:\n\nBudget constraints\nLimited user base\nShort timeframes\n\nWhat to do:\n\nAcknowledge power limitations upfront\nFocus on effect sizes and CIs (not p-values)\nUse findings as pilot evidence for larger study\nConsider multiple small studies (meta-analysis later)\n\n\n‚ÄúUnderpowered ‚â† useless. Just be transparent.‚Äù\n‚ÄúReport: ‚ÄòThis study had 40% power to detect a 5pp effect. Results are exploratory.‚Äô‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#multiple-comparisons-problem",
    "href": "slides/statistics_101_for_evaluation/index.html#multiple-comparisons-problem",
    "title": "Statistics 101 for Evaluation",
    "section": "Multiple Comparisons Problem",
    "text": "Multiple Comparisons Problem\n\n\n\n The Issue\n\n\nTest many outcomes ‚Üí some will be ‚Äúsignificant‚Äù by chance\n\n\n\n\n\n Example\nYou measure 20 outcomes at Œ±=0.05\n\n\nExpect 1 false positive even if intervention has zero effect\nRisk of cherry-picking the ‚Äúsignificant‚Äù one\n\n\n\n\nThis is rampant in evaluations\n\n\n\n Mitigation\n\n Pre-specify primary outcome\n Adjust Œ± for multiple tests\n Report all tests, not just significant ones\n\n\n\n\n\nDecide what you‚Äôre testing before looking at data\n\n\n\n\n\n\n‚ÄúThis is rampant in evaluations: measure everything, then trumpet the one thing that worked.‚Äù\n‚ÄúSolution: decide what you‚Äôre testing before you look at data.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#p-hacking-researcher-degrees-of-freedom",
    "href": "slides/statistics_101_for_evaluation/index.html#p-hacking-researcher-degrees-of-freedom",
    "title": "Statistics 101 for Evaluation",
    "section": "p-hacking & Researcher Degrees of Freedom",
    "text": "p-hacking & Researcher Degrees of Freedom"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#p-hacking",
    "href": "slides/statistics_101_for_evaluation/index.html#p-hacking",
    "title": "Statistics 101 for Evaluation",
    "section": " p-hacking",
    "text": "p-hacking\nTweaking analysis until you get p&lt;0.05"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#the-garden-of-forking-paths",
    "href": "slides/statistics_101_for_evaluation/index.html#the-garden-of-forking-paths",
    "title": "Statistics 101 for Evaluation",
    "section": "The Garden of Forking Paths",
    "text": "The Garden of Forking Paths\nVisualization:\n\n\n\n\n\ngraph TD\n    A[Collect Data] --&gt; B{Analyze as binary&lt;br/&gt;or continuous?}\n    B --&gt;|Binary| C{Include outliers?}\n    B --&gt;|Continuous| D{Transform variable?}\n    C --&gt;|Yes| E{Control for age?}\n    C --&gt;|No| F{Control for age?}\n    D --&gt;|Log| G{Control for age?}\n    D --&gt;|None| H{Control for age?}\n    E --&gt; I[p=0.08]\n    F --&gt; J[p=0.04]\n    G --&gt; K[p=0.06]\n    H --&gt; L[p=0.03]\n\n\n\n\n\n\nResult: You find a ‚Äúsignificant‚Äù path ‚Üí false positive!\n\n‚ÄúEach fork is a reasonable choice. But exploring all paths and reporting the best one is cheating.‚Äù\n‚ÄúDecide the path before looking at outcomes.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#regression-to-the-mean",
    "href": "slides/statistics_101_for_evaluation/index.html#regression-to-the-mean",
    "title": "Statistics 101 for Evaluation",
    "section": "Regression to the Mean",
    "text": "Regression to the Mean\nPhenomenon: Extreme values tend to move toward average on re-measurement\nExample: Hotspot policing\n\nIdentify areas with highest crime last month\nIntervene there\nCrime drops next month\n\nIs it the intervention? Partly, but also: extreme months are often followed by less extreme months naturally.\nMitigation: Use control areas, longer baselines, or regression discontinuity designs\n\n‚ÄúThis is subtle but pervasive.‚Äù\n‚ÄúAny time you select ‚Äòworst performers‚Äô then intervene, you‚Äôll see improvement‚Äîsome of it is regression to mean.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#measurement-drift",
    "href": "slides/statistics_101_for_evaluation/index.html#measurement-drift",
    "title": "Statistics 101 for Evaluation",
    "section": "Measurement Drift",
    "text": "Measurement Drift\nThe issue: Changing definitions or instruments mid-study\nExamples:\n\nRedefining ‚Äúengagement‚Äù after launch\nSwitching survey questions\nChanging data collection process\n\nImpact: Pre/post comparisons are invalid\nPrevention: Lock in measurement approach at design phase; document any changes\n\n‚ÄúIf your thermometer changes midway, you can‚Äôt tell if temperature changed or your measure did.‚Äù\n‚ÄúConsistency in measurement is crucial.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#pre-registration-as-protection",
    "href": "slides/statistics_101_for_evaluation/index.html#pre-registration-as-protection",
    "title": "Statistics 101 for Evaluation",
    "section": "Pre-Registration as Protection",
    "text": "Pre-Registration as Protection\nPre-analysis plan (PAP): Document before seeing outcome data:\n\nPrimary and secondary outcomes\nSubgroups of interest\nStatistical methods\nSample size\nAnalysis code (where feasible)\n\nBenefits:\n\nPrevents p-hacking\nBuilds credibility\nClarifies confirmatory vs exploratory\n\n\n‚ÄúPAPs are standard in medical trials. Should be in policy evaluation too.‚Äù\n‚ÄúYou can still do exploratory analysis‚Äîjust label it as such.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#a-different-philosophical-frame",
    "href": "slides/statistics_101_for_evaluation/index.html#a-different-philosophical-frame",
    "title": "Statistics 101 for Evaluation",
    "section": "A Different Philosophical Frame",
    "text": "A Different Philosophical Frame\nFrequentist (what we‚Äôve been doing):\n\nQuestion: ‚ÄúHow surprising is the data if H‚ÇÄ were true?‚Äù\nOutputs: p-values, CIs (based on imagined repeated sampling)\nNo direct probability about hypotheses\n\nBayesian:\n\nQuestion: ‚ÄúGiven the data, how probable is H‚ÇÅ?‚Äù\nOutputs: Posterior distributions, credible intervals\nDirect probability statements about effects\n\n\n‚ÄúWe‚Äôve used frequentist tools because they‚Äôre standard in policy evaluation.‚Äù\n‚ÄúBut Bayesian methods are increasingly popular. Here‚Äôs a 10-minute tour.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#bayes-theorem-intuition",
    "href": "slides/statistics_101_for_evaluation/index.html#bayes-theorem-intuition",
    "title": "Statistics 101 for Evaluation",
    "section": "Bayes‚Äô Theorem (Intuition)",
    "text": "Bayes‚Äô Theorem (Intuition)\nCore idea: Update beliefs based on evidence\n\\[\n\\text{Posterior} \\propto \\text{Prior} \\times \\text{Likelihood}\n\\]\nTranslation:\n\nPrior: What you believed before seeing data\nLikelihood: How consistent data is with different effect sizes\nPosterior: Updated belief after seeing data\n\n\n‚ÄúBayesian inference combines prior knowledge with new evidence.‚Äù\n‚ÄúExample: You think an intervention helps by 0‚Äì5pp (prior). After study, data shifts belief toward 3pp (posterior).‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#bayesian-vs-frequentist-interpretation",
    "href": "slides/statistics_101_for_evaluation/index.html#bayesian-vs-frequentist-interpretation",
    "title": "Statistics 101 for Evaluation",
    "section": "Bayesian vs Frequentist: Interpretation",
    "text": "Bayesian vs Frequentist: Interpretation\nFrequentist 95% CI: ‚ÄúIn repeated sampling, 95% of intervals would contain true effect‚Äù\n\nRefers to the method, not the specific interval\nCan‚Äôt say ‚Äú95% probability true effect is here‚Äù\n\nBayesian 95% Credible Interval: ‚ÄúThere‚Äôs a 95% probability true effect lies in this range‚Äù\n\nDirect probability statement about the parameter\n\nIntuitive appeal: Bayesian intervals say what people often think frequentist CIs say\n\n‚ÄúThis is why Bayesian methods are attractive: more intuitive interpretation.‚Äù\n‚ÄúBut require specifying prior‚Äîcan be subjective.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#priors-blessing-or-curse",
    "href": "slides/statistics_101_for_evaluation/index.html#priors-blessing-or-curse",
    "title": "Statistics 101 for Evaluation",
    "section": "Priors: Blessing or Curse?",
    "text": "Priors: Blessing or Curse?\nStrength: Incorporate existing evidence\n\nMeta-analyses\nPrior evaluations\nExpert judgment\n\nConcern: Subjectivity\n\nDifferent priors ‚Üí different posteriors\nRisk of bias\n\nResponse: Sensitivity analysis‚Äîtry different priors, see if conclusion robust\n\n‚ÄúIn practice, with enough data, prior matters less‚Äîdata swamps prior.‚Äù\n‚ÄúWith weak data, prior matters a lot.‚Äù\n‚ÄúTransparent priors + sensitivity checks = credible Bayesian analysis.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#when-to-consider-bayesian-approaches",
    "href": "slides/statistics_101_for_evaluation/index.html#when-to-consider-bayesian-approaches",
    "title": "Statistics 101 for Evaluation",
    "section": "When to Consider Bayesian Approaches",
    "text": "When to Consider Bayesian Approaches\nGood fits:\n\nAccumulating evidence across studies (Bayesian meta-analysis)\nSmall samples where prior info helps\nDecision-making under uncertainty (expected value calculations)\nAdaptive trials (update as data comes in)\n\nPractical reality: Most policy evaluation still uses frequentist methods\n\nFamiliarity\nStandard in guidelines (Magenta Book, etc.)\nEasier to explain to non-technical audiences\n\n\n‚ÄúYou don‚Äôt need to become a Bayesian to do good evaluation.‚Äù\n‚ÄúBut know it exists, and that it offers different (sometimes better) tools for certain problems.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#further-learning",
    "href": "slides/statistics_101_for_evaluation/index.html#further-learning",
    "title": "Statistics 101 for Evaluation",
    "section": "Further Learning",
    "text": "Further Learning\nIf interested in Bayesian methods:\n\nStatistical Rethinking by Richard McElreath (accessible)\nDoing Bayesian Data Analysis by John Kruschke\nOnline: StatModeling blog (Andrew Gelman)\n\nFor evaluation work: Frequentist methods remain standard and sufficient for most civic tech projects\n\n‚ÄúToday‚Äôs goal: awareness. You don‚Äôt need to master Bayes now.‚Äù\n‚ÄúTakeaway: there are different statistical philosophies. Frequentist is dominant but not the only way.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#the-statistical-evaluation-toolkit",
    "href": "slides/statistics_101_for_evaluation/index.html#the-statistical-evaluation-toolkit",
    "title": "Statistics 101 for Evaluation",
    "section": "The Statistical Evaluation Toolkit",
    "text": "The Statistical Evaluation Toolkit\n\nSeven Pillars Complete \n\n\n\n\n Uncertainty & variability\nNoise vs signal\n Probability\nLanguage of chance\n Sampling\nWho we measure\n Inference\np-values, CIs, interpretation\n\n\n\n Power\nPlanning for detection\n Pitfalls\nCommon errors to avoid\n Bayesian perspective\nAlternative lens\n\n\n\n\n\n\n\n\n\nNext Steps\n\n\nApply these to real evaluation designs\n\n\n\n\n\n‚ÄúThis was the theory spine. Next sessions: designing randomized trials, quasi-experimental methods, measurement.‚Äù\n‚ÄúThese stats concepts underpin all of that work.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#decision-framework-choosing-methods",
    "href": "slides/statistics_101_for_evaluation/index.html#decision-framework-choosing-methods",
    "title": "Statistics 101 for Evaluation",
    "section": "Decision Framework: Choosing Methods",
    "text": "Decision Framework: Choosing Methods\n\n\n\n When Planning an Evaluation\n\n\n\n\n\n\n\n\n Key Questions\n\n Can I randomize?\n‚Üí RCT with power analysis\n Is there natural variation?\n‚Üí Quasi-experimental, check balance\n What‚Äôs my outcome type?\n‚Üí Determines test choice\n\n\n\n\n How large is my sample?\n‚Üí Power calculation, adjust expectations\n What biases exist?\n‚Üí Sampling strategy, measurement plan\n\n\n\n\n\n\n\n\n\n\nStats informs design, design enables stats.\n\n\n\n\n\n‚ÄúStatistics isn‚Äôt separate from design. They‚Äôre intertwined.‚Äù\n‚ÄúGood evaluation starts with: What‚Äôs my question, who do I sample, how do I randomize (if possible), what‚Äôs my power?‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#communicating-statistical-results",
    "href": "slides/statistics_101_for_evaluation/index.html#communicating-statistical-results",
    "title": "Statistics 101 for Evaluation",
    "section": "Communicating Statistical Results",
    "text": "Communicating Statistical Results\n\n\n What to Include\n\n Effect size (magnitude + direction)\n Uncertainty (CI or SE)\n Sample size and response rate\n Method (in brief)\n Caveats (limitations, assumptions)\n\n\n What to Avoid\n\n P-values without context\n ‚ÄúMarginally significant‚Äù weasel words\n Overconfident causal language from weak designs\n\n\n\n\n\nAudience matters: Policymakers need the ‚Äúso what?‚Äù, not the t-statistic\n\n\n\n\n\n\n‚ÄúYour job: translate stats into decision-relevant language.‚Äù\n‚ÄúExample: Instead of ‚Äòt=2.3, p=0.02‚Äô, say ‚ÄòResponse time improved by an average of 3 days (95% CI: 0.5 to 5.5 days), a meaningful improvement for residents.‚Äô‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#evaluation-integrity-checklist",
    "href": "slides/statistics_101_for_evaluation/index.html#evaluation-integrity-checklist",
    "title": "Statistics 101 for Evaluation",
    "section": "Evaluation Integrity Checklist",
    "text": "Evaluation Integrity Checklist\n\n\n\n Before Running Analysis\n\n\nConfirm these items:\n\n\n\n\n\n\n Pre-specified primary outcome\n Sample size justified (power calc)\n Sampling method documented\n Randomization protocol (if applicable)\n\n\n\n Measurement consistent pre/post\n Analysis plan written down\n Ethical approval obtained\n Data quality checks planned\n\n\n\n\n\n\n\n\n\n Remember\n\n\nIntegrity = Credibility\n\n\n\n\n\n‚ÄúThis checklist prevents most pitfalls we discussed.‚Äù\n‚ÄúTreat it like a pilot‚Äôs pre-flight checklist. Non-negotiable.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#worked-example-revisited-service-form",
    "href": "slides/statistics_101_for_evaluation/index.html#worked-example-revisited-service-form",
    "title": "Statistics 101 for Evaluation",
    "section": "Worked Example Revisited: Service Form",
    "text": "Worked Example Revisited: Service Form\n\n\n\n Recall Our Earlier Question\n\n\n\nBefore: 42 completions/day (SD=8)\nAfter: 47 completions/day (first week)\n\n\n\n\n\n\n Now We Can:\n\nCalculate difference: +5\nCompute SE: ‚âà 3.2\nTest statistic: 1.56\np-value: ‚âà 0.12\n95% CI: (‚àí1.3, +11.3)\n\n\n Interpretation\n\n\n\n\n\n\n\nWarning\n\n\nInsufficient evidence of improvement\n\nCI includes zero\np &gt; 0.05\nNeed more data or longer follow-up\n\n\n\n\n\n\nThis is how the pieces fit together!\n\n\n\n‚ÄúThis is how the pieces fit together.‚Äù\n‚ÄúWe quantified noise (SD=8), calculated uncertainty (SE), tested (p=0.12), and gave a range (CI).‚Äù\n‚ÄúConclusion: suggestive but not conclusive. Design a longer evaluation.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#your-turn-apply-to-your-project",
    "href": "slides/statistics_101_for_evaluation/index.html#your-turn-apply-to-your-project",
    "title": "Statistics 101 for Evaluation",
    "section": "Your Turn: Apply to Your Project",
    "text": "Your Turn: Apply to Your Project\n\n\n\n Reflection Exercise (5 minutes)\n\n\n\n\n\n\n\n\n\n What‚Äôs your primary outcome?\n What‚Äôs a plausible effect size worth detecting?\n What‚Äôs your sampling strategy?\n\n\n\n What are the biggest threats to validity?\n What‚Äôs one statistical pitfall you‚Äôll avoid?\n\n\n\nGive participants 5 minutes of silent writing time.\nOptional: Have 2‚Äì3 people share one insight.\nThis cements learning through application."
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#key-takeaways",
    "href": "slides/statistics_101_for_evaluation/index.html#key-takeaways",
    "title": "Statistics 101 for Evaluation",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThe Essential Six \n\n\n Statistics quantifies uncertainty ‚Äî essential for credible evaluation\n Random sampling eliminates bias ‚Äî who you measure matters\n CIs &gt; p-values alone ‚Äî always report effect sizes and ranges\n Power planning prevents wasted effort ‚Äî design for detection\n Pre-registration prevents p-hacking ‚Äî decide before you peek\n Communication is part of the science ‚Äî translate findings for action\n\n\n\n\n‚ÄúGood statistics turns noise into knowledge and knowledge into policy.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#next-session-preview",
    "href": "slides/statistics_101_for_evaluation/index.html#next-session-preview",
    "title": "Statistics 101 for Evaluation",
    "section": "Next Session Preview",
    "text": "Next Session Preview\n\n\n\n Next Up: Designing Randomized Evaluations\n\n\n\n\n\n\n\n\n Building on Today\n\n RCT design principles\n Randomization mechanics\n Balance checks\n\n\n\n\n Compliance and attrition\n Ethics and implementation\n\n\n\n\n\n\nPrep: Think about a project where randomization might be feasible\n\n\n\n\n\n\n‚ÄúToday gave you the statistical grammar. Next week: applying it to experimental design.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#resources-further-reading",
    "href": "slides/statistics_101_for_evaluation/index.html#resources-further-reading",
    "title": "Statistics 101 for Evaluation",
    "section": "Resources & Further Reading",
    "text": "Resources & Further Reading\nAccessible:\n\nAngrist & Pischke, Mastering ‚ÄôMetrics\nGertler et al., Impact Evaluation in Practice (World Bank, free PDF)\nJ-PAL Research Resources (online)\n\nTechnical:\n\nImbens & Rubin, Causal Inference for Statistics\nGelman & Hill, Data Analysis Using Regression\n\nUK policy:\n\nHM Treasury Magenta Book (evaluation guidance)\nWhat Works Network resources\n\n\n‚ÄúDon‚Äôt feel you need to read all of these. Pick one that matches your level.‚Äù\n‚ÄúMastering ‚ÄôMetrics is fantastic and very readable.‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#questions",
    "href": "slides/statistics_101_for_evaluation/index.html#questions",
    "title": "Statistics 101 for Evaluation",
    "section": "Questions?",
    "text": "Questions?\nOpen floor for questions:\n\nConcepts unclear?\nSpecific evaluation scenarios?\nTool requests?\n\nOffice hours: [Insert your availability]\nSlack channel: #evaluation-stats\n\nTake 5‚Äì10 minutes for Q&A.\nEncourage questions on their own projects.\nRemind: ‚ÄúNo question is too basic. This stuff is hard!‚Äù"
  },
  {
    "objectID": "slides/statistics_101_for_evaluation/index.html#thank-you",
    "href": "slides/statistics_101_for_evaluation/index.html#thank-you",
    "title": "Statistics 101 for Evaluation",
    "section": "Thank You!",
    "text": "Thank You!\n\n‚ÄúStatistics is the science of learning from experience, especially experience that arrives a little bit at a time.‚Äù ‚Äî Frederick Mosteller\n\nSee you next session!\n\nEnd on a positive, encouraging note.\nRemind them: statistical thinking is a skill, improves with practice.\nThank them for engagement."
  },
  {
    "objectID": "slides/intro/intro.html#aims-for-today",
    "href": "slides/intro/intro.html#aims-for-today",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Aims for today",
    "text": "Aims for today\n\nIntroduction to the module content\nSome group decisions\nA case study from me"
  },
  {
    "objectID": "slides/intro/intro.html#about-me",
    "href": "slides/intro/intro.html#about-me",
    "title": "Delivering Innovation in Public Institutions",
    "section": "About Me",
    "text": "About Me\n\nOperational policing and crime background\nLots of work around community tech innovation\nFocus on impact evaluation\nNow work in central govt"
  },
  {
    "objectID": "slides/intro/intro.html#module-aims",
    "href": "slides/intro/intro.html#module-aims",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Module Aims",
    "text": "Module Aims\n\nFocus is not on building innovation (covered in plenty of other modules)\nInstead, we‚Äôll look at how to implement innovation (and when it fails)\n\nImplementation and delivery\nPushback, internal and external\nIncentives and pushback\nImplementation delivery challenges that can kill good ideas."
  },
  {
    "objectID": "slides/intro/intro.html#linked-modules",
    "href": "slides/intro/intro.html#linked-modules",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Linked Modules",
    "text": "Linked Modules\n\nThere are several other modules that will touch on topics touched here.\n\nInstitutional Analysis\nPolitical Organising\nService Design\nProduct Design\nand more!\nEvaluation masterclass next year?"
  },
  {
    "objectID": "slides/intro/intro.html#structure",
    "href": "slides/intro/intro.html#structure",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Structure",
    "text": "Structure\n\nAttend Meetups and Events: I‚Äôll mantain a list of government innovation associated meetups. You should attend at least 1 of these, and share notes and reflections with the group\nCase Study Discussion Sessions: As a group, we‚Äôll select 2-3 examples of innovation as case studies, to read up and discuss\nOnline Discussion Platform: Space for ongoing conversations, article sharing, and reflection."
  },
  {
    "objectID": "slides/intro/intro.html#online-content",
    "href": "slides/intro/intro.html#online-content",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Online Content",
    "text": "Online Content\n\nWhere do you want online stuff?\n\nSlack\nDiscord\nWhatsApp\nOthers?\n\nConsider what works best for interaction, notifications, and accessibility."
  },
  {
    "objectID": "slides/intro/intro.html#in-person---where-and-when",
    "href": "slides/intro/intro.html#in-person---where-and-when",
    "title": "Delivering Innovation in Public Institutions",
    "section": "In Person - Where and When?",
    "text": "In Person - Where and When?\n\nI‚Äôll aim to pencil in the next event for the end of this year.\nEvery 6-8 weeks?\nCase studies 4 weeks in advance"
  },
  {
    "objectID": "slides/intro/intro.html#case-studies",
    "href": "slides/intro/intro.html#case-studies",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Case Studies",
    "text": "Case Studies\n\nI‚Äôll suggest a few interesting case studies‚Ä¶\nBut you should also come up with your own!\nWe‚Äôll aim to select the first in the next couple of weeks"
  },
  {
    "objectID": "slides/intro/intro.html#finding-case-studies",
    "href": "slides/intro/intro.html#finding-case-studies",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Finding case studies",
    "text": "Finding case studies\n\nI‚Äôve listed a few potential case studies in the module guide. These include\n\nGOV.UK\nNHS Horizon\nHealthcare.gov\nX-Road\n\nbut I‚Äôd encourage you to think beyond the technical! For example‚Ä¶"
  },
  {
    "objectID": "slides/intro/intro.html#case-study-1---sortition-in-ancient-athens",
    "href": "slides/intro/intro.html#case-study-1---sortition-in-ancient-athens",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Case Study 1 - Sortition in Ancient Athens",
    "text": "Case Study 1 - Sortition in Ancient Athens"
  },
  {
    "objectID": "slides/intro/intro.html#case-study-2---pneumatic-tube-mail-systems",
    "href": "slides/intro/intro.html#case-study-2---pneumatic-tube-mail-systems",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Case Study 2 - Pneumatic Tube Mail Systems",
    "text": "Case Study 2 - Pneumatic Tube Mail Systems"
  },
  {
    "objectID": "slides/intro/intro.html#key-resources-1",
    "href": "slides/intro/intro.html#key-resources-1",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Key Resources",
    "text": "Key Resources\n\nSmithsonian Postal Museum - Pneumatic Mail\nAtlas Obscura - Pneumatic Tubes\nBook: ‚ÄúNeither Snow Nor Rain: A History of the United States Postal Service‚Äù by Devin Leonard"
  },
  {
    "objectID": "slides/intro/intro.html#case-study-3---metric-system-implementation-in-the-u.s.",
    "href": "slides/intro/intro.html#case-study-3---metric-system-implementation-in-the-u.s.",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Case Study 3 - Metric System Implementation in the U.S.",
    "text": "Case Study 3 - Metric System Implementation in the U.S."
  },
  {
    "objectID": "slides/intro/intro.html#case-study-4---proportional-representation-in-the-uk",
    "href": "slides/intro/intro.html#case-study-4---proportional-representation-in-the-uk",
    "title": "Delivering Innovation in Public Institutions",
    "section": "Case Study 4 - Proportional Representation in the UK",
    "text": "Case Study 4 - Proportional Representation in the UK"
  },
  {
    "objectID": "slides/intro/intro.html#and-more",
    "href": "slides/intro/intro.html#and-more",
    "title": "Delivering Innovation in Public Institutions",
    "section": "And more!",
    "text": "And more!\n\nThere are plenty of other suggested case studies in the module handbook\nWe‚Äôll aim to highlight a few, and identify suggested reading, in the next few weeks"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#why-evaluate",
    "href": "slides/intro_to_evaluation/index.html#why-evaluate",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Why Evaluate?",
    "text": "Why Evaluate?\n\nBuilds credibility and trust ‚Äî essential in civic tech and policy.\nTurns stories into strategy ‚Äî bridges activism and evidence.\nEnsures resources go where impact is real, not assumed.\nTranslates innovation into policy adoption.\n\n\nOpen by asking: ‚ÄúWho here has had to prove impact to a funder or policymaker?‚Äù Emphasize evaluation as learning + accountability."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#what-counts-as-evaluation",
    "href": "slides/intro_to_evaluation/index.html#what-counts-as-evaluation",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "What Counts as Evaluation?",
    "text": "What Counts as Evaluation?\nEvaluation ‚â† Reporting\n\n\n\nReporting\nEvaluation\n\n\n\n\nWhat we did\nWhat difference it made\n\n\n\nLevels:\n\nOutputs ‚Üí activities or reach\nOutcomes ‚Üí behavioural or system change\nImpact ‚Üí long-term societal effects\n\n\nEvaluation is the bridge from prototype ‚Üí policy."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#causality-101",
    "href": "slides/intro_to_evaluation/index.html#causality-101",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Causality 101",
    "text": "Causality 101\nGoal: Understand if outcomes happened because of your intervention.\n\nCorrelation ‚â† causation\nCounterfactuals matter ‚Äî what if the program didn‚Äôt exist?\n\nExample: A civic app increases volunteer sign-ups ‚Äî but so did a local festival that month. Which caused it?"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#designing-randomness-in",
    "href": "slides/intro_to_evaluation/index.html#designing-randomness-in",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Designing Randomness In",
    "text": "Designing Randomness In\n\nFor credible evaluation, you either design randomness in or find it later.\n\nProspective approaches (experimental):\n\nRCTs: randomized treatment/control groups\nA/B tests: feature flags, phased rollouts\nPre-registration: analysis plans, power calculations, balance checks\n\nVisual flow:\nDesign Phase ‚Üí Random Assignment ‚Üí Measure Outcomes ‚Üí Compare\nKey advantage: Strong causal claims because you control the assignment\n\nEmphasize: if you can randomize, do it. It‚Äôs the gold standard."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#finding-randomness-later",
    "href": "slides/intro_to_evaluation/index.html#finding-randomness-later",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Finding Randomness Later",
    "text": "Finding Randomness Later\nRetrospective approaches (quasi-experimental):\n\nNatural experiments: staggered rollouts across councils\nRegression discontinuity: eligibility thresholds (age, geography)\nInstrumental variables: policy timing, external shocks\n\nVisual flow:\nExisting Variation ‚Üí Identify It ‚Üí Exploit It ‚Üí Measure ‚Üí Compare\nSanity checks needed: parallel trends, placebo tests, robustness checks\n\nWhen randomization isn‚Äôt possible, look for ‚Äúas-if random‚Äù variation in the real world."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#evaluation-approaches",
    "href": "slides/intro_to_evaluation/index.html#evaluation-approaches",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Evaluation Approaches",
    "text": "Evaluation Approaches\n\n\n\n\n\n\n\n\n\nType\nDescription\nPros\nCons\n\n\n\n\nRCTs\nRandomly assign treatment/control\nStrong causal inference\nCostly, sometimes unethical\n\n\nQuasi-experimental\nNatural variation or rollout timing\nOften feasible\nHarder to prove causality\n\n\nObservational\nCompare existing data patterns\nEasy, fast\nProne to bias\n\n\nQualitative / Participatory\nInterviews, ethnography, co-design\nContext-rich\nHard to generalize\n\n\n\n\nAsk: ‚ÄúWhich methods do you think most civic tech orgs actually use ‚Äî and why?‚Äù"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#choosing-your-method-decision-tree",
    "href": "slides/intro_to_evaluation/index.html#choosing-your-method-decision-tree",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Choosing Your Method: Decision Tree",
    "text": "Choosing Your Method: Decision Tree\n\n\n\n\n\ngraph TD\n    A[Can you randomize assignment?] --&gt;|Yes| B[RCT / A-B Test]\n    A --&gt;|No| C[Is there natural variation?]\n    C --&gt;|Yes| D[Quasi-experimental&lt;br/&gt;Natural experiment, RD, IV]\n    C --&gt;|No| E[Strong theory + controls?]\n    E --&gt;|Yes| F[Observational&lt;br/&gt;with caution]\n    E --&gt;|No| G[Qualitative / Mixed Methods&lt;br/&gt;Focus on mechanism]\n\n\n\n\n\n\n\nWalk through the tree with a real example from the class."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#case-study-sure-start-uk",
    "href": "slides/intro_to_evaluation/index.html#case-study-sure-start-uk",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Case Study: Sure Start (UK)",
    "text": "Case Study: Sure Start (UK)\n\nEarly evaluations: little measurable impact.\nLong-term follow-ups: strong benefits in education and health.\nDemonstrates value of persistence and longitudinal data.\n\nLesson: Good programs take time to show impact.\nSure Start Impact Study ‚Üí"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#case-study-fixmystreet-mysociety",
    "href": "slides/intro_to_evaluation/index.html#case-study-fixmystreet-mysociety",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Case Study: FixMyStreet (mySociety)",
    "text": "Case Study: FixMyStreet (mySociety)\n\nEvaluated via behavioural data (reports before/after launch).\nFound sustained increases in citizen engagement.\nMethod: Quasi-experimental (variation by council rollout).\n\nLesson: Behavioural data can be powerful when context is understood.\nmySociety Research ‚Üí"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#case-study-citizenlab",
    "href": "slides/intro_to_evaluation/index.html#case-study-citizenlab",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Case Study: CitizenLab",
    "text": "Case Study: CitizenLab\n\nCombines platform analytics (quant) + partner interviews (qual).\nFound: engagement increases when participants see visible outcomes.\nIllustrates value of mixed methods.\n\nCitizenLab Impact Report ‚Üí"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#case-study-d.a.r.e.-us",
    "href": "slides/intro_to_evaluation/index.html#case-study-d.a.r.e.-us",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Case Study: D.A.R.E. (US)",
    "text": "Case Study: D.A.R.E. (US)\n\nPopular anti-drug education program.\nEvaluations showed no measurable effect on drug use.\nYet widely funded for decades.\n\nLesson: Popular ‚â† effective ‚Äî evidence must challenge assumptions."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#d.a.r.e.-evaluation",
    "href": "slides/intro_to_evaluation/index.html#d.a.r.e.-evaluation",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "D.A.R.E. Evaluation ‚Üí",
    "text": "D.A.R.E. Evaluation ‚Üí"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#quantitative-vs-qualitative",
    "href": "slides/intro_to_evaluation/index.html#quantitative-vs-qualitative",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Quantitative vs Qualitative",
    "text": "Quantitative vs Qualitative\n\n\n\nQuantitative\nQualitative\n\n\n\n\nHow much, how often\nWhy and how\n\n\nNumbers, patterns\nStories, meaning\n\n\nScale\nContext\n\n\n\n\nBest practice: Combine both ‚Äî Quant shows pattern, Qual explains mechanism."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#theory-of-change-the-foundation",
    "href": "slides/intro_to_evaluation/index.html#theory-of-change-the-foundation",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Theory of Change: The Foundation",
    "text": "Theory of Change: The Foundation\nVisual framework:\nInputs ‚Üí Activities ‚Üí Outputs ‚Üí Outcomes ‚Üí Impact\n  ‚Üì         ‚Üì           ‚Üì          ‚Üì          ‚Üì\nStaff,   Workshops,  # People   Behaviour  Long-term\nFunding  Platforms   Reached    Change     Societal\n                                           Change\nExample (civic tech):\n\nInput: Developer time, funding\nActivity: Build reporting platform\nOutput: 1,000 reports submitted\nOutcome: Council response time ‚Üì 40%\nImpact: Improved local accountability\n\n\nAlways start with theory of change before choosing metrics."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#designing-meaningful-metrics",
    "href": "slides/intro_to_evaluation/index.html#designing-meaningful-metrics",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Designing Meaningful Metrics",
    "text": "Designing Meaningful Metrics\nPrinciples (beyond SMART):\n\nDecision-linked: if the metric moves, someone acts\nBehavioural: measure revealed behaviours, not just attitudes\nAttributable: plausibly tied to your intervention (via design/ID)\nAffordable: feasible to collect reliably\n\n‚úÖ Examples:\n\n% of residents who resolve an issue via the platform (not just reports)\nResponse-time distribution by council before/after tool launch\nEvidence of policy uptake: citations, budget lines, guidelines changed\n\n\nPush ‚Äúoutcomes, not outputs‚Äù; tie each metric to a use-case/decision."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#finding-the-right-metrics",
    "href": "slides/intro_to_evaluation/index.html#finding-the-right-metrics",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Finding the Right Metrics",
    "text": "Finding the Right Metrics\n\nStart from theory of change ‚Üí map inputs ‚Üí activities ‚Üí outputs ‚Üí outcomes ‚Üí impact\nAsk: ‚ÄúIf it worked, what would be observably different in 3, 6, 12 months?‚Äù\nPrioritise leading indicators (early, sensitive) + a few lagging indicators (durable)\n\nSources:\n\nProduct logs / admin data\nSurveys (pre/post, panels)\nOpen data / FOI\nPolicy documents / Hansard / guidance changes"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#data-quality-challenges",
    "href": "slides/intro_to_evaluation/index.html#data-quality-challenges",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Data Quality Challenges",
    "text": "Data Quality Challenges\nCommon issues:\n\nMissingness: incomplete records, dropout\nDuplicates: same user, multiple accounts\nBots & spam: automated submissions\nChannel shifts: users move between platforms\nDefinition drift: what counts as ‚Äúengagement‚Äù changes over time\n\nMitigations:\n\nInstrument events early; pilot measures\nPre-register core outcomes; freeze queries\nTrack adoption & exposure (who actually saw the ‚Äútreatment‚Äù?)"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#design-ethics-challenges",
    "href": "slides/intro_to_evaluation/index.html#design-ethics-challenges",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Design & Ethics Challenges",
    "text": "Design & Ethics Challenges\nKey considerations:\n\nEthics & consent: DPIA, minimal data collection, retention policies\nStatistical power: enough N to detect plausible effects (MDE thinking)\nExposure tracking: who actually experienced the intervention?\nTiming: seasonality, external events affecting baseline\n\nBest practices:\n\nGet ethical approval early\nCalculate required sample sizes upfront\nDocument all design decisions\nPlan for sensitivity analyses"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#communicating-findings",
    "href": "slides/intro_to_evaluation/index.html#communicating-findings",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Communicating Findings",
    "text": "Communicating Findings\nWhy it matters: Evidence without communication ‚âà no impact.\nCompare these:\n\n\n‚ùå Weak communication:\n‚Äú500 people used our app and we got positive feedback.‚Äù\n\n‚úÖ Strong communication:\n‚ÄúDigital reporting reduced council response times by 40% (from 14 to 8 days, p&lt;0.01, RCT with n=20 councils, 6-month follow-up).‚Äù\n\nKey elements: Outcome + magnitude + uncertainty + method + context"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#audience-first-packaging",
    "href": "slides/intro_to_evaluation/index.html#audience-first-packaging",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Audience-First Packaging",
    "text": "Audience-First Packaging\nTailor your message:\n\nPolicymakers/Funders: 2‚Äì3 page brief, counterfactual logic, cost/benefit, timing\nPractitioners: playbooks, checklists, implementation notes\nPublic/Media: plain-language summary, visuals, examples\n\nDo better than the PDF:\n\nOne-pager + open dataset + reproducible notebook\nAccessible dashboards with methods appendix\nShare failures + ‚Äúwhat we‚Äôd change next time‚Äù\n\nChecklist: headline, context, method (in one paragraph), 3 findings, limits, action items"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#linking-evaluation-to-policy-impact",
    "href": "slides/intro_to_evaluation/index.html#linking-evaluation-to-policy-impact",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Linking Evaluation to Policy Impact",
    "text": "Linking Evaluation to Policy Impact\n\nPolicymakers respond to robust, communicable evidence.\nCivic tech influences policy when:\n\nEvaluation aligns with timing and priorities.\nThere‚Äôs a clear theory of change.\nResults are framed in actionable terms.\n\n\nExample: ‚ÄúDigital participation improves local accountability‚Äù &gt; ‚ÄúUsers liked our app.‚Äù"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#common-pitfalls",
    "href": "slides/intro_to_evaluation/index.html#common-pitfalls",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Common Pitfalls",
    "text": "Common Pitfalls\n\nEvaluating too early or too narrowly.\nOver-attributing success to tech layer.\nIgnoring social/economic context.\nTreating evaluation as funder compliance, not learning."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#project-design-your-evaluation",
    "href": "slides/intro_to_evaluation/index.html#project-design-your-evaluation",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Project: Design Your Evaluation",
    "text": "Project: Design Your Evaluation\nTask (in groups of 2-3):\n\nPick a civic-tech/policy project (real or planned)\nSketch the theory of change (inputs ‚Üí impact)\nPropose one key metric + one identification strategy\n\nWhere‚Äôs the randomness/variation?\n\nDraft how you‚Äôll communicate results\n\nWho‚Äôs the audience? What format?\n\n\nDeliverables:\n\n5-minute presentation to class\nOne-page evaluation plan (template provided)\n\n\nGive 20 minutes for group work, then 2-3 groups present."
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#key-takeaways",
    "href": "slides/intro_to_evaluation/index.html#key-takeaways",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nEvaluation moves civic tech from experiment ‚Üí evidence ‚Üí policy.\nYou need randomness or strong quasi-ID for credible impact.\nGood metrics are decision-linked and feasible to measure.\nCommunication is part of the science.\n\n\n‚ÄúWhat gets measured gets improved ‚Äî and what gets understood gets funded.‚Äù"
  },
  {
    "objectID": "slides/intro_to_evaluation/index.html#further-reading",
    "href": "slides/intro_to_evaluation/index.html#further-reading",
    "title": "Evaluation & Impact in Civic Technology",
    "section": "Further Reading",
    "text": "Further Reading\n\nAndrew Leigh, Randomistas: How Radical Researchers Changed the World\nNesta, Standards of Evidence\nmySociety Research Reports\nInstitute for Fiscal Studies, Sure Start Evaluations\nWhat Works Digital, Evaluation Guidance for Digital Services (2024)"
  }
]